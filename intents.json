{
    "intents": [
        {
            "tag": "greeting",
            "patterns": [
              "Hi",
              "Hey",
              "How are you",
              "Is anyone there?",
              "Hello",
              "Good day"
            ],
            "responses": [
              "Hey :-)",
              "Hello, thanks for visiting",
              "Hi there, what can I do for you?",
              "Hi there, how can I help?"
            ]
          },
          {
            "tag": "goodbye",
            "patterns": ["Bye", "See you later", "Goodbye"],
            "responses": [
              "See you later, thanks for visiting",
              "Have a nice day",
              "Bye! Come back again soon."
            ]
          },
          {
            "tag": "thanks",
            "patterns": ["Thanks", "Thank you", "That's helpful", "Thank's a lot!"],
            "responses": ["Happy to help!", "Any time!", "My pleasure"]
          },
          {
            "tag": "funny",
            "patterns": [
              "Tell me a joke!",
              "Tell me something funny!",
              "Do you know a joke?"
            ],
            "responses": [
              "Why did the hipster burn his mouth? He drank the coffee before it was cool.",
              "What did the buffalo say when his son left for college? Bison."
            ]
          },
        {
            "tag": "machine_learning",
            "patterns": [
                "What are some common machine learning interview questions?",
                "What are the questions I can expect in Machine Learning interview specifically for my ML projects?",
                "What types of technical questions are most commonly asked in Senior Machine Learning Engineer interviews?",
                "What are some of the important machine learning questions/topics I should be prepared for in 2022 before attending an interview?",
                "What are some of the important machine learning questions/topics I should be prepared for in 2022 before attending an interview?",
                "What are some common machine learning interview questions?",
                "What are some common machine learning interview questions?",
                "What are some common machine learning interview questions?",
                "What are some common machine learning interview questions?",
                "What are some common machine learning interview questions?",
                "What are some common machine learning interview questions?",
                "What are some common machine learning interview questions?",
                "What types of technical questions are most commonly asked in Senior Machine Learning Engineer interviews?",
                "What are the best interview questions to evaluate a machine learning researcher?",
                "What are the best interview questions to evaluate a machine learning researcher?",
                "What are some common machine learning interview questions?",
                "What are machine learning questions for beginners?",
                "What are the Machine Learning topics and algorithms a beginner should know?",
                "Is machine learning really difficult?",
                "Should a machine learning beginner go straight for deep learning?",
                "How do I learn mathematics for machine learning?",
                "What are the courses on machine learning for a beginner?",
                "What are some common machine learning interview questions?",
                "What are some of the best interview questions in Machine Learning?",
                "What are the best interview questions to evaluate a machine learning researcher?",
                "What are the questions I can expect in Machine Learning interview specifically for my ML projects?",
                "What are the most important questions in machine learning?",
                "What are the next big questions being asked in machine learning in 2019?",
                "What are the next big questions being asked in machine learning in 2019?",
                "What are the most important questions in machine learning?",
                "What will likely be the biggest machine learning advances of 2018?",
                "What is the next big thing that comes after machine learning?",
                "What is the next hot topic in machine learning?",
                "What are the questions I can expect in Machine Learning interview specifically for my ML projects?",
                "What types of technical questions are most commonly asked in Senior Machine Learning Engineer interviews?",
                "How do I crack any machine learning interview? What kinds of questions should I expect? What types of relevant side projects would look good on a CV?",
                "What are some of the best interview questions in Machine Learning?",
                "What are some common machine learning interview questions?",
                "What are the best interview questions to evaluate a machine learning researcher?",
                "How do I crack any machine learning interview? What kinds of questions should I expect? What types of relevant side projects would look good on a CV?",
                "What kind of technical questions can I expect in a machine learning interview?",
                "What are the best interview questions to evaluate a machine learning researcher?",
                "If I am applying for machine learning jobs, what are the concepts I should mainly focus on? Also, where can I get a set of interview questions to prepare myself?",
                "What are the most interesting questions about machine learning asked on Quora?",
                "What are interesting topics in machine learning?",
                "Will an AI use Quora to ask questions and learn?",
                "How does Quora use machine learning in 2015?",
                "How is machine learning patterned through asking questions?",
                "What are some of the best interview questions in Machine Learning?",
                "How would you recommend to study machine learning (book, courses, etc.)?",
                "Which are the top 3 books that you would recommend in Machine Learning?",
                "What are the must-read papers on data mining and machine learning?",
                "What are some good books on machine learning?",
                "Which books are best for beginner in machine learning?",
                "What are the must-read papers on data mining and machine learning?",
                "What is the best algorithm book for a beginner in machine learning?",
                "Practical advice for machine learning?",
                "Which is the best book to get Started with Machine Learning in Python?",
                "What is the difference between Data Analytics, Data Analysis, Data Mining, Data Science, Machine Learning, and Big Data?",
                "What is the difference between Data Analytics, Data Analysis, Data Mining, Data Science, Machine Learning, and Big Data?",
                "What is the difference between Data Analytics, Data Analysis, Data Mining, Data Science, Machine Learning, and Big Data?",
                "What is the difference between Data Analytics, Data Analysis, Data Mining, Data Science, Machine Learning, and Big Data?",
                "What is the difference between Data Analytics, Data Analysis, Data Mining, Data Science, Machine Learning, and Big Data?",
                "How can I become a data scientist?",
                "What machine learning theory do I need to know in order to be a successful machine learning practitioner?",
                "What are the best talks/lectures related to big data/algorithms/machine learning?",
                "What are the best talks/lectures related to big data/algorithms/machine learning?",
                "What are the best talks/lectures related to big data/algorithms/machine learning?",
                "What are the best talks/lectures related to big data/algorithms/machine learning?",
                "What are the best talks/lectures related to big data/algorithms/machine learning?",
                "How do I learn big data technologies?",
                "What are some interesting things to do with Python? I want to make something related to big data or machine learning.",
                "What are some interesting things to do with Python? I want to make something related to big data or machine learning.",
                "Is machine learning really difficult?",
                "What should everyone know about machine learning?",
                "How do I learn mathematics for machine learning?",
                "How do I learn mathematics for machine learning?",
                "What math do you need to know to learn machine learning?",
                "How do I learn mathematics for machine learning?",
                "How can I become a data scientist?",
                "What machine learning theory do I need to know in order to be a successful machine learning practitioner?",
                "Why is machine learning regarded as the best career?",
                "What are the benefits of pursuing a career in machine learning?",
                "How should I start to learn machine learning from scratch, from the beginner to the advanced level?",
                "Is machine learning a made-up profession?",
                "How do I begin a career in Machine Learning in India? What are prospects of ML in India? Which OSS are better to work on to gain a good experience in ML?",
                "Is Machine Learning a good choice for a career?",
                "What exactly does a machine learning engineer do?",
                "Is machine learning a made-up profession?",
                "Why is machine learning regarded as the best career?",
                "What exactly does a machine learning engineer do?",
                "What is the job of a machine learning engineer?",
                "What are the benefits of pursuing a career in machine learning?",
                "What is the job of a machine learning engineer?",
                "What exactly does a machine learning engineer do?",
                "What do machine learning engineers do on a daily basis?",
                "What do mainly ML engineers do?",
                "What do machine learning engineers do?",
                "How does a typical day for someone working in Machine Learning look like?",
                "What does a machine learning engineer\u2019s work entail?",
                "What exactly does a machine learning engineer do?",
                "What are the tasks of a machine learning engineer?",
                "What is the job of a machine learning engineer?",
                "Can you explain what you did as a machine learning engineer?",
                "What is the job of a machine learning engineer?",
                "What exactly does a machine learning engineer do?",
                "What do machine learning engineers do on a daily basis?",
                "What do mainly ML engineers do?",
                "What do machine learning engineers do?",
                "How does a typical day for someone working in Machine Learning look like?",
                "What does a machine learning engineer\u2019s work entail?",
                "How does a typical day for someone working in Machine Learning look like?",
                "What is the job of a machine learning engineer?",
                "What exactly does a machine learning engineer do?",
                "How does a typical day for someone working in Machine Learning look like?",
                "How can one become a good machine learning engineer?",
                "How can one become a good machine learning engineer?",
                "How much time does it take to become a machine learning engineer?",
                "To be recruited as a machine learning engineer, one has to have at least 3 years of experience. How do I gain experience in machine learning as a fresher?",
                "I am a mechanical engineering student. How can I learn machine learning? Where should I get started?",
                "Can I learn and get a job in Machine Learning without studying CS Master and PhD?",
                "How much time does it take to become a machine learning engineer?",
                "Is it possible to land a machine learning engineer job without a college degree?",
                "Can I learn and get a job in Machine Learning without studying CS Master and PhD?",
                "How can I switch career to Machine Learning?",
                "What's a machine learning career like?",
                "What are the best companies to work for as a machine learning engineer?",
                "What is the first thing to do if I want to start a career in AI and machine learning?",
                "How can one become a good machine learning engineer?",
                "What is the difference between machine learning and artificial intelligence?",
                "Will getting a degree in CS help me to get into machine learning and AI?",
                "Is it possible for someone who has no CS degree to learn machine learning and get a job?",
                "Can I learn and get a job in Machine Learning without studying CS Master and PhD?",
                "I absolutely love artificial intelligence but am not good at math and average in coding, should I continue?",
                "What's the next step after getting a bachelor's degree in Computer Science if I want to do a research in Artificial Intelligence and Machine Learning?",
                "Can I learn and get a job in Machine Learning without studying CS Master and PhD?",
                "Which degree should I choose if I want to work in AI: computer science, software engineering or computer engineering?",
                "What's the easiest way to learn machine learning?",
                "How do I learn mathematics for machine learning?",
                "How should I start to learn machine learning from scratch, from the beginner to the advanced level?",
                "What math do you need to know to learn machine learning?",
                "What is the usual way to start learning Machine learning?",
                "Is machine learning really difficult?",
                "How do I begin a career in Machine Learning in India? What are prospects of ML in India? Which OSS are better to work on to gain a good experience in ML?",
                "How does a total beginner start to learn machine learning if they have some knowledge of programming languages?",
                "What is machine learning?",
                "What is machine learning?",
                "How does machine learning work?",
                "What is machine learning?",
                "What is machine learning?",
                "What is machine learning?",
                "What is machine learning for?",
                "How does machine learning work?",
                "How does machine learning work?",
                "What math do you need to know to learn machine learning?",
                "What is the brutal truth about machine learning?",
                "How do I learn mathematics for machine learning?",
                "What is the brutal truth about machine learning?",
                "Which maths topics should I learn before learning data science?",
                "How can I become a data scientist?",
                "What are the key skills of a data scientist?",
                "What are the key skills of a data scientist?",
                "How can I become a data scientist?",
                "How can I start learning data science and become a master in it?",
                "What are the key skills of a data scientist?",
                "How can I become a data scientist?",
                "What new data science skills did you learn in the past week?",
                "What are the key skills of a data scientist?",
                "How can I become a data scientist?",
                "What are the most marketable skills in the field of Data, Analysis, and Data Science?",
                "Why do you think data science is going to be so important?",
                "How can I become a data scientist?",
                "What are the skills needed for a data scientist job?",
                "Can I become a self-taught data scientist?",
                "What are the skills needed for a data scientist job?",
                "Which companies are considered dream companies for data scientists and data engineers to work for?",
                "What should you do if you are the first data scientist in a company?",
                "How can I become a data scientist?",
                "What is a data scientist?",
                "How can I become a data scientist?",
                "Are you a self-made data scientist? How did you do it?",
                "What are the key skills of a data scientist?",
                "How can I become a data scientist?",
                "Which of these careers is less stressful: Data scientist, Data engineer or software engineer? Why?",
                "What is a data scientist?",
                "Can I get job as a data scientist, being a fresh graduate?",
                "What are the benefits of pursuing a career in machine learning?",
                "How can I become a data scientist?",
                "Which of these careers is less stressful: Data scientist, Data engineer or software engineer? Why?",
                "What do the top 1% of software engineers do that the other 99% do not?",
                "What do the top 1% of software engineers do that the other 99% do not?",
                "What are some interesting things to do with Python? I want to make something related to big data or machine learning.",
                "What are some interesting things to do with Python? I want to make something related to big data or machine learning.",
                "What are the steps for learning python?",
                "How do I learn Python?",
                "How do I learn Python?",
                "How do I learn Python?",
                "What are the best tips for learning Python within one month?",
                "Can I learn Python in a month?",
                "What are the best tips for learning Python within one month?",
                "What are the best tips for learning Python within one month?",
                "In how much time can I learn Python?"
            ],
            "responses": [
                "When I am asked to interview people, I try to ascertain whether they know the math or not, and how to apply it  in a real world context.  I also look to see if they understand high performance computing and not just vanilla coding.  \n\nI was asked to do this as a consultant, acting as a subject matter expert to help interview junior people for the firm.  \n\nIn our interviews, we asked a candidate to present some code they had written and to talk through it.   For an ML person, it would be some kind of ML code.\n\nSo, for example, I was involved with an interview with a Physics PhD from MIT discussing some NMF code he wrote in javascript.   The javascript was very good and he would be fine doing GUI work , Node.JS work, etc.  Certainly not something I could do.\n\nCan he do Machine Learning.  Mind you, he has a PhD in a math heavy subject from one of the top 10 schools in the world.  So he should know the math.\n\nI wanted to see if he knew how to get it to converge properly.  He did not.     He knew it was non-convex, but he did not know how to seed it, nor did he know about the convex variants. He tried to give me some nonsense about it being Bi-convex and whatnot. Dude, just use Kmeans++  to seed it.  Thats it.  Thats all you had to say.   This got totally past the VP of engineering and the CTO.    (They were just impressed that machine learning involved computing a first order derivative--something neither had since since college calculus)\n\nSo here, he knew some basic methods, but did not really know the most important ideas in the field, the important developments,  how to really code this.  It is clear that he had never done anything like this in his former work, nor did he really understand numerical methods.\n\nThis means that his solution would never work in production and -- more importantly -- that he would have no idea how to evaluate it or how to  fix it.  I see this a lot.   Also, he did not know the available open source codes, how they worked internally, and  which one to use,  or how to evaluate their performance.  For being a PhD from MIT, this was unacceptable to me.\n\nThere was also a code evaluation.  For me, one needs to know what runs fast and what does not. What good is a method that only runs on 300 data points?!     In this case, this interviewee had written his own javascript matrix library.  Did he know the BLAS libraries and how they work?  Or an alternative?     This is critical because you can't run anything in production if the code is too slow.   I see the same problem with most ruby coders--they just don't know numerical computing.    \n\nI was not looking to evaluate 10,000 of complex code , whether he used Agile or Unit Testing.  Nor did I care about solving some high school brain teaser. I just wanted to see a small piece of code, with good engineering choices ,   a good understanding of the math, and how to make this solution work in a modest production environment . \n\nId rather see old fashioned spectral clustering with a  Fortran library, which can scale, as opposed to trying to use a \"fancy\" method like NMF or LDA if you can not get it to work in production at scale.  (I'm not saying they don't scale--I am saying you better know how to get them to scale if you choose to use them)\n\nIn another interview, again a PhD (Ukrainian I think) who was very bright and had solved some good problems and had experience.  He was using an off-the-shelf SVM tool--a tool I know very well.   I asked a very basic question--how do you adjust the cost parameter for the SVM regularizer.   I rephrased the question a couple of different ways to give him a chance.    FAIL   In other words, did you read the documentation of the tool and did you understand which parameters to tweak and which ones to leave at the default settings  ( I kinda would like the person to have read the entire source code of the tool and know how it works. )   Again, this demonstrates a failure of the most basic mathematical concepts in ML -- Regularization-- and how they would apply in production.  Tuning this parameter can increase accuracy by %10-15 (or more).  Again, just simple stuff--but important stuff  This also shows a lack of attention to notice the important details of the work.  We actually offered this guy the job and he asked for a salary way out of the ballpark.  If he had not missed this critical question he might have been able to make the case for the salary.\n\n\nHaving shared all this, I would add that I think , for you, the market is very good and you will probably not encounter anything like this.  Why?  All you need to do is know more machine learning than the VP and the CTO--and here the bar is very low.   Everyone and his brother has a funding to do machine learning and they usually just need to solve one small problem and get the product out the door.  Most  (i.e 7/10 ) CTOs and VPs know absolutely nothing about even basic   machine learning  so they have no clue  even what to  ask. (Newton Raphson will blow them away, and they will think you are too expensive if you try compare stochastic gradient descent to interior point methods)  They got their start up funded based on the market potential of  the idea, and they are expected to hire people to invent their IP. \n\n  (Obviously if you are interviewing at Google or Lockheed Martin, disregard all of this and hire me once you get in)\n\nP.S.   I was asked once by some VP/CTO evaluating me what the volume of a rectangular prism is.  AlI could think of was this old Pink Floyd album Dark Side of the Moon  with the Prism on it \n\nhttp://en.wikipedia.org/wiki/The_Dark_Side_of_the_Moon\n\nI would never ask this kind of question but you will probably get asked many   puzzle questions like this if you are fresh out of school (or an old man like me I guess)  I seem to recall there are books and/or web sites with tons of these.\n\n\n  Good Luck",
                "When asking about a project (permitting time, sometimes i cut things short) I take the following route:\n\nWhat did you do? Why did you do it? Why did you define the project as you did? How did you evaluate it? What is the new part? How is this different (ensure basic understanding of prior work)\n\nWhat did you do to the data? Why? How did you set it up? feature engineering? What topology did you use? Why? What else did you try? What didn\u2019t work? Why? What tools did you use?\n\nWhat else can/should be done? What did you do wrong? Not only mistakes which wasted time, but in the final setup what is wrong/cheating/definitely not the best way?\n\nI will also randomly stop on a key word or technical term and ask explaining, For example if you use drop-out in your neural network I may pause and ask for an explanation on drop out.",
                "Every place I\u2019ve interviewed with or worked at had a similar process.\n\nAs an aside, Microsoft uses me as an interview resource. Internal Microsoft employees aren\u2019t allowed to set up or participate in interviews outside of Microsoft so when a client asks them, they\u2019ll use outside resources.\n\nHere\u2019s the basic process.\n\nThey have a phone screen to ascertain your technical acumen.\n\nOur phone screen is all data. Here\u2019s a short list to prepare you for the phone screen. [ https://www.machinelearningmike.com/post/phone-screen-preparation ]\n\nAll of our models are on structured data and we use BigQuery so if you don\u2019t have strong data skills, we end the phone screen. Most companies do the same thing. We actually won\u2019t interview anyone without 3\u20135 years of SQL in a real-world setting. Teaching data skills is far more difficult than Python or modeling basics.\n\nSo, we start easy.\n\nWhat are the two types of SQL?\n\nWhat\u2019s a relational database?\n\nWhat\u2019s a table?\n\nWhat the difference between structured data and unstructured data?\n\nHow do you create a table?\n\nIn the phone screen, we also throw in a few Python questions. Easy shit\u2026 like what does a single equals sign mean? What\u2019s an equality operator? What\u2019s a model called in SciKit-Learn.\n\nIf you make it past the phone screen you have your first in-person.\n\nThis is where we dive into machine learning. Again, starting off simple like\u2026 can you whiteboard out the machine learning pipeline?\n\nCan you draw a simple ANN? What\u2019s the difference between deep learning and an ANN?\n\nAfter the candidate makes it past these it\u2019s on to coding questions in Python, Pandas, SciKit-Learn.\n\nThen basic stats questions. All applied questions.\n\nThen I\u2019ll put up Jupyter Notebook with an end to end pipeline and have them walk me through each line of code.\n\nLike\u2026\n\nimport pandas as pd %3CTell me what every word means.\n\nThe word import means to \u201cbring in\u201d\n\nPandas is a library for data manipulation.\n\nas pd is an alias. An alias is used so you don\u2019t have to write out the entire library during the call.\n\nWe have about 100 questions. It\u2019s really important to understand, companies aren\u2019t going to pay you 200K to learn on the job.",
                "Here are a few things to keep in mind:\n\n\u2022 Simple descriptions of the goals shared by most algorithms (decision trees: recursive partitioning of the data to get homogenous subsets)\n\n\u2022 How to identify and deal with the most prevalent issues (e.g., overfitting, imbalance, missing values)\n\n\u2022 How to make a decision and where to find them (e.g., selecting a threshold for a binary classification problem to get the right balance between false positives and false negatives or maybe based on base rate)\n\n\u2022 It is possible to use a wide range of performance metrics (e.g., accuracy, precision, recall, F1, R2, RMSE, etc.)",
                "A real-world warning first. If you don\u2019t have any real-world experience you have ZERO chance of getting any job in machine learning. Sorry. Just how it is right now.\n\nIf you\u2019re fresh out of college then you\u2019re really in trouble. Either way, good luck and watch this video.\n\nhttps://youtu.be/xKYDdk4r88w\nLook at the job description.\n\nYou\u2019ll know exactly what the questions will be.\n\nYou see the section called required qualifications?\n\nThere\u2019s where you start interviewing.\n\n",
                "Amazon came to our campus to offer Data Science internships last October. After clearing the Machine Learning interview and shortlisting coding round, I was offered a data science internship of 5 months duration. Apart from that, I had applied at Walmart Labs off-campus for the position of Data Scientist and fortunately, was able to convert it.\n\nSome of the questions that I have been asked in the ML interview for above two companies are:\n\n1. Explain Linear and Logistic Regression. List their assumptions. Why cannot we use Linear Regression on categorical output?\n2. Explain Bias-Variance Tradeoff. Explain underfitting and overfitting. What is the need for regularization?\n3. Explain variants of Gradient Descent and the pros and cons of each variant.\n4. Difference between Bagging and Boosting. Explain Random Forest.\n5. Explain Precision and Recall measures and give examples of use cases where each is measured.\n6. Briefly discuss some dimension reduction techniques. Difference between PCA and SVD.\n7. Explain ROC Curve. What do the axes of the ROC Curve represent? Elaborate on the two extreme points of the ROC Curve \u2013 (0, 0) and (1, 1).\n8. Explain AUC and its physical interpretation? Is it possible to get AUC below 0.5? What is the worst AUC that you can possibly achieve?\n9. Explain the problem of vanishing and exploding gradients. Briefly describe some methods to solve these.\n10. What is the need for a pooling layer in CNNs? Difference between max pooling and average pooling.\n11. Explain how will you forecast a time series? Can we perform Linear Regression on time-series data?\n12. What is Central Limit Theorem? Give an example where it is used.\n13. Why is hyperparameter tuning required? Elaborate on some common hyperparameters for tree-based models.\n14. Briefly discuss some clustering methods. What are the drawbacks of K-Means Clustering?\n15. Differentiate between generative and discriminative models and give examples of each.\nAnother thing to note is that I wasn\u2019t asked questions specifically on my academic ML projects that I did during the course during my interviews in the Walmart process. I was briefly asked about my work at Amazon at the end of 2nd round and about my projects in Computer Vision in the 3rd round. Although some companies do discuss projects done, I think companies like Amazon, Microsoft have their own way of assessing the candidates.",
                "When I am asked to interview people, I try to ascertain whether they know the math or not, and how to apply it  in a real world context.  I also look to see if they understand high performance computing and not just vanilla coding.  \n\nI was asked to do this as a consultant, acting as a subject matter expert to help interview junior people for the firm.  \n\nIn our interviews, we asked a candidate to present some code they had written and to talk through it.   For an ML person, it would be some kind of ML code.\n\nSo, for example, I was involved with an interview with a Physics PhD from MIT discussing some NMF code he wrote in javascript.   The javascript was very good and he would be fine doing GUI work , Node.JS work, etc.  Certainly not something I could do.\n\nCan he do Machine Learning.  Mind you, he has a PhD in a math heavy subject from one of the top 10 schools in the world.  So he should know the math.\n\nI wanted to see if he knew how to get it to converge properly.  He did not.     He knew it was non-convex, but he did not know how to seed it, nor did he know about the convex variants. He tried to give me some nonsense about it being Bi-convex and whatnot. Dude, just use Kmeans++  to seed it.  Thats it.  Thats all you had to say.   This got totally past the VP of engineering and the CTO.    (They were just impressed that machine learning involved computing a first order derivative--something neither had since since college calculus)\n\nSo here, he knew some basic methods, but did not really know the most important ideas in the field, the important developments,  how to really code this.  It is clear that he had never done anything like this in his former work, nor did he really understand numerical methods.\n\nThis means that his solution would never work in production and -- more importantly -- that he would have no idea how to evaluate it or how to  fix it.  I see this a lot.   Also, he did not know the available open source codes, how they worked internally, and  which one to use,  or how to evaluate their performance.  For being a PhD from MIT, this was unacceptable to me.\n\nThere was also a code evaluation.  For me, one needs to know what runs fast and what does not. What good is a method that only runs on 300 data points?!     In this case, this interviewee had written his own javascript matrix library.  Did he know the BLAS libraries and how they work?  Or an alternative?     This is critical because you can't run anything in production if the code is too slow.   I see the same problem with most ruby coders--they just don't know numerical computing.    \n\nI was not looking to evaluate 10,000 of complex code , whether he used Agile or Unit Testing.  Nor did I care about solving some high school brain teaser. I just wanted to see a small piece of code, with good engineering choices ,   a good understanding of the math, and how to make this solution work in a modest production environment . \n\nId rather see old fashioned spectral clustering with a  Fortran library, which can scale, as opposed to trying to use a \"fancy\" method like NMF or LDA if you can not get it to work in production at scale.  (I'm not saying they don't scale--I am saying you better know how to get them to scale if you choose to use them)\n\nIn another interview, again a PhD (Ukrainian I think) who was very bright and had solved some good problems and had experience.  He was using an off-the-shelf SVM tool--a tool I know very well.   I asked a very basic question--how do you adjust the cost parameter for the SVM regularizer.   I rephrased the question a couple of different ways to give him a chance.    FAIL   In other words, did you read the documentation of the tool and did you understand which parameters to tweak and which ones to leave at the default settings  ( I kinda would like the person to have read the entire source code of the tool and know how it works. )   Again, this demonstrates a failure of the most basic mathematical concepts in ML -- Regularization-- and how they would apply in production.  Tuning this parameter can increase accuracy by %10-15 (or more).  Again, just simple stuff--but important stuff  This also shows a lack of attention to notice the important details of the work.  We actually offered this guy the job and he asked for a salary way out of the ballpark.  If he had not missed this critical question he might have been able to make the case for the salary.\n\n\nHaving shared all this, I would add that I think , for you, the market is very good and you will probably not encounter anything like this.  Why?  All you need to do is know more machine learning than the VP and the CTO--and here the bar is very low.   Everyone and his brother has a funding to do machine learning and they usually just need to solve one small problem and get the product out the door.  Most  (i.e 7/10 ) CTOs and VPs know absolutely nothing about even basic   machine learning  so they have no clue  even what to  ask. (Newton Raphson will blow them away, and they will think you are too expensive if you try compare stochastic gradient descent to interior point methods)  They got their start up funded based on the market potential of  the idea, and they are expected to hire people to invent their IP. \n\n  (Obviously if you are interviewing at Google or Lockheed Martin, disregard all of this and hire me once you get in)\n\nP.S.   I was asked once by some VP/CTO evaluating me what the volume of a rectangular prism is.  AlI could think of was this old Pink Floyd album Dark Side of the Moon  with the Prism on it \n\nhttp://en.wikipedia.org/wiki/The_Dark_Side_of_the_Moon\n\nI would never ask this kind of question but you will probably get asked many   puzzle questions like this if you are fresh out of school (or an old man like me I guess)  I seem to recall there are books and/or web sites with tons of these.\n\n\n  Good Luck",
                "Machine learning interview questions usually involve bringing up a real-world problem and asking the candidate how to solve it. The type of the question varies a lot depending on the field of the company.\n\nAn image processing company may ask you \u201chow can you find all the images which are a photo of a landscape?\u201d. A video processing company may ask you \u201cIn a video of a soccer match, how can you mark all the times that a certain player is in the view?\u201d. A speech processing company may ask you \u201cAmong a large number of voicemails, how can you detect the ones that an old woman is talking?\u201d. An NLP company might ask you \u201cHow would you provide suggestions for the next word in an incomplete sentence?\u201d. An online shopping company may ask you \u201chow would you store products in fixed-size bins where people are most likely to buy them together?\u201d.\n\nThere are so many use cases and many of these types of questions which may be asked. I suggest that before the interview you read about a few case studies which relates to the company\u2019s business. For example if it\u2019s an image processing company, read about popular approaches for image detection, identification, segmentation, tracking, clustering, etc.\n\nAlso, there are a few things which you are expected to know and are shared among almost all use cases. You should have ideas how to approach scenarios like this:\n\n * How to use labeled and unlabeled data?\n * What if you don\u2019t have any labeled data?\n * What if your data set is skewed (e.g. 99.99 % positive and 0.01% negative labels)?\n * How to test and know whether or not we have overfitting problem?\n * How to avoid overfitting?\n * How to make training faster?\n * How to make predictions faster?\nAlso, It\u2019s very common in machine learning interviews that they ask you about a previous machine learning work that you have done in details. You need to be able to explain what challenges you were facing and how you solved them.\n\nAt the end, I\u2019d like to introduce our incredible service which may be helpful for people who are preparing for machine learning interviews: It\u2019s www.techmockinterview.com . We launched this service a few months ago and it\u2019s been very well-received. You will get a mock interview experience just like the real interview. A senior machine learning engineer interviews you and provides you with feedback and the areas which you need to work on them.",
                "I conduct a lot of basic phone screens in this area, but I'm not a ML expert (by ANY stretch of the imagination).  These Q&As I've listed are helpful to me as a recruiter.\n\nQuestion: What areas of machine learning are you most familiar with? \n\nAnswers:\n\n * supervised learning\n * unsupervised learning\n * anomaly detection\n * active learning\n * bandits \n * gaussian processes\n * kernel methods\n * deep networks \nQuestion: What sort of optimization problem would you be solving to train a support vector machine?\n\nAnswers: maximize margin (best answer), quadratic program, quadratic with linear constraints, reference to solving the primal or dual form.\n\nQuestion: Tell me about positives and negatives of using Gaussian processes / general kernel methods approach to learning.\n\nAnswer: Positives - non-linear, non-parametric. Negatives - bad scaling with instances, need to do hyper-parameter tuning\n\nQuestion: How does a kernel method scale with the number of instances (e.g. with a Gaussian rbf kernel)?\n\nAnswer: Quadratic (referring to construction of the gram (kernel) matrix), cubic (referring to the matrix inversion)\n\nQuestion: Describe ways to overcome scaling issues.\n\nAnswers: nystrom methods/low-rank kernel matrix approximations, random features, local by query/near neighbors\n\nQuestion: What are some tools for parallelizing machine learning algorithms?\n\nAnswers: GPUs, Matlab parfor, write your own using low level primitives/RPC/MPI, mapreduce, spark, vowpal, graphlab, giraph, petuum, parameterserver\n\nQuestion: In Python, do you have a favorite/least favorite PEP?\n\nAnswer: Peps are python enhancement proposal. If you have a favorite or least favorite, it means they have knowledge of Python. \n\nHope this little snippet of Q&A helps you.",
                "During our interviews, we prepared for some questions and here is the list for the same that I feel are important:\n\nStatistics:\n\n1. What is the Central Limit Theorem and why is it important?\n\n2. What is the difference between type I vs type II error?\n\n3. What is linear regression?\n\n4. What do the terms p-value, coefficient, and r-squared value mean? What is the significance of each of these components?\n\n5. What is selection bias?\n\n6. What is the Binomial Probability Formula?\n\n7. What do you understand by the term Normal Distribution?\n\n8. What is correlation and covariance in statistics?\n\n9. What is the goal of A/B Testing?\n\nData Science:\n\n1. What is your understanding of Data Science?\n\n2. List the differences between supervised and unsupervised learning?\n\n3. What is the bias-variance trade-off?\n\n4. How is KNN different from k-means clustering?\n\n5. What is a confusion matrix?\n\n6. Explain how a ROC curve works.\n\n7. What is Bayes\u2019 Theorem? How is it useful in a machine learning context?\n\n8. What is Naive Bayes's theorem?\n\n9. What are the differences between over-fitting and under-fitting?\n\n10. What is Cluster Sampling?\n\nMACHINE LEARNING:\n\n1. What is Machine Learning?\n2. What are the various classification algorithms?\n3. What is \u2018Naive\u2019 in a Naive Bayes?\n4. Explain SVM algorithm in detail.\n5. What are the different kernels in SVM?\n6. What is Decision Tree?\n7. What are Entropy and Information gain in the Decision tree algorithm?\n8. What is logistic regression? State an example when you have used logistic regression recently\n9. What Are the Drawbacks of the Linear Model?\n10. What is the difference between Regression and classification of ML techniques?\n11. What are Recommender Systems?\n12. How can outlier values be treated?\n13. What is a Random Forest? How does it work?\nI hope these questions will help you. All The Best",
                "You need to be really clear about the coding language and the packages you are using. For example, if you are using Python, you should remember which version have you used. Which Python IDE and environment you are using and the open-source libraries like Keras or PyTorch.\n\nA lot depends on the job description given by the company. If the interview call is for a generic Machine Learning Engineer then mostly your basics will be tested. Some of the sample questions are:\n\n1. What is bias and variance?\n2. Difference between unsupervised and supervised learning?\n3. Define accuracy and validation loss?\n4. Define optimizers? Name some of them?\n5. Difference between L1 and L2 regularization?\n6. What does F1 score imply?\n7. Difference between overfitting and underfitting?\n8. Talk about a ML project you have recently worked on\nIf the position is specifically Deep Learning Engineer, you will be tested more on your deep neural network knowledge. In addition to the above-mentioned questions, here are some sample questions:\n\n1. How do you eliminate overfitting/underfitting?\n2. Which mathematical operation is mainly used when image is passed through the DNN layers?\n3. Explain one tactic to increase accuracy and one tactic to reduce the loss?\n4. How do you reduce model size without affecting the accuracy much? Explain one method to do so (Pruning/Deep compression/Design Space Exploration)?\n5. What is model speed?\n6. Name an activation function better than ReLU?\nIf the position is specifically Junior Data Scientist, you may be asked about k-NN. Logistic Regression, PCA, K-means Clustering, etc.\n\nIf Computer Vision is also required along with Machine Learning for the job role, the most common topics are Object recognition, Object detection, Image classification, Semantic segmentation, YOLO, R-CNN.",
                "Machine Learning interviews are more logic-oriented than concept-based. The interviewer doesn\u2019t want a specific answer from you but he wants to know how strong your logical thinking is. You would be asked to explain your thought process, your observations and suggestions.\n\nInstead of asking you different concepts, the interviewer will generally focus on one algorithm or language and go deeper into it. So, you should be prepared to convey your thoughts properly and demonstrate the skills mentioned in the Job Description.\n\nIn my 6 years of experience as an AI/ML Architect, I have realized that these are the most important questions you should prepare for:\n\nGeneral Questions:\n\n * What is supervised learning?\n * What according to you is the most important ML skill?\n * How is ML connected to AI and Deep Learning?\n * What are some methods to handle missing data?\nML Specific Questions:\n\n * Explain the working principles of common ML algorithms like linear and logistic regression, decision trees, and k-nearest neighbors.\n * What are common evaluation metrics for classification problems (e.g., accuracy, precision, recall, F1-score)?\n * What are the common techniques in unsupervised learning (e.g., clustering, dimensionality reduction)?\n * What are the activation functions in deep learning?\n * What are the ethical considerations in machine learning?\n * How would you design an ad prediction system to show relevant ads to users?\nProject-based questions:\n\n * Please elaborate on the dataset [data pipeline] you used in this specific project.\n * Do you think this is the most optimized approach for the problem?\n * If you could change one thing in this project, what would it be?\nRemember, the interviewer will test your answer later and how you get to the answer at first. Try appearing for mock interviews to evaluate your thought process and appear confident in real-life interviews.\n\nIf you are preparing for interviews, I recommend starting your preparation from the beginning. In addition, you can connect with me to receive personalized guidance from Day 1. I can help you understand everything from scratch, including preparing the right questions and practicing\n\nthem through continuous mock interviews, as well as applying for jobs.\n\nYou can connect with me for a 1:1 Preplaced [ https://www.quora.com/profile/Preplaced-1 ] session here - Explore Mentor Ekta Shah Profile | Preplaced [ https://www.preplaced.in/profile/ekta-shah-9 ]\n\nP.S.: Always consider your case studies and model projects when answering questions. \u2728\n\n",
                "Every place I\u2019ve interviewed with or worked at had a similar process.\n\nAs an aside, Microsoft uses me as an interview resource. Internal Microsoft employees aren\u2019t allowed to set up or participate in interviews outside of Microsoft so when a client asks them, they\u2019ll use outside resources.\n\nHere\u2019s the basic process.\n\nThey have a phone screen to ascertain your technical acumen.\n\nOur phone screen is all data. Here\u2019s a short list to prepare you for the phone screen. [ https://www.machinelearningmike.com/post/phone-screen-preparation ]\n\nAll of our models are on structured data and we use BigQuery so if you don\u2019t have strong data skills, we end the phone screen. Most companies do the same thing. We actually won\u2019t interview anyone without 3\u20135 years of SQL in a real-world setting. Teaching data skills is far more difficult than Python or modeling basics.\n\nSo, we start easy.\n\nWhat are the two types of SQL?\n\nWhat\u2019s a relational database?\n\nWhat\u2019s a table?\n\nWhat the difference between structured data and unstructured data?\n\nHow do you create a table?\n\nIn the phone screen, we also throw in a few Python questions. Easy shit\u2026 like what does a single equals sign mean? What\u2019s an equality operator? What\u2019s a model called in SciKit-Learn.\n\nIf you make it past the phone screen you have your first in-person.\n\nThis is where we dive into machine learning. Again, starting off simple like\u2026 can you whiteboard out the machine learning pipeline?\n\nCan you draw a simple ANN? What\u2019s the difference between deep learning and an ANN?\n\nAfter the candidate makes it past these it\u2019s on to coding questions in Python, Pandas, SciKit-Learn.\n\nThen basic stats questions. All applied questions.\n\nThen I\u2019ll put up Jupyter Notebook with an end to end pipeline and have them walk me through each line of code.\n\nLike\u2026\n\nimport pandas as pd %3CTell me what every word means.\n\nThe word import means to \u201cbring in\u201d\n\nPandas is a library for data manipulation.\n\nas pd is an alias. An alias is used so you don\u2019t have to write out the entire library during the call.\n\nWe have about 100 questions. It\u2019s really important to understand, companies aren\u2019t going to pay you 200K to learn on the job.",
                "I ask them to attack some common machine learning problems from the web domain that most machine learning researchers should have heard of.  It can be hard to bring people up to speed on your problem domain during a short interview, so I try to ask more familiar questions like \"How would you approach the Netflix Prize?\", \"How would you generate related searches on Bing?\", \"How would you suggest followers on Twitter?\".\n\nI ask them to describe a \"Rolls Royce\" solution that you could implement in 3 months, then a version you could implement in 3 weeks, and another version you could implement if you only had 3 days.  This tells you how adaptable they are and if they can come up with pragmatic approaches.\n\nMost of the things we build need to scale, so I probe them about how their approach would work on large amounts of data.  MapReduce based solutions are nice to see when appropriate.\n\nI also ask some theory questions like Josh suggested along with a few real world ML challenges we face to see if they have any creative solutions.  Since my group is very product focused and team members are self directed, I also try to get a sense of whether the candidate can come up with new products themselves that will make a difference for the company and our users.\n\n** Another type of question I often ask involves finding or collecting your own training data.  Most applicants have only worked with toy datasets or data where the labeled examples were provided to them.  I pose a typical machine learning problem they might face, and ask how they would quickly gather additional data to help solve it.",
                "I try to ask very basic but fundamental questions to see if they actually understand the math\n\n I would expect everyone to know how to solve\n\nAx=b\n\nAnd to know the difference between a convex and non-convex solution.\n\n\nFinding any reasonable candidate is quite hard, and the real goal is to disqualify people.  This is usually enough to weed out %90 of the posers.\n\nAlso, there are lots of ways to solve the same problem, and ML is a big field.\nKeeping it simple and basic is important. \n\nAt this point, it might be useful to ask about regularization.  It's enough to understand that A^{-1} is ill conditioned; although that would weed out %99 of the candidates, and no one could fill the role.  \n\nI generally don't ask questions like \"implement logistic regression\" unless the job actual involved writing custom solvers since doing this well is hard, and implementing a simple solution is pointless  counterproductive. (I have seen new hires try this and fall flat on their a$$).  \n\n I would rather they know how that running NMF will bomb out if you don't seed it properly, how to adjust the cost parameter on liblinear , or just knowing what Newton's method is  \n\n---\n\n Note that the real key to evaluate a PhD level researcher, however, is to determine if they are capable of doing independent research (if the job actually entails this), or if they just did what their advisor told them to do.   Here, one wants to see if they have a broad knowledge of the own field and how creative their PhD research was.  This is not, however, necessarily part of the interview process but more part of the screening process.\n\n When I evaluate a more senior person, especially an older PhD,  I use the old University of Chicago whiteboard method, meaning that anything someone brings up, they had better be able to go to the whiteboard (or chalkboard, in the old days) and work through the math.  This is absolutely critical, because it is necessary to see if you have kept yours math skills up and also to see if think using language or you think using math.   Thinking in math is hard and requires you maintain the skill.\n\nyou can get more of an idea of what I expect by looking at my blog\nhttp://charlesmartin14.wordpress.com/",
                "I think that your interviewers will want to see some experience in ML.\nI guess that this is your first job in this domain so I would suggest working on a small-medium independent open source project which will be \"cool\" and based upon an ML algo'.\nSomething like a facebook app, mobile app, small web site.\nThis will give you that needed edge over other newbies.\nBy doing so, there is a good chance that they will spend some of the interview time by asking you about your project, which you will know well, instead of asking you more and more \"tricky\" questions..Good luck!",
                "If you are asking for interview questions then here is the list of them\n\n * What do you mean by machine learning\n * What is precision, recall & f1 score\n * Where do we get over-fitting in ML models.\n * What are projects you have done (Important one)\n * \n * Based on algorithm you have used they will ask details about it like\n * \n * Why this algorithm only?\n * Where did you get data from\n * Why that hyper-parameter?\n * What was the result\n * Any data visualisation techniques you have used?\n * On which basis you declared model fit for testing?\n * What is the concept of that algo\n\n * It is an endless cycle once you are into the project section. So it is better to take interviewer towards the projects. But most importantly clear all the concepts which occur into the project you show in resume.\n\n * Difference of Machine learning, deep learning & artificial intelligence.\n * Why machine learning & not deep learning (ML, DL can be interchanged).\n * Basics of python especially\n * \n * String operations\n * List comprehension\n * Numpy\n * Pandas\n\n * Or basics of other language in which company is doing ML\nI would suggest that you go for projects as many as you can & understand them better. That way you give confidence to your interviewer that you have worked on ML, DL & will learn on your own if given a hefty task. Hope it helps you.",
                "Hello,\n\nMachine learning (ML) is a type of artificial intelligence (AI) that allows software applications to become more accurate at predicting outcomes without being explicitly programmed to do so. Machine learning algorithms use historical data as input to predict new output values.\n\nNow talking about the topics & algorithms, a beginner should know are :\u2014\n\n1.) Supervised Learning\n\n * Regression :\u2014 Regression is a statistical method used in finance, investing, and other disciplines that attempts to determine the strength and character of the relationship between one dependent variable (usually denoted by Y) and a series of other variables (known as independent variables). We have different ML algorithms to implement Regression such as linear regression, multilinear regression etc.\n * Classification :\u2014 In statistics, classification is the problem of identifying which of a set of categories an observation, belongs to. Examples are assigning a given email to the \"spam\" or \"non-spam\" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient. Some of the most used classification algorithms you should know are, logistic regression, SVM, KNN, Decision trees etc.\n2.) Unsupervised Learning\n\n * Clustering :\u2014 Clustering or cluster analysis is a machine learning technique, which groups the unlabelled dataset. It can be defined as \"A way of grouping the data points into different clusters, consisting of similar data points. Clustering algorithms are DBSCAN, K-means, mean-shift etc.\n * Association :\u2014 Association rule learning is a type of unsupervised learning technique that checks for the dependency of one data item on another data item and maps accordingly so that it can be more profitable. It tries to find some interesting relations or associations among the variables of dataset. It is based on different rules to discover the interesting relations between variables in the database. Recommendation System is one of the example of association.\nBut believe me, when it comes to learning all these topics and algorithms, it is not a piece of cake. It requires a proper guidance & content to go through it. Therefore I am recommending you a course that personally helped me a lot to get all these concepts thoroughly. Well, the course is \"Machine learning course\" by [code ]Coding Ninjas[/code]. The course is taught by highly educated instructors & also provides teaching assistants those make sure that you doubt doesn't become a hurdle in your learning. Also the the course has some amazing projects that helped me to enhance my resume. Some other features of the course are that :\u2014\n\n1. The course is entirely built with a mindset of training a beginner to an expert in machine learning.\n2. The course not only focusses on the Coding aspect of Machine Learning, but also covers theoretical concepts of Machine Learning concepts like Regression, Classifiers and Decision Trees.\n3. Other than this there are almost 10 Projects that are embedded in the curriculum that that will not only help you to gain more understanding of the concepts but will also increase your problem solving abilities. The projects included are:\n4. \n1. Case Study - Indian Startups\n2. Zomato API\n3. Web Scraping : InstaBot\n4. Decision Tree Implementation\n5. Text Classification\n6. Cifar10\n7. Twitter Sentiment Analysis\n8. Image caption generator\n9. Music Note Generation\n\n5. The course also contains data structures and algorithms which will help you alot to crack coding interviews.\nAt the end, I will suggest you to atleast take a free trial of the course. And if you like the course, then purchase it.\n\nWell, That's all from my side. I hope i resolved your doubt. Please feel free to reach through comments in case of any other doubt. I will be happy to assist.\n\nHappy Learning !!",
                "No. I wrote my first ML program waaay back in 1982, before there was Internet, Google, GPU computing, laptops, cellphones, digital cameras, desktop PCs, heck before there was almost anything remotely resembling what you see in the tech world around you today.\n\nHow did I even discover the existence of such a field? Back then, to educate oneself, you read books. Of course, you had to either go to a library, or in my case, a quaint event called a book fair. I attended a large book fair in New Delhi, India\u2019s capital, and picked up this 800 page tome, a fairly massive affair. Why, I don\u2019t know. After all, I was at the Indian Institute of Technology, Kanpur, studying to become an electrical engineer.\n\nHofstadter\u2019s first book, and in my opinion, still his best, was an utter revelation to me. It opened up a whole new world of imagination of what deep links there are between art, music and abstract math, realized by the three central characters \u2014 Johann Sebastian Bach, Maurice Escher, Kurt G\u00f6del \u2014 and computing, including of course AI and ML.\n\nThe book featured an intriguing set of visual puzzles from a Russian researcher named Bongard, where the task is to discover a rule that separates the six figures on the left from those on the right. This is an elementary problem in ML called classification. It\u2019s analogous to distinguishing email from spam or deciding if an image contains a face. As humans, we classify sensory stimuli billions of times through our lives, and our very survival depends on it. As you cross the road, is the object approaching you a person or a Fedex truck? Get the answer wrong and your life may indeed be over. Not surprisingly, we solve such problems amazingly well.\n\nI\u2019ll leave you to work this one out, but with absolutely no training in this field, I nonetheless decided to foolishly make this the core of my Masters thesis project. Somehow I plodded through and worked out a solution, however naively it seems in retrospect. That experience made me realize that AI and ML was my life\u2019s goal, and I decided to come to the United States in 1983, where I was incredibly fortunate to work with this brilliant Stanford educated researcher, Thomas Mitchell, now the Dean of the School of Computer Science at Carnegie Mellon University.\n\nFrom Tom, I learned the most important lesson of all, which no book can teach you. Research is *fun*. He simply embodied the spirit of a researcher who bubbled with enthusiasm for the field of ML. He worked harder than anyone I had met, yet seemed to be having a ball. That lesson made a huge imprint on me and stayed with me ever since.\n\nAfter getting my PhD, I joined IBM Watson Research in New York in late 1989, where they couldn\u2019t figure out what to do with an ML researcher. So they threw me into a newly formed robotics group, even though I had absolutely no background in this field. I had never ever programmed a robot. Amazingly enough, I seemed to thrive in this somewhat challenging situation, and ended up writing some of my most highly cited papers, exploring how robots can acquire new behaviors using the newly emerging field of reinforcement learning. I also published in 1993 perhaps the first book on robot learning, which featured research from all over the world in this new area of AI. Despite having no background in robotics, I still managed to make a name for myself.\n\nMany years later, I was elected a Fellow of AAAI, the leading international professional society for AI researchers. Each year, a small handful of researchers are selected and the competition is fierce. This year\u2019s AAAI Fellows include some of the founders of the deep learning revolution: Yoshua Bengio, and Yann Le Cun.\n\nThe list of AAAI Fellows include some of the most amazing researchers in AI and ML, and I\u2019m humbled to be listed in such distinguished company. None of this would have happened if back in 1982, I thought doing ML with no formal training in this field, with primitive computing, or doing robot learning at IBM in 1990 with no training in robotics, was \u201cdifficult\u201d.\n\nFor those aspiring young researchers reading this on Quora, the best advice I can give you is that nothing is \u201cdifficult\u201d if you set yourself the challenge of working on it. Above all, remember: research is fun! It\u2019s an exploration into the unknown.\n\nFor many years, from 2001\u20132018, I was privileged to co-direct the Autonomous Learning Laboratory at the University of Massachusetts, Amherst, with one of reinforcement learning\u2019s true pioneers, Andrew Barto. Andy and his former PhD student, Rich Sutton, helped establish the modern field of RL, the area that gave rise to Deep Mind and Alpha Go Zero. Andy and Rich embodied the true spirit of researchers having fun and working with them was the best professional experience of my career.\n\nPhD students at the lab hung up a sign on the main door that was a quote from one of the most distinguished scientists of all time, Albert Einstein.\n\nThat sums it all. Research doesn\u2019t need expertise. Einstein in fact hated textbook knowledge. Above all, he prized imagination, the ability to dream. If you want to make your children smart, he told parents, teach them fairy stories.\n\nAs we battle the latest pandemic, the Wuhan coronavirus, the biggest weapon at our disposal is our ability to sequence its genome. The biggest breakthrough in biology of the 20th century came from Watson and Crick, two brash biologists who upturned the world of biology by having fun! Watson went on to write a highly popular account of their discovery called the Double Helix. In it he tells the story of how they scandalized established researchers, like Oswald Avery of Columbia University, when he realized they didn\u2019t know elementary biochemistry. Yet, by playing with 3D models and in effect stealing from Rosalind Franklin\u2019s carefully gathered data sets, they cracked the secret of life. They were simply having fun!\n\nSo, again, my answer is, no, ML is not difficult. It is fun!",
                "Today, everybody wants to be a Deep Learning expert. But if you asked this question 15 years ago, you probably would have asked about Support Vector Machines because that was the hot new thing in machine learning back then. If this was 10 years ago, then you would have asked about ensemble methods because Random Forests and Gradient Boosted Trees were all the rage. And 10 years from now, Deep Learning's popularity will probably be overtaken by some newer technique, and newbies on Quora will ask if it's best to go straight into that. See a pattern here?\n\nYou never know what will be the next trendy thing and don't want to risk getting pigeonholed into an area that was once hot but later becomes outdated. To avoid that fate, you should become a well-rounded machine learning expert rather than a Deep Learning expert.",
                "Great question! How indeed does one prepare oneself for a (research or otherwise) career in machine learning, in particular in terms of familiarizing oneself with the underlying mathematics? I\u2019m going to resist the temptation of trotting out some standard books, and instead, focus on giving broad advice.\n\nThere\u2019s some bad news on this front, and it\u2019s best to get this out of the way as quickly as possible. Having spent 35+ years studying machine learning, let me put this in the most direct way possible: no matter how much time and effort you devote to it, you can never know enough math to read through all the ML literature. Different parts of ML use a variety of esoteric math. There\u2019s just no way one person can know all of this math, so it\u2019s good to be forewarned.\n\nOK, with that out of the way, how does one prepare oneself? Think of the process analogous to conditioning your mind and body to run a marathon. It\u2019s a gradual process, of improving your fitness, your ability to run for longer and longer distances, your breathing technique, your mental focus, and dozens of other dimensions. Working in ML is not like running a 100 meter sprint, where the race is pretty much over in a single breath. It\u2019s much more of an endurance sport, where you have to constantly work at it to remain in shape, and there\u2019s no point at which you can relax and say: OK, I know it all! Because no one does!\n\nAn example from my recent work will clarify the issues involved. One of the major challenges in machine learning is that there\u2019s never enough training data to tackle every ML problem that presents itself. Humans are especially adept in solving this challenge. I can get on a flight from San Francisco and within a few short hours find myself in a dizzying diversity of new environments, from the glitzy subways of Tokyo and the bleak winter in Scandinavia to an arid savannah in Africa, or a swampy rainforest in Brazil. There\u2019s no way I can ever hope to collect training samples from every possible environment that I can encounter in life. So, what do we do? We transfer our learned knowledge from places we\u2019ve been \u2014 so, having taken the BART subway in San Francisco, and subways in New York and London, I can try to handle the complexity of the subway in Tokyo by drawing upon my previous experience. Of course, it doesn\u2019t quite match \u2014 the language is completely different, the tone and texture of the visual experience is completely different (attendants in gloved hands show you the way in Tokyo \u2014 no such luxury is available in the US!). Yet, we somehow manage, and plod our way through new experiences. We even cherish the prospect of finding ourselves in some alien new culture, where we don\u2019t speak the language and can\u2019t ask for directions. It opens up our mind to new horizons, all part of the charm of travel.\n\nSo, what\u2019s the mathematics involved in implementing a transfer learning algorithm? It varies a lot depending on what type of approach you investigate. Let\u2019s review some approaches from computer vision over the past few years. One class of approaches are so-called subspace methods, where the training data from a collection of images in the \u201csource\u201d domain (which conveniently has labels given to us) is to be compared with a collection of unlabeled images from a \u201ctarget\u201d domain (e.g., \u201csource\u201d \u2192 NY subway, \u201ctarget\u201d \u2192 Tokyo subway).\n\nOne can take a collection of images of size NxN and using a variety of different methods find the smallest subspace that the source images lie in (treating each image as a vector in N^2 dimensions). Now, to understand this body of work, you obviously need to know some linear algebra. So, if you don\u2019t understand linear algebra, or you took a class way back when and forgot it all, it\u2019s time to refresh your memory or learn anew. Fortunately, there are excellent textbooks (Strang is usually a good place to start) and also something like MATLAB will let you explore linear algebraic ML methods without having to implement things like eigenvalue or singular value decomposition. As I usually told my students, keep the motto \u201ceigen do it if I try\u201d in mind. Persevere, and keep the focus on why you are learning this math. Because it is important and essential to understand much of modern ML.\n\nOK, great, you\u2019ve managed to learn some linear algebra. Are you done? Ummm, not quite. So, back to our transfer learning example. You construct a source subspace from the source images, and a target subspace from the target images. Umm, how does one do that. OK, you can use a garden variety dimensionality reduction method like principal components analysis (PCA), which just computes the dominant eigenvectors of the covariance matrices of the source and target images. This is one subroutine call in MATLAB. But, PCA is 100 years old. How about something new and cool, like a ooh la la subspace tracking method like GOUDA, which uses the fancier math of Lie groups. Oops, now you need to learn some group theory, the mathematics of symmetry. As it turns out, matrices of certain types, like all invertible matrices, or all positive definite matrices, are not just linear algebraic objects, they are also of interest in group theory, a particularly important subfield of which is Lie groups (Lie \u2192 \u201cLee\u201d).\n\nOK, great, you have a smattering of knowledge of group theory and Lie groups. Are you done? Hmmm\u2026actually not, because it turns out Lie groups are not just groups, but they are also continuous manifolds. What in the blazes is a \u201cmanifold\u201d? If you google this, you are likely to encounter web pages describing engine parts! No, a manifold is something entirely different in machine learning, where it means a non-Euclidean space that has curvature. It turns out the set of all probability distributions (e.g., 1 dimensional Gaussians with a scalar variance dimension and a scalar mean dimension) are not Euclidean, but rather, describe a curved space. So, the set of all positive definite matrices form a Lie group, with a certain curvature. What this implies is that obvious operations like taking the average have to done with considerable care. So, off you go, learning all there is to know about manifolds, Riemannian manifolds, tangent spaces, covariant derivatives, exp and log mappings, etc. Oh, what fun!\n\nGetting back to our transfer learning method, if you compute the source covariance matrix C_s and the target covariance matrix C_t, then there is a simple method called CORAL (for correlational alignment) that figures out how to transform C_s into C_t using some invertible mapping A. CORAL is popular as a transfer learning method in computer vision. But, CORAL does not actually use the knowledge that the space of positive definite matrices (or covariance matrices) forms a manifold. In fact, it forms something called a cone in convex analysis. If you subtract one covariance matrix from another, the result is not a covariance matrix. So, they do not form a vector space, but rather something else entirely. Oops, it turns out the study of cones is important in convex analysis, so there you go again, you need to learn about convex sets and functions, projections onto convex sets, etc. The dividing line between tractable and intractable optimization is not linear vs. nonlinear, but rather, convex vs. non-convex.\n\nI hope the pattern is becoming clear. Like one of those legendary Russian dolls, where each time you open one, you find it is not the end, but there\u2019s another one inside it, so it is with learning math in machine learning. Each time you learn a bit of math, you find it opens the door to an entirely new field of math, which you need to know something about as well. For my most recent paper, I had to digest a whole book devoted entirely to the topic of positive definite matrices (it\u2019s like the old joke, where the deeper you go, the more you know about a specialized topic, until you know everything about \u2014- nothing!).\n\nAny given problem in machine learning, like transfer learning, can be formulated as a convex optimization problem, as a manifold learning problem, as a multivariate statistical estimation problem, as a nonlinear gradient based deep learning problem, etc. etc. Each of these requires learning a bit about the underlying math involved.\n\nIf you feel discouraged, and feel like tearing your hair out at this point, I sympathize with you. But, on the other hand, you can look on the positive side, and realize that in terms of our analogy of running a marathon, you are steadily becoming better at running the long race, building your mathematical muscle as you go along, and gradually things start falling into place. Things start to make sense, and different subfields start connecting with each other. Something strange happens. You start liking it! Of course, there\u2019s a drawback. Someone who doesn't understand any of the math you get good at using asks you to explain your work, and you realize that it\u2019s impossible to do that without writing equations.\n\nMost researchers find their comfort zone and try to stay within it, since otherwise, it takes a great deal of time and effort to master the dozens of mathematical subfields that modern ML uses. But, this strategy eventually fails, and one is always forced to get outside one\u2019s comfort zone and learn some new math, since otherwise, a whole area of the field becomes alien to you.\n\nFortunately, the human brain is an amazing instrument, and provides decades and decades of trouble-free operation, allowing us to continually learn over 40,50, 60, years or more. How precisely it does that without zeroing out all prior learning is one of the greatest unsolved mysteries in science!",
                "Read this:\n\nREAL WORLD WARNING: If you aren\u2019t highly skilled with SQL and working with data everyone of these courses is an abysmal waste of time, including mine. Sigh.\n\nYou won\u2019t be working in the real world (we call it the applied space) without having really solid SQL skills.\n\nReady to learn real-world machine learning? [ https://www.logikbot.com/ ]\n\nThe brutal truth is that no one cares where you attained your knowledge, they only care that you have it and can apply it in the real world.\n\nThe interview questions aren\u2019t dependent upon where you went to college, what your degree is in or what courses you took. The company interviewing is looking for a specific skill set. If you have that skill set they want you.\n\nSo, while modeling is part of your job, it\u2019s only a small part. Sure, it\u2019s the fun part but it\u2019s not what you\u2019ll be doing most of the time. Read this [ https://www.quora.com/What-are-some-of-the-scams-in-the-data-science-field-industry/answer/Mike-West-99 ].\n\nMost of the time you\u2019ll be massaging data into a shape the model can attain the best performance from\u2026 and\u2026 that sh!t isn\u2019t so fun.\n\nSo, while you learning modeling focus on what you\u2019ll need in the real world, and the number 1 thing you\u2019ll need in the real world is heavy data skills.\n\nIf you have those, then on to modeling. If you don\u2019t, then I\u2019d highly recommend you get those first.\n\nIf you aren\u2019t in some kind of job working with SQL then that\u2019s probably your best route to becoming a machine learning engineer.\n\nFor most of us, NO SQL = NO JOB.\n\nNo, I\u2019m not trying to be a Debbie Downer, just set some realistic expectations and learn what the real world is really about.",
                "When I am asked to interview people, I try to ascertain whether they know the math or not, and how to apply it  in a real world context.  I also look to see if they understand high performance computing and not just vanilla coding.  \n\nI was asked to do this as a consultant, acting as a subject matter expert to help interview junior people for the firm.  \n\nIn our interviews, we asked a candidate to present some code they had written and to talk through it.   For an ML person, it would be some kind of ML code.\n\nSo, for example, I was involved with an interview with a Physics PhD from MIT discussing some NMF code he wrote in javascript.   The javascript was very good and he would be fine doing GUI work , Node.JS work, etc.  Certainly not something I could do.\n\nCan he do Machine Learning.  Mind you, he has a PhD in a math heavy subject from one of the top 10 schools in the world.  So he should know the math.\n\nI wanted to see if he knew how to get it to converge properly.  He did not.     He knew it was non-convex, but he did not know how to seed it, nor did he know about the convex variants. He tried to give me some nonsense about it being Bi-convex and whatnot. Dude, just use Kmeans++  to seed it.  Thats it.  Thats all you had to say.   This got totally past the VP of engineering and the CTO.    (They were just impressed that machine learning involved computing a first order derivative--something neither had since since college calculus)\n\nSo here, he knew some basic methods, but did not really know the most important ideas in the field, the important developments,  how to really code this.  It is clear that he had never done anything like this in his former work, nor did he really understand numerical methods.\n\nThis means that his solution would never work in production and -- more importantly -- that he would have no idea how to evaluate it or how to  fix it.  I see this a lot.   Also, he did not know the available open source codes, how they worked internally, and  which one to use,  or how to evaluate their performance.  For being a PhD from MIT, this was unacceptable to me.\n\nThere was also a code evaluation.  For me, one needs to know what runs fast and what does not. What good is a method that only runs on 300 data points?!     In this case, this interviewee had written his own javascript matrix library.  Did he know the BLAS libraries and how they work?  Or an alternative?     This is critical because you can't run anything in production if the code is too slow.   I see the same problem with most ruby coders--they just don't know numerical computing.    \n\nI was not looking to evaluate 10,000 of complex code , whether he used Agile or Unit Testing.  Nor did I care about solving some high school brain teaser. I just wanted to see a small piece of code, with good engineering choices ,   a good understanding of the math, and how to make this solution work in a modest production environment . \n\nId rather see old fashioned spectral clustering with a  Fortran library, which can scale, as opposed to trying to use a \"fancy\" method like NMF or LDA if you can not get it to work in production at scale.  (I'm not saying they don't scale--I am saying you better know how to get them to scale if you choose to use them)\n\nIn another interview, again a PhD (Ukrainian I think) who was very bright and had solved some good problems and had experience.  He was using an off-the-shelf SVM tool--a tool I know very well.   I asked a very basic question--how do you adjust the cost parameter for the SVM regularizer.   I rephrased the question a couple of different ways to give him a chance.    FAIL   In other words, did you read the documentation of the tool and did you understand which parameters to tweak and which ones to leave at the default settings  ( I kinda would like the person to have read the entire source code of the tool and know how it works. )   Again, this demonstrates a failure of the most basic mathematical concepts in ML -- Regularization-- and how they would apply in production.  Tuning this parameter can increase accuracy by %10-15 (or more).  Again, just simple stuff--but important stuff  This also shows a lack of attention to notice the important details of the work.  We actually offered this guy the job and he asked for a salary way out of the ballpark.  If he had not missed this critical question he might have been able to make the case for the salary.\n\n\nHaving shared all this, I would add that I think , for you, the market is very good and you will probably not encounter anything like this.  Why?  All you need to do is know more machine learning than the VP and the CTO--and here the bar is very low.   Everyone and his brother has a funding to do machine learning and they usually just need to solve one small problem and get the product out the door.  Most  (i.e 7/10 ) CTOs and VPs know absolutely nothing about even basic   machine learning  so they have no clue  even what to  ask. (Newton Raphson will blow them away, and they will think you are too expensive if you try compare stochastic gradient descent to interior point methods)  They got their start up funded based on the market potential of  the idea, and they are expected to hire people to invent their IP. \n\n  (Obviously if you are interviewing at Google or Lockheed Martin, disregard all of this and hire me once you get in)\n\nP.S.   I was asked once by some VP/CTO evaluating me what the volume of a rectangular prism is.  AlI could think of was this old Pink Floyd album Dark Side of the Moon  with the Prism on it \n\nhttp://en.wikipedia.org/wiki/The_Dark_Side_of_the_Moon\n\nI would never ask this kind of question but you will probably get asked many   puzzle questions like this if you are fresh out of school (or an old man like me I guess)  I seem to recall there are books and/or web sites with tons of these.\n\n\n  Good Luck",
                "Machine learning interviews test your understanding of how and when to apply ML algorithms. In most cases, it will depend on the skills you have reported in your resume and online profiles. However, there are a few commonly asked questions divided into different categories:\n\n1. ML Coding:\n2. \n1. Implement the k-means clustering algorithm using only built-in functions.\n2. Train a logistic regression model for classifying the MNIST digit database.\n3. Implement a convolution layer using just NumPy\n\n3. ML Algorithms:\n4. \n1. Compare random forest and decision tree models. Which works better?\n2. How would you train a classifier on an imbalanced dataset?\n3. What are the different types of learning rate optimizers?\n4. What is the cross-entropy loss? Is it convex or not?\n5. What is the difference between L1 and L2 regularization? When to use L1?\n6. What is the difference between MAE, MAPE, and MSE? How would you decide when to use which metric?\n7. Why do you use feature selection?\n8. What's the difference between R-CNN and YOLO?\n9. Assuming a clustering model's labels are known, how do you evaluate the model's performance?\n10. How do you prevent overfitting?\n\n5. ML System Design:\n6. \n1. What would you do to summarize an Instagram feed?\n2. How would you design a system to optimize Microsoft's go-to-market strategy?\n3. How would you detect if multimedia content being posted on Facebook violates the terms or contains offensive materials?\n4. Build a model to predict which people are most likely to take a loan from the bank.\n\nYou can answer a lot of these questions with enough preparation. If you are looking for a guided course that covers these questions, feel free to check out Interview Kickstart's offering for Machine Learning Engineer [ https://learn.interviewkickstart.com/course/machine-learning-interview-masterclass?utm_source=Quora&utm_medium=answer&utm_campaign=post&utm_term=What-are-some-of-the-most-important-questions-in-machine-learning&utm_content=PST ] roles. The course provides help in all aspects of interviewing, including profile building, salary negotiations, and more. It covers all of the above topics and offers mock interviews, practice questions, and extended support. The course is suitable for ML engineers with at least 2 years of experience and up to the L5 level of responsibilities.",
                "I ask them to attack some common machine learning problems from the web domain that most machine learning researchers should have heard of.  It can be hard to bring people up to speed on your problem domain during a short interview, so I try to ask more familiar questions like \"How would you approach the Netflix Prize?\", \"How would you generate related searches on Bing?\", \"How would you suggest followers on Twitter?\".\n\nI ask them to describe a \"Rolls Royce\" solution that you could implement in 3 months, then a version you could implement in 3 weeks, and another version you could implement if you only had 3 days.  This tells you how adaptable they are and if they can come up with pragmatic approaches.\n\nMost of the things we build need to scale, so I probe them about how their approach would work on large amounts of data.  MapReduce based solutions are nice to see when appropriate.\n\nI also ask some theory questions like Josh suggested along with a few real world ML challenges we face to see if they have any creative solutions.  Since my group is very product focused and team members are self directed, I also try to get a sense of whether the candidate can come up with new products themselves that will make a difference for the company and our users.\n\n** Another type of question I often ask involves finding or collecting your own training data.  Most applicants have only worked with toy datasets or data where the labeled examples were provided to them.  I pose a typical machine learning problem they might face, and ask how they would quickly gather additional data to help solve it.",
                "When asking about a project (permitting time, sometimes i cut things short) I take the following route:\n\nWhat did you do? Why did you do it? Why did you define the project as you did? How did you evaluate it? What is the new part? How is this different (ensure basic understanding of prior work)\n\nWhat did you do to the data? Why? How did you set it up? feature engineering? What topology did you use? Why? What else did you try? What didn\u2019t work? Why? What tools did you use?\n\nWhat else can/should be done? What did you do wrong? Not only mistakes which wasted time, but in the final setup what is wrong/cheating/definitely not the best way?\n\nI will also randomly stop on a key word or technical term and ask explaining, For example if you use drop-out in your neural network I may pause and ask for an explanation on drop out.",
                "OK, here's a wild idea.  I think it would be interesting to extend the general theory of machine learning (ala Smale) to asymptotically convergent approximations , and perhaps even non-convex ones.  We uses these approaches all the time in chemical physics and they worked well (i.e. Effective Operators), even though a mathematician would cringe.  Generally this means there is new math to do.",
                "This is a fun question to try to answer, since the answer ends up generating more questions! But, it may surprise some of you to learn that all of science works this way. Every scientific discovery, whether it be the discovery of the Big Bang or the discovery of DNA or the development of quantum mechanics, only leads to more questions. The greater the discovery, the more profound the questions it generates (e.g., what happened \u201cbefore\u201d the Big Bang?).\n\nI don\u2019t like to peg questions to years, since to me, a question that is worth asking may be something that takes years to pin down (scientists have been trying to understand Big Bang physics for decades now!).\n\nSo, here are 5 questions about ML and AI, and how it may shed light on the brain. These are of course generated by me on the spur of the moment, but as a 35 year veteran of ML and AI , I would like to get some answers to these questions.\n\nThe great English mathematician G.H. Hardy kept around a notebook of questions that he wanted to ask his Maker whenever he got the opportunity. First among his questions was whether the Riemann hypothesis was true (Riemann hypothesis - Wikipedia [ https://en.wikipedia.org/wiki/Riemann_hypothesis ]). It might sound like a quirky question to you, but to any mathematician of note, this question might really be at the top of their lists too.\n\nSo, in that spirit, when I get to meet my Maker, what questions would I ask? To keep this brief, I\u2019ll limit myself to 5 questions.\n\n1. Deep learning without gradient descent: Stochastic gradient descent (SGD) is the most popular method for training neural networks. It is easy to implement, but painfully slow to learn, and despite what you read in the popular press, it is NOT how the brain works. The idea that 100 billion neurons in our brains are getting tuned at each time step with some chemical version of gradient descent seems awfully far fetched. Besides, we can easily learn complex concepts from one or two examples (how many examples of the dollar or Euro symbol do you need to see to recognize it henceforth?). So, SGD is not the solution we are looking for. What other approach could be used, other than SGD, which would give us similar sample efficiency that the brain exhibits? The late Nobel prize winning biologist, Gerard Edelman, who won the prize for elucidating the structure of the immune system, spent the remainder of his life trying to explore the structure of the brain. He came up with an intriguing hypothesis called \u201cneural Darwinism\u201d (Darwin's neuroscientist: Gerald M. Edelman, 1929\u20132014 [ https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00896/full ]), wherein groups of interacting neurons compete with each other. Is this how the brain works? If it\u2019s not SGD running the \u201cmachine\u201d, what is it? I can\u2019t think of a more important question to ask my Maker.\n2. Modularity of the brain: it is by now an established fact that the brain is modular, in a very specific way that was beautifully elucidated in a book by Gerry Fodor (see The Modularity of Mind [ https://mitpress.mit.edu/books/modularity-mind ]). There are many ways to illustrate this concept. Consider some simple optical illusions. For example, in the below picture, why does it appear that the dots are always moving, when they are not (one reason is that tiny eye movements generate this illusion).\nConsider the two central dots below. They are of the same size in reality, but appear to be of very different sizes.\n\nOr consider the \u201ccafe illusion\u201d, first discovered by Richard Gregory from a wall in a cafe. The gray lines appear to be slanted, but they are actually straight.\n\nIf visual cognition is indeed the result of some \u201cdeep learning\u201d process, why do we all suffer from the same optical illusions? Furthermore, why can\u2019t we \u201cteach\u201d our brains (or our visual cortex) to undo the illusion? This is because the brain is modular. Your visual cortex is not under your direct conscious control.Similar examples can be given for other modalities (e.g., if you can speak English, or some other language, then speech in that language will always be interpreted by your brain as utterances in that language, and not puffs of air or sound, so there is no way to \u201cturn off\u201d your speech recognizer). This leads to the question: how much structure is built in to the brain at birth? How do we generate similarly structured deep networks, so that the mind of a robot is modular, and far more efficient to train than current highly sample inefficient approaches?\n\n3. Correlation to causation: much of modern machine learning focuses on correlation, which is a simple measure of association between random variables. However, causal knowledge is far more useful in the world, particularly in reasoning about change. If I change this variable X, would it change Y (even if X and Y are correlated, that would not necessarily indicate causation). How does causal knowledge emerge from a cluster of deep learning networks, if that is what underlies our brain?\n\n4. Developmental learning: Jean Piaget, the foremost developmental psychologist of the 20th century showed that children all over the world develop their mental capabilities in a number of broad phases. These have been experimentally confirmed with numerous experiments. Why does the child\u2019s brain develop in this way, and what does it suggest for the way lifelong learning in machines should progress? In particular, how can we move deep learning from its current limited abilities to learn one specific task at a time, towards a more flexible framework where multiple tasks can be learned in parallel over years of experience?\n\n5. OK, last question, so I better make it something interesting! Animal cognition researchers divide species into two categories based on something called the \u201cmirror test\u201d: does an animal shown its reflection in a mirror recognize itself and become conscious of its own self, or is it unable to understand that it is looking at a reflection of itself in the mirror? This test is taken as a proxy for whether animals are conscious of their own selves. What would it take for a machine to become self-aware?\n\nHope you can spend some time pondering about what questions you would like answered, when you get to meet your Maker! Wouldn\u2019t this be the ultimate Quora system? Any question you ask can of course be answered by the Maker.",
                "Just for machine learning (not AI):\n\nQ1 now you have this much publicity how will you continue to fund this activity - ie where will future revenues come from?\n\nMachine learning has a limited use for most businesses. So what can be done to pay for more research in this technology? The link with AI is very tenuous so you can\u2019t keep selling it on that basis. Image recognition is a way forward - so medical seems a good way forward. Satellite images would be another possibility. It does recognise quite quickly, but for real-time operations the domain is often too large for learning to be viable (self drive cars for example) . Factory inspection work however, is viable. Expansion into other patterns - so patterns in computer code might be plausible - looking for virus (this is being done by at least one compay).\n\nQ2 So what new applications can ML be used for? This could be their next big question.",
                "OK, here's a wild idea.  I think it would be interesting to extend the general theory of machine learning (ala Smale) to asymptotically convergent approximations , and perhaps even non-convex ones.  We uses these approaches all the time in chemical physics and they worked well (i.e. Effective Operators), even though a mathematician would cringe.  Generally this means there is new math to do.",
                "Machine learning (ML) as a field is highly unpredictable as there are many interesting potential breakthroughs in transfer learning, adversarial examples, one/zero shot learning, learning to learn, reinforcement learning (RL), memory networks, reasoning and attention.\n\nThere is a lot of research work in making models more robust to adversarial examples.\n\nLearning to learn or meta-learning which is about models that learn automatically given only the data is a path towards general intelligence or general purpose machine learning whereby, the only requirement is to provide the data and the models will figure out their hyperparameters automatically.\n\nI think it is a very important thing for ML models to learn everything because currently ML researchers/engineers spend a great deal of time working out the model hyperparameters that work best. If a model can learn on it\u2019s own, given only the data then that would be a great way to have general purpose accessible ML models.\n\nThough it is very computationally expensive because in actual sense the methods used to currently achieve meta-learning are based on evolution to evolve the best neural network architectures.\n\nReinforcement learning can also be used to search for the right hyperparameters whereby the accuracy of a model is used as the reward/punishment signal.\n\nSuch methods that allow models to find their own optimized hyperparameters like Google AutoML is a potential hot area that is likely to see substantial amount of progress in 2018.\n\nThe computation demand comes in because for every potential model, it must be trained and evaluated. This means Google Cloud computing running AutoML must be heavily equiped with some serious compute power. Especially if more businesses start trying out AutoML, I fear the compute resources might get overwhelmed.\n\nZero/one shot learning is very important because it can allow models to learn from very few to no learning examples. AlphaGo Zero actually used zero examples to learn but it had to go through millions of training iterations.\n\nIn actual zero/one shot learning, transfer learning plays a major role because transferring knowledge from one problem to another reduces the need for a lot of training data.\n\n\nThus if I have to pick which areas will see the most progress in 2018 I would pick:\n\n * Learning to learn.\n * Transfer learning.\n * And adversarial examples problem.\nNot to mention that we need to start looking at unsupervised learning methods as well as learning from richer datasets like videos.\n\nHope this helps.",
                "Machine learning is one piece of the puzzle of artificial intelligence. Suppose you \"solved\" machine learning, and had a way to efficiently compute a function that perfectly maps inputs to outputs on any structured dataset. The next step is to develop a set of methods that effectively tie together machine learning with knowledge representation and planning. Intelligence is not just about learning. It's also about reasoning and action. So computer scientists will search for a unifying theory of artificial intelligence much like physicists search for a unifying theory of fundamental physical forces.",
                "Quantum Machine Learning is a very interesting field: Lecture Schedule - Quantum Robotics Reading Group [ https://sites.google.com/site/quantumroboticsreadinggroup/lecture-schedule ] with some possible speedup promises and new algorithmic features.\n\nThere is also the emerging discipline of Neuro-Quantum AI.",
                "When asking about a project (permitting time, sometimes i cut things short) I take the following route:\n\nWhat did you do? Why did you do it? Why did you define the project as you did? How did you evaluate it? What is the new part? How is this different (ensure basic understanding of prior work)\n\nWhat did you do to the data? Why? How did you set it up? feature engineering? What topology did you use? Why? What else did you try? What didn\u2019t work? Why? What tools did you use?\n\nWhat else can/should be done? What did you do wrong? Not only mistakes which wasted time, but in the final setup what is wrong/cheating/definitely not the best way?\n\nI will also randomly stop on a key word or technical term and ask explaining, For example if you use drop-out in your neural network I may pause and ask for an explanation on drop out.",
                "Every place I\u2019ve interviewed with or worked at had a similar process.\n\nAs an aside, Microsoft uses me as an interview resource. Internal Microsoft employees aren\u2019t allowed to set up or participate in interviews outside of Microsoft so when a client asks them, they\u2019ll use outside resources.\n\nHere\u2019s the basic process.\n\nThey have a phone screen to ascertain your technical acumen.\n\nOur phone screen is all data. Here\u2019s a short list to prepare you for the phone screen. [ https://www.machinelearningmike.com/post/phone-screen-preparation ]\n\nAll of our models are on structured data and we use BigQuery so if you don\u2019t have strong data skills, we end the phone screen. Most companies do the same thing. We actually won\u2019t interview anyone without 3\u20135 years of SQL in a real-world setting. Teaching data skills is far more difficult than Python or modeling basics.\n\nSo, we start easy.\n\nWhat are the two types of SQL?\n\nWhat\u2019s a relational database?\n\nWhat\u2019s a table?\n\nWhat the difference between structured data and unstructured data?\n\nHow do you create a table?\n\nIn the phone screen, we also throw in a few Python questions. Easy shit\u2026 like what does a single equals sign mean? What\u2019s an equality operator? What\u2019s a model called in SciKit-Learn.\n\nIf you make it past the phone screen you have your first in-person.\n\nThis is where we dive into machine learning. Again, starting off simple like\u2026 can you whiteboard out the machine learning pipeline?\n\nCan you draw a simple ANN? What\u2019s the difference between deep learning and an ANN?\n\nAfter the candidate makes it past these it\u2019s on to coding questions in Python, Pandas, SciKit-Learn.\n\nThen basic stats questions. All applied questions.\n\nThen I\u2019ll put up Jupyter Notebook with an end to end pipeline and have them walk me through each line of code.\n\nLike\u2026\n\nimport pandas as pd %3CTell me what every word means.\n\nThe word import means to \u201cbring in\u201d\n\nPandas is a library for data manipulation.\n\nas pd is an alias. An alias is used so you don\u2019t have to write out the entire library during the call.\n\nWe have about 100 questions. It\u2019s really important to understand, companies aren\u2019t going to pay you 200K to learn on the job.",
                "I think it would be hard to prepare to crack 'any' machine learning interview without years of study because machine learning is a broad field.  It would be much more efficient to select a specific area of machine learning (like deep learning or reinforcement learning or clustering) and to focus on passing any interview in just that area.\n\nIf you want to be able to pass any machine learning interview, one option would be to get a degree like a MS in machine learning.  A PhD degree will go quite narrow, and you're unlikely to get much out of the years of independent research that would generalize to many areas of machine learning.  You could therefore enroll in such a MS program or find syllabi of graduate ML courses and study materials on those syllabi.\n\nYou'd also want to get real-world experience with the methods described in the books and papers, as ML interviews will often involve some discussion about your projects.\n\nFinally, you'd need to be very comfortable with probability and statistics.  This would hopefully come from the MS courses, but it would also help to have taken at least one full probability course to be introduced to probabilistic intuition and a full statistics course to be introduced to statistical thinking.\n\nTo answer sub-questions on your original question:\n\nWhat kinds of questions to expect?\n\nI'm just making these up, but I think they're also fairly standard:\n\n * derive the update equation for linear regression\n * how do you handle categorical variables with linear regression?\n * explain logistic regression\n * tell me how you (as a data scientist at Foursquare) might figure out which restaurant a person is in given their lat/long.\n * tell me what EM is\n * no, seriously, what objective is EM maximizing?\n * how would you do feature selection on such-and-such a project?\n * what do you do when your data has missing / incorrect values?\n * tell me about such-and-such a project on your resume\n * why is convexity important for learning algorithms?\nWhat kinds of relevant side projects would look good when added in CV?\n\n * a project where you had to collect the data yourself, e.g. scraping products reviews from a website\n * a project where you had to deal with missing or messy data, e.g. cases where some people provide their location and some people don't\n * a project where you had to figure out an appropriate \"ground truth\".  maybe it was clicks; maybe it was human labels; maybe it was an unsupervised method\n * a project where there are lots of places to apply statistical reasoning, i.e., not just a flat table of data\n * a project where the application of machine learning is solving an important problem (you can use ML on anything; whether it's useful is another question), e.g. predicting whether people describing symptoms on a medical forum are eventually diagnosed with a disease.\n",
                "Machine learning interviews test your understanding of how and when to apply ML algorithms. In most cases, it will depend on the skills you have reported in your resume and online profiles. However, there are a few commonly asked questions divided into different categories:\n\n1. ML Coding:\n2. \n1. Implement the k-means clustering algorithm using only built-in functions.\n2. Train a logistic regression model for classifying the MNIST digit database.\n3. Implement a convolution layer using just NumPy\n\n3. ML Algorithms:\n4. \n1. Compare random forest and decision tree models. Which works better?\n2. How would you train a classifier on an imbalanced dataset?\n3. What are the different types of learning rate optimizers?\n4. What is the cross-entropy loss? Is it convex or not?\n5. What is the difference between L1 and L2 regularization? When to use L1?\n6. What is the difference between MAE, MAPE, and MSE? How would you decide when to use which metric?\n7. Why do you use feature selection?\n8. What's the difference between R-CNN and YOLO?\n9. Assuming a clustering model's labels are known, how do you evaluate the model's performance?\n10. How do you prevent overfitting?\n\n5. ML System Design:\n6. \n1. What would you do to summarize an Instagram feed?\n2. How would you design a system to optimize Microsoft's go-to-market strategy?\n3. How would you detect if multimedia content being posted on Facebook violates the terms or contains offensive materials?\n4. Build a model to predict which people are most likely to take a loan from the bank.\n\nYou can answer a lot of these questions with enough preparation. If you are looking for a guided course that covers these questions, feel free to check out Interview Kickstart's offering for Machine Learning Engineer [ https://learn.interviewkickstart.com/course/machine-learning-interview-masterclass?utm_source=Quora&utm_medium=answer&utm_campaign=post&utm_term=What-are-some-of-the-most-important-questions-in-machine-learning&utm_content=PST ] roles. The course provides help in all aspects of interviewing, including profile building, salary negotiations, and more. It covers all of the above topics and offers mock interviews, practice questions, and extended support. The course is suitable for ML engineers with at least 2 years of experience and up to the L5 level of responsibilities.",
                "When I am asked to interview people, I try to ascertain whether they know the math or not, and how to apply it  in a real world context.  I also look to see if they understand high performance computing and not just vanilla coding.  \n\nI was asked to do this as a consultant, acting as a subject matter expert to help interview junior people for the firm.  \n\nIn our interviews, we asked a candidate to present some code they had written and to talk through it.   For an ML person, it would be some kind of ML code.\n\nSo, for example, I was involved with an interview with a Physics PhD from MIT discussing some NMF code he wrote in javascript.   The javascript was very good and he would be fine doing GUI work , Node.JS work, etc.  Certainly not something I could do.\n\nCan he do Machine Learning.  Mind you, he has a PhD in a math heavy subject from one of the top 10 schools in the world.  So he should know the math.\n\nI wanted to see if he knew how to get it to converge properly.  He did not.     He knew it was non-convex, but he did not know how to seed it, nor did he know about the convex variants. He tried to give me some nonsense about it being Bi-convex and whatnot. Dude, just use Kmeans++  to seed it.  Thats it.  Thats all you had to say.   This got totally past the VP of engineering and the CTO.    (They were just impressed that machine learning involved computing a first order derivative--something neither had since since college calculus)\n\nSo here, he knew some basic methods, but did not really know the most important ideas in the field, the important developments,  how to really code this.  It is clear that he had never done anything like this in his former work, nor did he really understand numerical methods.\n\nThis means that his solution would never work in production and -- more importantly -- that he would have no idea how to evaluate it or how to  fix it.  I see this a lot.   Also, he did not know the available open source codes, how they worked internally, and  which one to use,  or how to evaluate their performance.  For being a PhD from MIT, this was unacceptable to me.\n\nThere was also a code evaluation.  For me, one needs to know what runs fast and what does not. What good is a method that only runs on 300 data points?!     In this case, this interviewee had written his own javascript matrix library.  Did he know the BLAS libraries and how they work?  Or an alternative?     This is critical because you can't run anything in production if the code is too slow.   I see the same problem with most ruby coders--they just don't know numerical computing.    \n\nI was not looking to evaluate 10,000 of complex code , whether he used Agile or Unit Testing.  Nor did I care about solving some high school brain teaser. I just wanted to see a small piece of code, with good engineering choices ,   a good understanding of the math, and how to make this solution work in a modest production environment . \n\nId rather see old fashioned spectral clustering with a  Fortran library, which can scale, as opposed to trying to use a \"fancy\" method like NMF or LDA if you can not get it to work in production at scale.  (I'm not saying they don't scale--I am saying you better know how to get them to scale if you choose to use them)\n\nIn another interview, again a PhD (Ukrainian I think) who was very bright and had solved some good problems and had experience.  He was using an off-the-shelf SVM tool--a tool I know very well.   I asked a very basic question--how do you adjust the cost parameter for the SVM regularizer.   I rephrased the question a couple of different ways to give him a chance.    FAIL   In other words, did you read the documentation of the tool and did you understand which parameters to tweak and which ones to leave at the default settings  ( I kinda would like the person to have read the entire source code of the tool and know how it works. )   Again, this demonstrates a failure of the most basic mathematical concepts in ML -- Regularization-- and how they would apply in production.  Tuning this parameter can increase accuracy by %10-15 (or more).  Again, just simple stuff--but important stuff  This also shows a lack of attention to notice the important details of the work.  We actually offered this guy the job and he asked for a salary way out of the ballpark.  If he had not missed this critical question he might have been able to make the case for the salary.\n\n\nHaving shared all this, I would add that I think , for you, the market is very good and you will probably not encounter anything like this.  Why?  All you need to do is know more machine learning than the VP and the CTO--and here the bar is very low.   Everyone and his brother has a funding to do machine learning and they usually just need to solve one small problem and get the product out the door.  Most  (i.e 7/10 ) CTOs and VPs know absolutely nothing about even basic   machine learning  so they have no clue  even what to  ask. (Newton Raphson will blow them away, and they will think you are too expensive if you try compare stochastic gradient descent to interior point methods)  They got their start up funded based on the market potential of  the idea, and they are expected to hire people to invent their IP. \n\n  (Obviously if you are interviewing at Google or Lockheed Martin, disregard all of this and hire me once you get in)\n\nP.S.   I was asked once by some VP/CTO evaluating me what the volume of a rectangular prism is.  AlI could think of was this old Pink Floyd album Dark Side of the Moon  with the Prism on it \n\nhttp://en.wikipedia.org/wiki/The_Dark_Side_of_the_Moon\n\nI would never ask this kind of question but you will probably get asked many   puzzle questions like this if you are fresh out of school (or an old man like me I guess)  I seem to recall there are books and/or web sites with tons of these.\n\n\n  Good Luck",
                "I ask them to attack some common machine learning problems from the web domain that most machine learning researchers should have heard of.  It can be hard to bring people up to speed on your problem domain during a short interview, so I try to ask more familiar questions like \"How would you approach the Netflix Prize?\", \"How would you generate related searches on Bing?\", \"How would you suggest followers on Twitter?\".\n\nI ask them to describe a \"Rolls Royce\" solution that you could implement in 3 months, then a version you could implement in 3 weeks, and another version you could implement if you only had 3 days.  This tells you how adaptable they are and if they can come up with pragmatic approaches.\n\nMost of the things we build need to scale, so I probe them about how their approach would work on large amounts of data.  MapReduce based solutions are nice to see when appropriate.\n\nI also ask some theory questions like Josh suggested along with a few real world ML challenges we face to see if they have any creative solutions.  Since my group is very product focused and team members are self directed, I also try to get a sense of whether the candidate can come up with new products themselves that will make a difference for the company and our users.\n\n** Another type of question I often ask involves finding or collecting your own training data.  Most applicants have only worked with toy datasets or data where the labeled examples were provided to them.  I pose a typical machine learning problem they might face, and ask how they would quickly gather additional data to help solve it.",
                "Edited: Question was edited to explicitly exclude this type of answer, but I believe still  relevant for those seeking to prepare for an interview. \n\nDon't!\n\nI keep giving the same answer, the best way to pass an interview is to be qualified for the position. If you are in fact qualified for the position a good interviewer will realize that. A good interviewer can't be tricked, and you want to work for a good interviewer.  There is little point in looking for shortcuts hoping to fool an interviewer on what you know. You should be honest in your resume, a good interviewer will try to verify you know what you claim you know and not bother with details which might give bonus points but you never claimed you know. \n\nI myself have found myself getting asked a C++ question because I never erased it from my resume. I fumbled it, it wasn't terribly important for the job, but the interviewer wanted to know if I in fact know everything I said I know, and he did so by randomly selecting a minor item from my resume. He was well within his right and I was wrong for not cleaning up my resume.\n\nGet hired based on who you are and what you know, don't take a job if you weren't challenged in the interview process. If the interview process can be tricked with a few tips from Quora many of your future co-workers got in that way, don't take the job.\n\nFocus on getting hired by good companies.",
                "Almost of our phone screen is data related. It\u2019s 90% SQL, Pandas, data sourcing and data wrangling questions. This weeds out a lot of statisticians and mathematicians turned machine learning engineer.\n\nWhat\u2019s a primary key? What\u2019s an inner join? What\u2019s a where clause do? How do you retrieve the min and max values from a column? What\u2019s a Dataframe? What object does a Dataframe sit on top of? What does a single equals sign mean in Python? What\u2019s an attribute?\n\nIf the candidate can\u2019t source data then the interview is over.\n\nMost won\u2019t make it past the phone screen.\n\nThe first in-person is more data related questions, cloud questions, modeling and model tuning.\n\nWe look for a solid understanding of the machine learning process as it relates to the real world.\n\nWe ask scenario based questions like:\n\nYou have data in a SQL Server database that\u2019s 500 gigs in size. How do you get that data to a cloud provider so it can be molded at scale? It can be answered quite a few ways but only if you\u2019ve done it before.\n\nWe also do a lot of white boarding like everyone else. For example, draw a liner regression model. Now, draw a classification model. Now, tell me what\u2019s special about those two models? You should know that most real world modeling is either classification or regression and be able to explain it to a project manager so he/she can understand it.\n\nWe also do many one liners you should know without thinking. For example:\n\nWhat\u2019s supervised machine learning?\n\nWhat\u2019s the machine learning pipeline?\n\nDefine data wrangling?\n\nWhat\u2019s an array? How do you find values in an array? What\u2019s so special about the array?\n\nWhat model do you start with when modeling highly structured data?\n\nThat\u2019s a good high level overview.\n\nCheck out this YouTube video [ https://www.youtube.com/watch?v=uQsLXB1Pmqk&t=797s ] on a few of the top ML Libraries in Machine Learning.",
                "I try to ask very basic but fundamental questions to see if they actually understand the math\n\n I would expect everyone to know how to solve\n\nAx=b\n\nAnd to know the difference between a convex and non-convex solution.\n\n\nFinding any reasonable candidate is quite hard, and the real goal is to disqualify people.  This is usually enough to weed out %90 of the posers.\n\nAlso, there are lots of ways to solve the same problem, and ML is a big field.\nKeeping it simple and basic is important. \n\nAt this point, it might be useful to ask about regularization.  It's enough to understand that A^{-1} is ill conditioned; although that would weed out %99 of the candidates, and no one could fill the role.  \n\nI generally don't ask questions like \"implement logistic regression\" unless the job actual involved writing custom solvers since doing this well is hard, and implementing a simple solution is pointless  counterproductive. (I have seen new hires try this and fall flat on their a$$).  \n\n I would rather they know how that running NMF will bomb out if you don't seed it properly, how to adjust the cost parameter on liblinear , or just knowing what Newton's method is  \n\n---\n\n Note that the real key to evaluate a PhD level researcher, however, is to determine if they are capable of doing independent research (if the job actually entails this), or if they just did what their advisor told them to do.   Here, one wants to see if they have a broad knowledge of the own field and how creative their PhD research was.  This is not, however, necessarily part of the interview process but more part of the screening process.\n\n When I evaluate a more senior person, especially an older PhD,  I use the old University of Chicago whiteboard method, meaning that anything someone brings up, they had better be able to go to the whiteboard (or chalkboard, in the old days) and work through the math.  This is absolutely critical, because it is necessary to see if you have kept yours math skills up and also to see if think using language or you think using math.   Thinking in math is hard and requires you maintain the skill.\n\nyou can get more of an idea of what I expect by looking at my blog\nhttp://charlesmartin14.wordpress.com/",
                "The interviewers would mostly look at your practical experience with machine learning. The practical experience would be the machine learning projects that you have carried out during studies or industry projects etc.\n\nAlong with this, below are few questions that are good to know for a machine learning interview.\n\n1. How to evaluate a binary classifer?\n2. How to check for overfitting from a machine learning model?\n3. Explain what regularization is and why it is useful.\n4. How does the k-nearest neighbor algorithm work and what are its drawbacks?\n5. Explain what precision and recall are. How do they relate to the ROC curve?\n6. How can you prove that one improvement you've brought to an algorithm is really an improvement over not doing anything?\n7. What is root cause analysis?\n8. How does a k-means algorithm work? How to select an appropriate k value? What are the assumptions for k-means.\n9. Explain backpropagation algorithm.\n10. Explain what resampling methods are and why they are useful. Also explain their limitations.\n11. Is it better to have too many false positives, or too many false negatives? Explain.\n12. What is bias-variance tradeoff?\n13. Hoes does a Random forest algorithm work?\n14. How to perform feature selection for a binary classification problem?\n15. How would you screen for outliers and what should you do if you find one?\n16. How does a logistic regression work and how is it different from linear regression?\n17. What is kernel trick in SVM?\n18. What is a recommendation engine? How does a collaborative filtering work?\n19. Explain what a false positive and a false negative are. Why is it important to differentiate these from each other?\n20. What is PCA and why it is used?\nReference:\n\n20 Questions to Detect Fake Data Scientists [ http://www.kdnuggets.com/2016/01/20-questions-to-detect-fake-data-scientists.html ]\n\n21 Must-Know Data Science Interview Questions and Answers [ http://www.kdnuggets.com/2016/02/21-data-science-interview-questions-answers.html ]",
                "They\u2019re quite subtle in how they are formulated.\n\nA lot of the time, an interesting question is not nessecarily meant to be so - nor is it revealing to be so.\n\nIt sometimes, has been that the question is a genuine \u201cBut why?\u201d\n\nAs in, they did some project - something went wrong - why?\n\nOther times, it\u2019s about very specific dynamics that has no clear answer.\n\nA lot of the times, whenever people ask about ML, the questions are very complex.\n\nBecause, it\u2019s not as simple - in truth - as saying \u201cWell, due to using a SELU or that you constructed a 5 layer of RNN\u2019s with said Algorithmics etc. it caused the divergent at timestep X due to set belonging gradient partitioning on this fractional interplay with bounds of etc. etc.\u201d\n\nI mean, that isolated piece of knowledge - might be true - but, it fairly rarely answers the question.\n\nThus, the most interesting questions - are the ones that are legitimate questions - Things you can\u2019t reasonably procure an answer to, without some intimate knowledge or some source of strong reasoning.",
                "Instead of giving you a list of interesting topics in machine learning, I\u2019ll give you one specific topic: Generative Adversarial Networks, or GANs.\n\nGANs are one of the most exciting topics in machine learning. Very broadly, a GAN is a neural network \u2018system\u2019 comprising of 2 neural networks trying to outdo each other (adversaries to each other).\n\nSay your goal is to have an algorithm generate a high quality photo-realistic image. By generate, we mean create a new image, an image it has never seen before. GANs are the currently most successful algorithm for this problem. You have a neural network G, which tries to generate images. And you have another neural network D, which tries to distinguish generated (fake) images from real images (say images downloaded from the web). The two networks are trying to outdo each other.\n\nG starts by generating completely noisy looking output, but continually improves as it tries to fool D. But D keeps getting better too. Eventually, both get really good, and we have the best image generator system currently known.\n\nFurther reading:\n\n1. You can read the original research paper here: [1406.2661] Generative Adversarial Networks [ https://arxiv.org/abs/1406.2661 ]\n2. You can read an intuitive tutorial on the topic here: Generative Adversarial Networks [ https://www.commonlounge.com/discussion/8af86f1e90504ad28541838fcf947ef7 ]\n",
                "Highly unlikely.\n\nExtracting meaningful knowledge from conversational language is far more difficult than extracting knowledge from structured data formats that are plentiful on the Internet. You don\u2019t need Quora when you can parse the world\u2019s knowledge bases, wikipedias, library of congress etc.\n\nIf and when machine learning and NLP are sufficiently advanced to not only interact with human writers, but also to extract real knowledge from those interactions, then ML will have long passed the point of needing to learn from humans.",
                "One of the reasons I was excited to join Quora as VP of Engineering was how important machine learning was for the company's success. As of 2015, machine learning approaches are used extensively in many different parts of the Quora product. Most of these applications were in place before I joined. However, we are constantly coming up with new approaches and making big improvements to the existing ones. It is important to note that all these improvements are first optimized and tested offline by using many different kinds of offline metrics but are always finally tested online through A/B tests.\n\nIn the following paragraphs, I will describe some of the most important applications and techniques of ML at Quora as of 2015.\n\nRanking\n\nRanking is arguably one of the most important applications of machine learning on the Web. Companies ranging from very large to small have created business models around the ability to rank, for example, results to a query string. At Quora we use ranking algorithms in different contexts and with different purposes.\n\nOne interesting example is answer ranking. Given a question and several answers to this question, we are interested in ranking them in decreasing order so to keep the \u201cbest\u201d answer at the top and the worst at the bottom (see screen capture below).\n\nThere are many different features that could go into determining the right order of the answers to a question. In order to determine that, we actually first need to determine what we at Quora define as a \u201cgood\u201d answer. One good way to come up with that definition is to look at what Quora considers a \u201chelpful answer\u201d [ https://www.quora.com/What-does-a-good-answer-on-Quora-look-like-What-does-it-mean-to-be-helpful/answer/Quora ]. You will read about answers being \u201ctruthful\u201d, \u201creusable\u201d, \u201cprovide explanation\u201d, or \u201cwell formatted\u201d among others. Our machine learning algorithm, which implements a particular learning-to-rank approach, has many different features that try to encode many dimensions that ultimately relate to all those abstract concepts. For example, we have features that encode information about the quality of the writing, but also use features that inform about the kind of interactions that the answer received (e.g. upvotes, downvotes, or expands). We also have features related to the user who wrote the answer including, for example, the expertise of the user in the question topic.\n\nWe have many other applications of ranking at Quora, some of which might even go unnoticed. For example, the names that appear as having upvoted a given answer are also ranked in a way that we present at top the ones that we consider are most informative for that given question/answer. The same is true for those names that are suggested when presenting possible answerers to a particular question.\n\nLet's take a closer look at two particular instances of ranking ML algorithms: Search, and Personalized Ranking.\n\nSearch algorithms\n\nFor an application such as Quora, search algorithms can be seen as just another application of ranking. As a matter of fact, search can be decomposed into two different steps: textual matching + ranking. In the first step, documents (questions) that match the query entered in the search box in some way are returned. Those documents are then treated as candidates of the second step, which ranks those results in order to optimize something such as the probability of click.\n\nThis second step is indeed another example of learning-to-rank application where many different features can be used. These features can go from simple textual features that were already used in the original textual matching step to others that relate to user actions, or object properties such as its popularity.\n\nPersonalized Ranking\n\nIn some situations such as the one outlined above, we might get by with a global optimal ranking for all users. In other words, we can assume that the order for most \u201chelpful\u201d answers to a given question is independent of the user who is reading the answer. That assumption clearly fails in many important cases though. One such very important case is the Quora Feed. The Quora Feed is basically the homepage that any user sees when logging into the product. In that page, we are trying to select and rank the most \u201cinteresting\u201d stories for that given user at a given time (see example below). This is a prototypical example of personalized learning-to-rank, similar to what the Netflix homepage does with Movies and TV Shows.\n\nThe Quora use-case though is a bit more challenging than the one of ranking movies and TV shows at Netflix. As a matter of fact, one could look at our use case as a combination of what Netflix, Facebook, and Google News need to do to optimize their personalized ranking. On the one hand, we want to ensure that stories that are ranked higher are topically relevant for the user. On the other hand, Quora also has explicit connections between users. Actions on your \u201csocial network\u201d should also influence the ranking. Finally, stories at Quora might sometimes relate to ongoing trending events. Timeliness is another element that should affect the model's decision to promote or demote a story.\n\nBecause of this, there are many different kinds of features that go into creating such a personalized rank at Quora. Let's list some of them:\n\n * Quality of the questions/answers\n * Topics the user is interested on\n * Other users the user is following\n * What is trending/popular\n * ...\nAs a matter of fact, it is important to keep in mind that at Quora we are interested in both engaging users to read interesting content and at presenting questions the users can write interesting content for. So, we have to include features that refer both to the interestingness of the answers as well as the questions. To derive those features, we use information derived from the actions done by the user, author, and object (i.e. answer/question). Those actions are considered and aggregated at different temporal windows and fed into the ranking algorithm. There are actually many different derived features that go into our personalized feed model, and we are constantly experimenting with adding more.\n\nAnother important consideration about our feed ranking application is that we need to be able to make it responsive to both user actions, impressions, and even trending events. We have an ever-growing collection of millions of questions and answers that we could not try to rank on real-time for a each user. In order to optimize the experience, we have implemented a multi-stage ranking solution where different candidates are selected and ranked in advanced before the final ranking is actually performed.\n\nRecommendations\n\nThe personalized ranking described above is already a form of recommendation. A similar approach is used in different use cases. For example, the popular Quora email digest includes a collection of stories that have been selected and are recommended for you. This is a different learning-to-rank model that is optimized for a different objective function.\n\nBesides ranking algorithms, we have other personalized recommendations that appear in different parts of the product. For example, in different places, you will see recommendations of people or topics you should follow (see below).\n\nRelated Questions\n\nAnother source of recommendations is giving users questions that are somewhat related to the current one.\n\nRelated questions are determined by using another machine learned model that takes into account different features that include, for example, textual similarity, co-visit data, or other shared features such as topics. Other features related to the popularity or quality of the question are also taking into account. It is important to note that what makes a good \u201csimilar\u201d recommendation is not only how similar that item is to the source but the \u201cinterestingness\u201d of the target. As a matter of fact, one of the trickiest issues for any \u201crelated items\u201d machine learned model is how to tradeoff similarity vs. other relevance elements.\n\nRelated questions is a model that is particularly effective in creating engagement of logged out users when they visit the question page from an external search. That is one of the reasons why for now this recommendation model is unpersonalized.\n\nDuplicates\n\nDuplicate questions is an extreme case of the related questions case described above. This is an issue for Quora because we want to make sure that all energies from users answering a particular question are shared and focused in the right place. It is also important to point users who have a question and want to add it to the site to pre-existing answers. Because of this, a lot of efforts go into detecting duplicate questions, especially at creation time.\n\nOur current solution is based on training a binary classifier trained with duplicate/non-duplicate labels. We use different kinds of signals that go from textual vector space models to usage-based features.\n\nUser Trust/Expertise Inference\n\nIn an application like Quora, it is important to understand how trustworthy a user is. As a matter of fact, we are not only interested in answering this question in an absolute way, but in relation to a given topic. A user might be very knowledgeable in some topics but not so much in others. At Quora we use ML techniques to infer user expertise. In order to train those models we have several important features. We not only know the answers a user has written in a given topic, but we also know the upvotes, downvotes, or views the user has received on those. We also know how many \u201cendorsements\u201d the user has on the given topic. Endorsements are an explicit acknowledgement on someone's expertise coming from other users.\n\nThe other important thing to keep in mind is that trust/expertise propagates through the network and that should be taken into account by the algorithms. For example, if I receive an upvote from an expert on Machine Learning to an answer on Machine Learning, this should count more than an upvote from a random user with no expertise on that topic. The same goes for endorsements and other user-to-user features.\n\nSpam Detection and Moderation\n\nA site like Quora that prides itself on maintaining a high quality threshold on the content has to be very vigilant on attempts to game the system by introducing spam, malicious, or simply very low quality content. A pure manual model to review content does not scale. The way to approach the problem, you guessed it, is to use ML models to detect these issues.\n\nAt Quora we have several models that detect different issues related to content quality. The output of those classifiers is in most cases not used directly to make a decision on the content but rather is used as a way to feed those questions/answers into moderation queues that are then manually reviewed.\n\nContent Creation Prediction\n\nA very important thing to keep in mind about Quora is the fact that we are optimizing many parts of the system not only to create an engaging experience for readers but also to maximize good quality and in-demand content being created. For this reason, we have a ML model that predicts the probability of a user writing an answer to a given question. This allows our system to prioritize those questions in different ways. One of them is the system's automatic A2A (Ask to Answer) that sends these questions to potential writers through notifications. Other ranking systems explained above will also use this predicted probability.\n\nModels\n\nQuora has tried many different models to address the different use cases described above. Sometimes we use open source implementations, but many others we end up implementing more efficient or flexible in-house versions.\n\nI won't go into the details of what model is used where, but here is a list of models that are used in different places of our system:\n\n * Logistic Regression\n * Elastic Nets\n * Gradient Boosted Decision Trees\n * Random Forests\n * Neural Networks\n * LambdaMART\n * Matrix Factorization\n * Vector models and other NLP techniques\n * ...\nConclusion\n\nAs described above, Quora uses Machine Learning in many different ways. While we have already been able to get very important gains by using these ML approaches, we feel confident that there are many more gains to be had and we continue investing in new techniques. Besides, there are exciting new applications of ML in the near future that we are already starting to think about. These include areas such as ad ranking, machine translation, and other applications of NLP that will directly feed into new product features we plan on adding soon.",
                "It is \u201cpatterned\u201d (I think you mean \u201ctrained\u201d) with a combination of statistics and calculus. Statistics is used to collect information about how some phenomenon in reality behaves, and calculus is used to derive a formula of that behavior. It is the process of repeatedly performing statistics on the phenomenon\u2018s data, generating a formula with calculus, then measuring the error of that formula against the phenomenon\u2018s data and finally the incorporation of that error back into the statistics and repeating this entire process that is the machine learning / algorithm training process.\n\nThis initial machine learning / algorithm training process may be augmented afterwards with what is called RLHF (Reinforcement Learning from Human Feedback). That is where the freshly trained AI is \u201casked questions\u201d - basically tested for accuracy. If the AI gives incorrect answers that \u201cquestion and answer\u201d are marked for inclusion in a near future training data set - a training at the tail end of this human asking questions period.\n\nNote that only a subset of AIs operate with human language, and those that do have an additional aspect of their RLHF, and that is a layer of ethical consideration where the AI is enhanced to look at the given question and compare the body of knowledge the question requires with basic human ethics. If the question\u2019s area of knowledge lies within an ethically questionable region, the AI is trained to not respond, or respond with a canned denial to answer.",
                "Machine learning interviews test your understanding of how and when to apply ML algorithms. In most cases, it will depend on the skills you have reported in your resume and online profiles. However, there are a few commonly asked questions divided into different categories:\n\n1. ML Coding:\n2. \n1. Implement the k-means clustering algorithm using only built-in functions.\n2. Train a logistic regression model for classifying the MNIST digit database.\n3. Implement a convolution layer using just NumPy\n\n3. ML Algorithms:\n4. \n1. Compare random forest and decision tree models. Which works better?\n2. How would you train a classifier on an imbalanced dataset?\n3. What are the different types of learning rate optimizers?\n4. What is the cross-entropy loss? Is it convex or not?\n5. What is the difference between L1 and L2 regularization? When to use L1?\n6. What is the difference between MAE, MAPE, and MSE? How would you decide when to use which metric?\n7. Why do you use feature selection?\n8. What's the difference between R-CNN and YOLO?\n9. Assuming a clustering model's labels are known, how do you evaluate the model's performance?\n10. How do you prevent overfitting?\n\n5. ML System Design:\n6. \n1. What would you do to summarize an Instagram feed?\n2. How would you design a system to optimize Microsoft's go-to-market strategy?\n3. How would you detect if multimedia content being posted on Facebook violates the terms or contains offensive materials?\n4. Build a model to predict which people are most likely to take a loan from the bank.\n\nYou can answer a lot of these questions with enough preparation. If you are looking for a guided course that covers these questions, feel free to check out Interview Kickstart's offering for Machine Learning Engineer [ https://learn.interviewkickstart.com/course/machine-learning-interview-masterclass?utm_source=Quora&utm_medium=answer&utm_campaign=post&utm_term=What-are-some-of-the-most-important-questions-in-machine-learning&utm_content=PST ] roles. The course provides help in all aspects of interviewing, including profile building, salary negotiations, and more. It covers all of the above topics and offers mock interviews, practice questions, and extended support. The course is suitable for ML engineers with at least 2 years of experience and up to the L5 level of responsibilities.",
                "In general, questions of the form \u201chow would you recommend to study X\u201d, whether X be physics, ML, baking, or knitting, are challenging to answer because the answer depends on a quality that psychologists call \u201cintrinsic motivation\u201d.\n\nIM is defined as the act of doing of something purely for the pleasure of doing it, rather than doing it for some external reward. Experiments with monkeys about 50 years ago revealed that they would play endlessly with some mechanical puzzles thrown into their cages, despite the fact that doing so earned them no brownie points with their caretakers \u2014 no extra bananas or other rewards were provided, and yet the monkeys persisted in this seemingly pointless activity.\n\nI began the study of ML almost four decades ago in 1982, almost entirely by accident. I was training to be an electrical engineer, as I liked the combination of elegant math \u2014 information theory, Maxwell\u2019s equations, quantum theory of transistors \u2014 and practical impact of EE in the fast expanding world of digital technology of the early 1980s. I discovered an entirely new universe of ideas in Douglas Hofstadter\u2019s breakthrough book Godel, Escher, Bach: An Eternal Golden Braid, a once in a generation type book that earned its first time author the prestigious Pulitzer prize in nonfiction. After devouring the 800 odd pages of GEB \u2014 remember, this was not required reading for my EE courses in grad school \u2014 I convinced my mentors at IIT Kanpur to let me reorient my entire MS grad program towards AI and ML.\n\nLooking back on my decision, it seems almost a foolhardy move to have made in 1982. Why would I turn my back on an established EE engineering field with guaranteed employment and job security in favor of a new fangled field called \u201cmachine learning\u201d, which then in India and even in the US was barely a term that registered to any would be employer. Doing ML or AI in the early 1980s was a good way to become unemployable. But, somehow, I made this fateful decision, and never regretted this apparently risky and downright foolish decision to study AI, when it was in its infancy.\n\nOf course, being an early entrant into the field does come with its eventual rewards, and I have spent a fun-filled 35 years, mostly as an academic writing over 150 papers in ML and AI. The risk taking nature that I exhibited in 1982 didn't change when I got my first job at IBM Research in 1990. Once again, IBM in 1990 had no clue what to do with a \u201cmachine learning\u201d PhD, and threw me in with a newly formed robotics group (I had of course no knowledge of robotics). But, I discovered the very nascent field of reinforcement learning, and once again, threw all caution to the wind and explored the use of RL to train robots, now 25 years later, a field that has become much more popular. My 1992 Artificial Intelligence journal paper on using Q-learning to train mobile robots remains one my top cited articles:\n\nScienceDirect [ https://www.sciencedirect.com/science/article/abs/pii/0004370292900586 ]\n\nThe reason for this long prelude is that in 2018, machine learning (and AI) are far different fields than they were back in the early 1980s. What was then a foolish decision now seems entirely rational and predictable. Of course, anyone would choose to study ML or AI now, when it promises endless employment potential and a highly bankable CV. So, this gets me to finally addressing your question:\n\n1. Are you seeking to study ML so you can transition from your current career, whatever it is, to becoming a data scientist?\n2. Are you, instead, intrinsically motivated to learn ML, not because you seek to financially profit from your studies, but rather because you are just curious about the field?\n3. Are you, finally, seeking to delve into some exciting field of research, perhaps become a trailblazer in some new emerging technology or scientific field?\nDepending on the answers you give for each of these questions, the approach to studying ML may be entirely different. If, for example, you want to quickly become proficient in some data science skills, say Python, R, or Tensor Flow/Pytorch, then there are any number of online courses and tutorials that can help you get up to speed. Of course, be aware that in this case, you are going to be in competition with several million other people who all have the same idea, and whose CVs come flooding through our email inboxes each and every day. So, it\u2019s a good idea to learn some data science skills, but be aware of it may not immediately lead to the dream job that you were hoping to get.\n\nIf you are, instead, intrinsically motivated to learn ML, then, my congratulations to you! You can begin, as I did, with some interesting puzzle, and see how to solve it. Why, you can study the same exact problem I did way back in 1982, and see if modern deep learning can solve the classic \u201cBongard Problems\u201d, simple pattern recognition puzzles that can be very easy to solve (for humans), or fiendishly difficult, depending on the problem. Solve a few Bongard problems listed here, and see how well you do:\n\nYou see listed above on the left a set of geometric shapes, and an equal-sized but different set of shapes on the right. What\u2019s the rule that separates all the shapes on the left from that on the right? It\u2019s a simple geometric property, one that turns out to be very important in the theory of machine learning, incidentally.\n\nOK, you were able to quickly solve the above problem. How about this one below?\n\nHere is an interesting Quanta article on Bongard problems:\n\nBongard Problems and Scientific Discovery [ https://www.quantamagazine.org/bongard-problems-and-scientific-discovery-20170608/ ]\n\nWell, I did my M.S. thesis at the Indian Institute of Technology in 1983 (spring semester), on a machine learning system to solve the Bongard problems, working all night on a clunky huge Digital Equipment Corporation DEC-10 computer using an arcane language popular in AI then called LISP. I worked at night, because the terminals were all busy during the day. So each night, from 10 p.m., till about 4 a.m. in the morning, I wrote code that tried to solve the problems. That was the way I learned to do ML. You can do this as well today, with the far more powerful computers and GPU machines accessible today (probably a million times faster than the old Dec-10 I used).\n\n36 years later, I would be surprised if someone managed to solve the Bongard problems with today\u2019s ML technology. They are fiendishly hard, and remember, you have only 6 examples of each class. You can\u2019t do the au courant NIPS (NeurIPS) style MNIST cheat of assuming that each class has 10,000 or 100,000 examples. Sorry, 6 is all you get. Hey, 2 year old kids learn the concept of a \u201cdog\u201d from one or two examples. The patterns are also endless. It\u2019s not just 10 categories of digits, or a few thousand objects, like Imagenet. Good luck!\n\nFinally, if you are the third sort of person, looking for a new exciting field, one that is in its nascent state, then I would not recommend ML. It\u2019s far too mature a technology, far too well established, with millions of prospective entrants and a lot of competition. The most recent ML conference sold out its registration in under 10 minutes! So, what should you do?\n\nWell, you could do something like what I have started to do. I have begun exploring an entirely new field, one I call \u201cimagination science\u201d, which tries to understand this mysterious ability that humans have called \u201cimagination\u201d. If you do a web search of this term, you will not find any significant papers on it in the AI or ML conferences. I am giving a tutorial on imagination machines at the next AAAI conference in Hawaii in 2019 (January end). I\u2019m pleased to say there is, to my knowledge, not one accepted paper at AAAI on imagination. That\u2019s exactly why I picked this subject. It\u2019s one that is hugely important to all the scientific and technological progress that man has made over the past 5 or more centuries, and yet, remains almost entirely unexplored.\n\nOnce again, I must caution you that studying imagination is no guarantee of job employment. In the Bay Area or elsewhere, tech companies are not falling over themselves trying to recruit \u201cimagination science\u201d researchers or engineers (as they are trying to hire ML folks now). So, it\u2019s a risky business, but heck, you live life once (I say this even though I am a Hindu, and we are supposed to believe in reincarnation). What\u2019s the point of living, unless one takes a few risks?\n\nSo, there you have it. My three answers to studying ML, or in fact, not studying ML, depending on your motivation. On this Christmas morning of 2018, may this answer help you in some small way to achieve your dreams!",
                "Warning: this answer is overly long! Apologies and feel free to skip if you are one of those who wants a Twitter style 140 character answer.\n\nBy and large, and I don\u2019t mean to offend anyone, I don't like any of the textbooks in machine learning. For 15 years, I taught the fall introductory course in machine learning for graduate students at UMass, Amherst. It was a wonderful way to keep current in the field, as well as to shore up one\u2019s basic foundations in math, statistics, and optimization. Sadly, I found none of the current books do justice to the breadth of topics that constitutes modern machine learning. For example, I find it sacrilegious that all the texts avoid any mention of reinforcement learning. Really? I mean, how far can one get in building AI learning systems without RL? Unless you believe that when you began to learn as a baby, your parents kept around large stashes of labeled training data for you to pore over every day. Hmmm.. good luck with that. Strike one!\n\nThe second problem is that most books begin by essentially giving you in chapter 1, a dictionary of probability distributions to essentially \u201cmemorize\u201d. If there\u2019s a worse way to teach machine learning, I don\u2019t know what that would be. As the work on deep learning should convince everyone, one can get very far in machine learning without having to explicitly appeal to machinations involving the multivariate normal distribution or the exponential family. While no one doubts the value of probability distributions, they don\u2019t have to introduced at the beginning. Strike two!\n\nFinally, most of the standard books do a relatively poor job of explaining basic ideas in statistics. Take the classic EM algorithm. Every book in ML that I know introduces EM in the context of relatively complex problems, like mixture models or HMMs. This is in fact not the way EM was developed in statistics, but it was developed for much simpler problems where the basic intuition of EM is much easier to grasp (e.g., assume you can observe only 90% of the samples from a 1-dimensional Gaussian (normal distribution) with unknown mean and variance, and 10% of the samples are \u201cmissing\u201d. What do you do? It\u2019s quite easy to think of beginning with a simple idea like filling in an initial mean for all unobserved samples, and writing out a recurrence relation. One can easily show this EM method converges to maximize observed data likelihood. ). Books on EM in statistics are far better at explaining the full richness of EM. Strike three!\n\nSo, how is one to cope in the interim, while we wait for someone to write the ideal ML textbook (which doesn't suffer the three gotchas above). Learning machine learning can be analogized to a travel vacation, where you have 3 months to learn about a foreign destination, say Europe. So, where do you go, and how much time do you spend in each place? You could spend all 3 months in London or Paris, but that would not teach you what Europe is (this is essentially what a lot of ML textbooks currently do, they focus on the one topic that the author(s) happen to expert at). You could sign up for one of these crazy guided tours, which takes you to a new city every other day. You would certainly see a lot of Europe this way, but you would be exhausted at the end of it, and probably not remember very much (and most of your time would be inside a bus or a train).\n\nSo, some compromise between breadth and depth is obviously called for. First, understand what the broad scope of machine learning is. It includes many fields, ranging from supervised learning (the most well studied, and as far as biology goes, the most unnatural and implausible approach), to unsupervised learning (extremely important in biology as well as ML), and reinforcement learning (hugely important in biology, well studied in ML, ignored by ML textbook writers). Before learning ML methods, and I can't emphasize this enough, learn the ML problem formulations! Einstein was once asked what he would do if he had one hour to live, and was told his life depended on solving some problem. He said he would take 55 minutes to understand the problem and ask the right question(s). In the remaining 5 minutes, he would spend solving the problem. This is a good lesson to keep in mind. Far too many students of ML get obsessed over methods. Realize that methods change all the time. Problems rarely change. Far better to learn things that don\u2019t change first than obsess over things that change every month or year.\n\nMy approach gradually evolved to become what was exactly the opposite of modern textbook writers. I began with unsupervised learning, because I consider this the foundation of all biological and machine learning. In the beginning, when you are born, and are barely functional (except bawling), labels make little sense. Your parents can whisper sweet nothings in your ears, and attach labels to objects all they can, but devoid of language and meaning, these make no sense to you. Clearly, this is the crucial stage where basic representations get learned (and there is a lot of neuroscience to back this up). So, learning representations is the basic problem. Start by learning the simplest methods, like principal components analysis (PCA), and canonical correlational analysis (CCA) for multiple datasets. Progress to more sophisticated methods that exploit the geometry of the data space, such as manifold learning methods (e..g, Laplacian eigenmaps). The need for a good dose of basic math (linear algebra) will become quickly clear to you. Some basic neural implementations of methods like PCA will be helpful in seeing how to turn batch type methods into incremental gradient type methods.\n\nOnce unsupervised learning methods are broadly understood, progress to reinforcement learning. This is the stage where babies begin to crawl around, learning to walk, and simple behaviors are mastered (picking up objects, still with no sense of what they are called or what \u201clanguage\u201d is). A good understanding of basic reinforcement learning will help also make the connection between machine learning and AI, robotics, and control theory. Sutton and Barto\u2019s textbook on RL should be on everyone\u2019s reading list. Eminently readable and highly engaging. A new edition is in the works and should be coming out soon.\n\nFinally, supervised learning can be studied, and there are of course a zillion methods and approaches, all clamoring to be heard. How to make sense of them all? A golden rule is that by and large, the simpler the method, the more longer lasting it will be, and the wider it will be used. Everyone should learn and master linear models of classification and regression, and linear models are by far still the most widely used models in science. Why? Because they are interpretable and understandable. Scientists care about explainability ultimately, not accuracy.\n\nI can\u2019t resist quoting the great Nobel-prizewinning economist Ronald Coase, who till recently taught at the University of Chicago (one of the best economics departments in the world). He said, and I quote:\n\n\u201c\u2022A theory is not like an airline or bus timetable. We are not interested simply in the accuracy of its predictions. A theory also serves as a base for thinking. It helps us to understand what is going on by enabling us to organize our thoughts.\n\n\u2022Faced with a choice between a theory which predicts well but gives us little insight into how the system works and one which gives us this insight but predicts badly, I would choose the latter, and I am inclined to think that most economists would do the same.\u201d\n\nThis is in a nutshell, the central quandary in modern machine learning. The best methods in terms of experimental performance on artificial datasets, come from deep learning. Unfortunately, these are the very methods we understand the least, and are essentially opaque. Linear models, on the other hand, are extremely well understood, and very explainable. But, they don\u2019t perform as well as deep learning methods. So, textbook writers face an additional quandary. They can cover the latest methods, but give up any hope of providing a rich theory and comprehensibility, or they can focus on simple linear models, provide a comprehensive theory and stress on explainability, but ignore the fact that the methods covered are not going to be the ones that win some \u201cbake off\u201d artificial competition. Coase says clearly what side of this debate he would choose.\n\nSo, is there a \u201cbest\u201d among the bad choices we do have? If forced to pick one book, I would choose \u201cLearning from Data\u201d by Abu-Mostafa and colleagues. It wisely sticks to things we understand well and can analyze well: linear models. It gives a great introduction to computational learning theory. It clearly explains basic concepts like overfitting. And finally, it does not \u201csnow\u201d you with probability distributions. Best of all, it is short and inexpensive.\n\nLearning From Data - A Short Course [ http://amlbook.com ]\n\nMy second choice would have to be Richard Sutton and Andrew Barto\u2019s timeless classic on reinforcement learning. Full of insight, and highly readable. It sets the standard by which all ML textbooks should be judged. Again, it is relatively short, relatively inexpensive, and a new edition is coming out soon\u2026.\n\nReinforcement Learning: An Introduction (Adaptive Computation and Machine Learning): Richard S. Sutton, Andrew G. Barto: 9780262193986: Amazon.com: Books [ https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262193981 ]\n\nWhich leaves one final book to recommend on the requested three books on ML shortlist. For this last choice, I would not pick an ML textbook, but rather something that nicely complements the above two choices. The central concept in statistics that ML students should learn is likelihood functions. If there is ONE idea everyone in ML should learn, it is why statistics does not equal probability. In my beginning lecture, I always ask students to tell me how statistics is different from probability theory. No one invariably gets this right. So, to understand this, I highly recommend this gem of a book called \u201cIn All Likelihood\u201d.\n\nAmazon.com: In All Likelihood: Statistical Modelling and Inference Using Likelihood (9780199671229): Yudi Pawitan: Books [ https://www.amazon.com/All-Likelihood-Statistical-Modelling-Inference/dp/0199671222/ref=sr_1_1?s=books&ie=UTF8&qid=1500837166&sr=1-1&keywords=in+all+likelihood ]\n\nYudi Pawitan\u2019s book is a gem, and uses many examples, most of them simple, to communicate the essence of likelihood functions. This is the one concept in statistics that separates it from probability theory, and it is central to machine learning. Again, a relatively cheap book, and very short in length!\n\nThere you have it! All these three books are 1) short 2) inexpensive as textbooks go 3) very well written and have great reviews on Amazon.\n\nWhatever course you take to learn about \u201cmachine learning\u201d, I envy you. This is an unforgettable experience, and my own journey began more than 35 years ago, when I discovered the book \u201cGodel, Escher, Bach\u201d by Douglas Hofstadter. It opened up a whole new world for me, one that I have spent most of my life happily exploring the ML problem. These days, I\u2019m more interested in another topic, but this message is already getting too long, so that will remain for another day!",
                "First, an important background citation: \n\nBreiman, L. 2001. \u201cStatistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).\u201d Statistical Science 16:199\u2013231.\n\nSupervised Learning\n\nRegression:\tPanik, M. J. 2009. Regression Modeling: Methods, Theory, and Computation with SAS. Boca Raton, FL: CRC Press. (Disclosure: my favorite regression book.)\n\nDecision tree:\tBreiman, L., Friedman, J., Olshen, R., and Stone, C. 1984. Classification and Regression Trees. Belmont, CA: Wadsworth.\n\nRandom forest:\tBreiman, L. 2001. \u201cRandom Forests.\u201d Machine Learning 45:5\u201332.\n\nGradient boosting:\tFriedman, J. H. 2001. \u201cGreedy Function Approximation: A Gradient Boosting Machine.\u201d Annals of Statistics 29:1189\u20131232.\n\nNeural network:\tRumelhart, D. E., Hinton, G. E., and Williams, R. J. 1986. \u201cLearning Representations by Back-Propagating Errors.\u201d Nature 323:533\u2013536.\n\nSupport vector machine:\tCortes, C. and Vapnik, V. 1995. \u201cSupport-Vector Networks.\u201d Machine Learning 20:273\u2013297.\n\nNa\u00efve Bayes:\tFriedman, N., Geiger, D., and Goldszmidt, M. 1997. \u201cBayesian Network Classifiers.\u201d Machine Learning 29:131\u2013163.\n\nNeighbors:\tCover, T. and Hart, P. 1967. \u201cNearest Neighbor Pattern Classification.\u201d IEEE Transactions on Information Theory 13:21\u201327.\n\nGaussian processes:\tSeeger, M. 2004. \u201cGaussian Processes for Machine Learning.\u201d International Journal of Neural Systems 14:69\u2013106.\n\nUnsupervised Learning\n\nA priori rules:\tAgrawal, R., Imieli\u0144ski, T., and Swami, A. 1993. \u201cMining Association Rules between Sets of Items in Large Databases.\u201d ACM SIGMOD Record 22:207\u2013216.\n\nk-means clustering:\tHartigan, J. A. and Wong, M. A. 1979. \u201cAlgorithm AS 136: A k-Means Clustering Algorithm.\u201d Journal of the Royal Statistical Society, Series C  28:100\u2013108.\n\nGloVe Term Embeddings: Jeffrey Pennington, Richard Socher, and Christopher D Manning. \"GloVe: Global Vectors for Word Representation.\"\n\nMean shift clustering:\tCheng, Y. 1995. \u201cMean Shift, Mode Seeking, and Clustering.\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence 17:790\u2013799.\n\nSpectral clustering:\tVon Luxburg, U. 2007. \u201cA Tutorial on Spectral Clustering.\u201d Statistics and Computing 17:395\u2013416.\n\nKernel density estimation:\tSilverman, B. W. 1986. Density Estimation for Statistics and Data Analysis. Vol. 26. Boca Raton, FL: CRC Press.\n\nNon-negative matrix factorization:\tLee, D. D. and Seung, H. S. 1999. \u201cLearning the Parts of Objects by Non-negative Matrix Factorization.\u201d Nature 401:788\u2013791.\n\nKernel PCA:\tSch\u00f6lkopf, B., Smola, A., and M\u00fcller, K.-R. 1997. \u201cKernel Principal Component Analysis.\u201d In Artificial Neural Networks\u2014ICANN'97, 583\u2013588. Berlin: Springer.\n\nSparse PCA:\tZou, H., Hastie, T., and Tibshirani, R. 2006. \u201cSparse Principal Component Analysis.\u201d Journal of Computational and Graphical Statistics 15:265\u2013286.\n\nSingular value decomposition:\tGolub, G. H. and Reinsch, C. 1970. \u201cSingular Value Decomposition and Least Squares Solutions.\u201d Numerische Mathematik 14:403\u2013420.\n\nSemi-supervised Learning*: \n\nDenoising autoencoders: Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.A. 2008. \u201cExtracting and Composing Robust Features with Denoising Autoencoders.\u201d Proceedings of the 25th International Conference on Machine Learning. New York: ACM.\n\nExpectation maximization:\tNigam, K., McCallum, A.K., Thrun, S. and Mitchell, T.  2000. \"Text Classification from Labeled and Unlabeled Documents using EM.\" Machine Learning 39:103-134.\n\nManifold regularization:\tBelkin, M., Niyogi, P., and Sindhwani, V. 2006. \u201cManifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples.\u201d The Journal of Machine Learning Research 7:2399-2434.\n\nTransductive support vector machines:\tJoachims, T. 1999. \u201cTransductive Inference for Text Classification Using Support Vector Machines.\u201d Proceedings of the 16th International Conference on Machine Learning. New York: ACM.\n\nWord2Vec Term Embeddings: Mikolov, Tomas, et al. \"Efficient estimation of word representations in vector space.\" arXiv preprint arXiv:1301.3781 (2013).\n\n*In semi-supervised learning, supervised prediction and classification algorithms are often combined with clustering. The algorithms noted here provide semi-supervised learning solutions directly. \n\nComments and concerns welcome.",
                "This answer attempts the very ambitious problem of producing an approximately complete list. Please leave comments and tell me what's wrong and/or what is missing -- right now it's a pretty small list so I've surely left something off.\n\nIntroductory remarks\nI think of most of ML as a contribution to the question of how we can perform statistical inference. So I will attempt to describe these books in terms of how they approach this problem (e.g., whether they are theoretical or practical, frequentist or Bayesian, and so on). Some of them will not be about ML per se, but will be about subjects on which ML people depend greatly. Generally I judge this based on whether you could publish something about it at a conference like NIPS or ICML, or whether ML people are likely to make up a significant amount of the audience when talks are delivered on the subject.\n\n\n\nIn no particular order:\n\n\nFoundations of Machine Learning, Mehryar Mohri, Afshin Rostamizadeh, Ameet Talwalkar [ http://www.amazon.com/dp/026201825X ]\nAn ambitious book that covers an impressive subset of the theoretical basis of machine learning. Includes excellent treatment of fundamentals (learning complexity, kernel methods, boosting, PAC learning, regression), as well as some subects that are almost never covered properly (ranking, multiclass, online). Additionally gives a nice ML perspective of some things (JL-lemma) that usually are not talked about well in textbooks.\n\nLearning From Data, Yaser S. Abu-Mostafa, Malik Magdon-Ismail, Hsuan-Tien Lin [ http://www.amazon.com/gp/product/1600490069 ]\nSort of like Foundations of Machine Learning, but seems to be built for an undergrad curriculum. Contains stuff about the theoretical underpinnings, but omits the more complicated theory. It is well-written and intuitive, with theoretically intense parts clearly marked and roped off for those who are, e.g., too mathematically \"young.\"\n\nInformation Theory, Inference, and Learning Algorithms, David J. C. MacKay [ http://www.amazon.com/Information-Theory-Inference-Learning-Algorithms/dp/0521642981 ] [free pdf [ http://free pdf ]]\nA presentation of statistical modeling that is unified across information theory, coding theory, statistics, physics, ML, computational biology, cryptography, and signal processing. It is fantastically and uniquely illuminating. Contains my favorite treatment of many subjects (Ising models, Monte Carlo methods, fountaint codes, etc.).\n\nAll of Statistics, Larry Wasserman [ http://www.amazon.com/All-Statistics-Statistical-Inference-Springer/dp/0387402721 ]\nMost of ML is collected into a series of tasks (regression, classification, clustering, etc.) whose only commonality is that they are types of statistical inference. Among the virtues of this remarkable book is that it approaches the subject of statistical inference in a very general fashion, and in a way that is graceful and approachable, but still rigorous. This is a good starting point for ML, as most of ML is a specialization of some subset of the topics presented here. IMO, it is the best book of its type.\n\nMachine Learning: A Probabilistic Perspective, Kevin P. Murphy [ http://www.amazon.com/dp/0262018020 ]\nMostly a roundup of the recent explosion in Bayesian inference. Unlike other books on this list, ML:APP seems to have been assembled by collecting papers, stapling them all together, making the notation consistent, and adding a bit of glue to unify them. For example, most of the figures are pulled (with permission) directly from other papers. This is not a bad thing! I'm very glad someone did it. But it does sometimes feel rushed, and there are many typos.\n\nPattern Recognition and Machine Learning, Christopher M. Bishop [ http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738 ]\nBishop's book is useful mainly because it is quite thorough, and contains a good treatment of many of the esoteric corners of well-known methods. But I also found it to be quite dense, and often didn't understand exactly what Bishop was saying, or how he derived something. I find that it is very useful when I consult it about a subject after having already read 2 or 3 other texts.\n\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction, Trevor Hastie, Robert Tibshirani, Jerome Friedman [ http://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics/dp/0387848576 ] [pdf [ http://statweb.stanford.edu/~tibs/ElemStatLearn/ ]]\nThe quintessential (frequentist) ML text. It's more computationally-minded than I would have expected from classically trained statisticians, but its lack of robustly Bayesian material dates it in some ways. (I'm not saying one perspective is better, I'm saing you should know both.\n\nProbabilistic Graphical Models: Principles and Techniques, Daphne Koller, Nir Friedman [ http://www.amazon.com/Probabilistic-Graphical-Models-Principles-Computation/dp/0262013193 ]\nA dense, comprehensive, and canonical treatment of graphical models. Covers Markov Random Fields, Contitional Random fields, Bayes nets, and so on.\n\nGaussian Processes For Machine Learning, Carl Edward Rasmussen, Christopher K. I. Williams [ http://www.amazon.com/Gaussian-Processes-Learning-Adaptive-Computation/dp/026218253X ] [free pdf [ http://free pdf ]]\nThe classical treatment of the Gaussian process, an important class of statistical model.\n\n\n\n\n\nIf you enjoyed this, you might like my Quora blog [ https://antics.quora.com/ ], which is mainly about this type of thing.",
                "It is hard to answer this question especially since I don't have any clue about your background. So, I will give you my background when I started with machine learning. I was a core engineer with little to zero knowledge in linear algebra, probability and optimization (note that I did learn probability, linear algebra, and optimization as part of my school and engineering coursework but they lacked the necessary rigour which would translate into learning ML). Having this background, this is how my ML learning route went:\n1. I started with Neural Network by Raul Rojas during my Bachelors [A]. Although, the title says, \"A Systematic Introduction\", it is hardly a systematic introduction. On the other hand, however, it gives sufficient details about the algorithm - enough to be able to implement the neural network on your own which is exactly what I did. My bachelor's final year project was to implement a Neural Network Simulator where I implemented 5 popular neural network all with a GUI etc. Another book that I came across much later and which you might prefer is [B].\n2. When I was doing my Master's, I did RA in an NLP group for a brief time, during which I had implemented Hidden Markov Model and Viterbi algorithm. The reference I had used at the time was [C].\n3. During my Master's, I did a course in \"Learning with Kernels\" (Department of Computer Science and Engineering [ http://www.cse.iitk.ac.in/teaching/courses/CS678.html ]). Our primary reference was [D]. This was the first book that taught me SVM - primals and duals and it actually gives a pseudocode for SMO algorithm for training SVMs. So, I went ahead and implemented the SMO algorithm (that is my own SVM) just for fun.\n4. As part of my Master's thesis, I had to implement Online Support Vector Regression [E].\n5. When I joined Yahoo! Research Lab as a Research Engineer, I was working with an amazing researcher along with whom I was implementing a graphical model for the first time. I had to write competing algorithms, so I also implemented HITS Algorithm [F] and TruthFinder [G].\n6. Later on, I worked on a model, that was similar in spirit to [H], so I implemented [H] as well so as to understand the model better. Working on this further, later lead to our paper in WSDM 2011.\n\n7. When I joined my PhD, I did a course in \"Convex Optimization\" ( http://www.cse.iitb.ac.in/saketh/teaching/cs709.html ). Our primary reference was [I]. At the end of the course, we had to do a course project, so we implemented cutting plane based Multiple Kernel Learning from this [J] paper. Thanks to that, I was able to utilize the MKL strategy and cutting plane technique in my recent ICML 2014 work.\nThe point I am trying to distinctly drive here is the fact that most of my learning has come by implementing stuff. I had nobody to tell me which books to follow and at the time, the internet did not have the kind of resources that it has today. I just picked different things and I just started implementing it. I would fail at times and succeed at times. However, the knowledge I gained during those times cannot be compared to anything. The knowledge from books augmented those experiences and enhanced my understanding even further.\n\nSo, my personal suggestion is to pick up any book, be it PRML by Chris Bishop or Machine Learning by Tom Mitchell or Andrew Ng's excellent online videos or this amazing online video course [K] by Prof. Yasser Abu-Mostafa, just go ahead and implement whatever you can or whatever you want to. There is abundance of resource out there.\n\n[A] Neural Networks - A Systematic Introduction [ http://page.mi.fu-berlin.de/rojas/neural/index.html.html ]\n[B] Practical Neural Network Recipies in C++: Masters: 9780124790407: Amazon.com: Books [ http://www.amazon.com/Practical-Neural-Network-Recipies-C/dp/0124790402 ]\n[C] Introduction [ http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/main.html ]\n[D] Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond (Adaptive Computation and Machine Learning): Bernhard Schlkopf, Alexander J. Smola: 9780262194754: Amazon.com: Books [ http://www.amazon.com/Learning-Kernels-Regularization-Optimization-Computation/dp/0262194759 ]\n[E] Page on lanl.gov [ http://public.lanl.gov/jt/Papers/aosvr-nc.pdf ]\n[F] HITS algorithm [ http://en.wikipedia.org/wiki/HITS_algorithm ]\n[G] Page on uiuc.edu [ http://www.cs.uiuc.edu/~hanj/pdf/kdd07_xyin.pdf ]\n[H] Page on aclweb.org [ http://aclweb.org/anthology//D/D09/D09-1064.pdf ]\n[I] http://www.amazon.in/Lectures-Modern-Convex-Optimization-Applications/dp/0898714915\n[J] http://eprints.pascal-network.org/archive/00006851/01/NIPS2009_0879.pdf\n[K] \nhttps://www.youtube.com/watch?v=mbyG85GZ0PI&amp;list=PLD63A284B7615313A\n",
                "A few great ones:\n\nThe Mathematics of Learning: Dealing with Data\nTomaso Poggio and Steve Smale\nhttp://www.ams.org/notices/200305/fea-smale.pdf\n\nNear Optimal Signal Recovery From Random Projections: Universal Encoding Strategies?\nEmmanuel Candes and Terence Tao\nhttp://statweb.stanford.edu/~candes/papers/OptimalRecovery.pdf\n\nTopology and Data\nGunnar Carlsson \nhttp://comptop.stanford.edu/u/preprints/topologyAndData.pdf\n\n\nSemi-Supervised Learning on Riemannian Manifolds\nMikhail Belkin, Partha Niyogi\nhttp://people.cs.uchicago.edu/~niyogi/papersps/BNMLJ.pdf\n\n\nStochastic Methods for L1-regularized Loss Minimization\nShai Shalev-Shwartz, Ambuj Tewari \nhttp://jmlr.org/papers/volume12/shalev-shwartz11a/shalev-shwartz11a.pdf\n\nand its implementation, in graphlab, described here:\n\nParallel Coordinate Descent for L1-Regularized Loss \nJoseph K. Bradley, Aapo Kyrola, [ http://, ] Danny Bickson, Carlos Guestrin\nhttp://www.select.cs.cmu.edu/publications/paperdir/icml2011-bradley-kyrola-bickson-guestrin.pdf\n\nand also the liblinear implemenation:\n\nA Comparison of Optimization Methods and Software for Large-scale L1-regularized Linear Classication\nGuo-Xun Yuan, Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin\nhttp://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf\n\n\nLearning the parts of objects by non-negative matrix factorization\nDaniel D. Lee & H. Sebastian Seung\nhttp://www.nature.com/nature/journal/v401/n6755/abs/401788a0.html\n\nand the modern form of NMF, which I prefer to use:\n\nConvex and Semi-Nonnegative Matrix Factorizations\nChris Ding, Tao Li, Michael I. Jordan\nhttp://www.cs.berkeley.edu/~jordan/papers/ding-li-jordan.pdf",
                "I recommend Learning From Data (LDA) by Yaser Abu-Mostafa. I tutored for the Machine Learning class at my university and later worked as a TA for the class, and this is the book we used. More concretely, out of 153 reviews on Amazon, ~78% of reviewers gave it 5 stars. It\u2019s an amazing book.\n\nWhy is this a great book to start your machine learning journey? Basically it is a simultaneously accessible, deep, and concise (~200 pages) discussion of the fundamentals of machine learning. It covers the ideas and concepts that are applicable to every machine learning algorithm from neural nets to random forests. While there are plenty of ml books out there, LDA is the one book I\u2019ve seen that covers this material at a reasonably deep level in an accessible, concise way.\n\nTo elaborate, note that you can view the body of knowledge that is \u201cmachine learning\u201d as two chunks of information. One chunk consists of a set of specific algorithms. These algorithms are characterized by their ability to \u201clearn\u201d, meaning that they improve at what they do from experience aka seeing more data. Examples include random forests, support vector machines, KNN, linear models, neural nets, etc.\n\nThe second chunk consists of concepts that underly all of those algorithms - concepts that let you understand why the algorithms work (or don\u2019t work), how to make them better, how to evaluate them, etc. Examples include overfitting, bagging, boosting, regularization, VC dimension (lets you prove that learning is possible), bias-variance tradeoff, cross validation etc.\n\nLDA is fantastic in that it provides a concise yet deep coverage of the \u201cunderlying concepts\u201d chunk of information.\n\nHere is an example to illustrate why this is so important. Imagine you want to predict the price of a house given info like total square footage, number of rooms, etc. You start off with a dataset describing 10,000 houses. This data contains the actual prices of these houses along with the \u201cpredictor\u201d variables total square footage, number of rooms, etc. You use this data to \u201ctrain\u201d (aka create) a machine learning algorithm that can take in predictor variables and return a predicted price. Note, the process of training the algorithm depends on the specific algo you use - different ml algorithms are trained in different ways (neural nets use back propagation, stochastic gradient descent with logistic regression, etc.).\n\nAfter you have your algorithm, you want to evaluate how well it works. To start, you select a house from your initial dataset (the data you used to train the algorithm). You feed its predictors into the algorithm and a predicted price is returned. You are happy to see the prediction is close to the true price. You try this a few more times and continue to observe that the predictions are close to the true values.\n\nNext you say, ok that\u2019s great. I wonder how well the algorithm will predict prices of houses that weren\u2019t in the dataset used to train the algorithm. This is key, because the algorithm would be almost useless if it could only predict the prices of houses in our original dataset; we already know the prices of those houses. We want to predict the prices of other houses.\n\nSo we get some new housing data. This data has prices and the same predictor variables as our original dataset. But our algorithm hasn\u2019t \u201cseen\u201d this data meaning that the data wasn\u2019t used to train the algorithm. We feed this data into the algorithm and to our dismay, the predictions it returns are really bad - they are very different from the true prices. It looks like we can\u2019t use this algorithm to predict housing prices with any reasonable degree of confidence.\n\nNow this is where LDA comes in. LDA covers this specific problem which is referred to as \u201coverfitting\u201d. It explains where overfitting comes from and how this is one of the core problems of machine learning that practitioners deal with all the time. Further, LDA explains techniques such as regularization, bagging, boosting, etc. that can likely improve the model\u2019s ability to make predictions. Further, LDA covers how you can quantify how confident you should be in your model\u2019s predictions after you improve it.\n\nAll to say, LDA - in 200 pages - covers those core principles that guide you as you develop you models.\n\nBest of luck, I hope this post helps you on your way!",
                "Machine Learning is basically trying to find a function which closely models your data. So, say you have one attribute(data) and your data is in the form (x,y) and looks like (2,6), (3,9), (5,15), (1,3) ... where x is the attribute and y is the target. Just by looking at the data you can understand that your data follows a relation Y = 3X (Y is dependent and X is dependent). This is of course a naive case and in real life, the function is not so easy to learn. Unlike the linear relationship, we have complex polynomial and even exponential relationships. And that is why we apply machine learning algorithms to understand how do we build a model (function) from existing data which can predict any new data.\n\nOccam's razor works everywhere. So if your data is simple, simpler techniques are better. If its complex, complex are better. Also, if your data has lot of errors and mislabeling, again simple methods are better. Complex methods might overfit the training set.\n\nSo basically you get the idea, you want to learn functions. There are simple non-parametric techniques like Decision Trees and k-Nearest Neighbors and other parametric models (like SVMs, linear regression etc). Now, generally with small data sets with not many attributes (or sparse data), non-parametric models do good. For moderate sized data, linear SVMs and for regression, linear regression and regularized linear regressors like lasso, ridge do quite well.\n\nNow, with more data, Neural Networks tend to do better (and also break the trade-off point for using them. Generally, more much more computationally expensive). For e.g. consider learning how to play Go which has a 19*19 board with more combinations than atoms in universe possible. Or, say learning how to classify tons of images. As you can imagine, the relationship between a ton of images cannot be modeled by a simple linear function. Convex functions are simpler as they have one maxima or minima i.e. local maxima or minima is the global one. This is not the case with a non-convex function. Hence, learning is hard. NNs (and in particular, Deep Learning) help to model such a complex function. Usually deep learning models are quite complex, and we do not have clear theoretical models for tweaking it. However, there are a great deal of heuristics.",
                "None other than this.\n\nEven if you read the entire book, you will learn a lot about the machine learning algorithms and system, along with common Deep Learning solutions. Also, there is a great NLP fundamental about smaller and large models along with Transformers too.\n\nIt also contains a full end to end ML project which gives you enough insights about Data science too.\n\nMost of the code in this book are a bit overkill for specs and requirements, but that's how stuff is done in development, expecially in ML if you think about. However, just every exercise teaches you a new thing about Machine Learning programming using tried and tested techniques.\n\nThis is where I cannot compare half knowledged YouTube videos to code snippets in this book, which are there to just teach you how to do the stuff normally and professionally like a senior ML engineer. Knowing train_test_split is fine but you should also know about other test set separation methods too and how to handle data snooping bias, which is very critical and important problem to deal with.\n\nThe book explains everything freaking single thing in detail and you cannot even skip 1 single line. It took me a month to properly understand the entire chapter 2, which was very very very well written and described. Kudos to Aurelien Geron.\n\nOverall, this is your complete ML book.",
                "If you are asking about the data science and others that means you are interested in this field. If you want to know only just for your knowledge then you can read my answer and check other materials on the internet is sufficient for you but if you want to make a successful career in this field then you need to join data science courses.Many institutes provide data science courses I would suggest you to join Learnbay for complete knowledge and learn required skills for data science career.\n\nHere i will mention details about these terms-\n\nData analysis is a procedure of investigating ,transforming , cleaning and training the data with the aim of finding some useful information, recommending conclusions and helping in decision making.\n\nData analytics consist of data collection and in general inspect the data and it has one or more usage. It is a conventional form of analytics which is used in many ways.\n\nBig Data refers to the vast volume of data that is difficult to store and process in real-time. The act of accessing and storing large amounts of information for analytics has been a long time.\n\nData Mining is also referred to as data or knowledge discovery, is the process of analyzing data and transforming it into insight that informs business decisions.\n\nMachine Learning means that the programs depend on some data, used as a training set to fine tune some models and algorithms parameters. ML is a group of techniques used by data scientists that allow computers to learn from data.\n\nData Science unifies statistics, data analysis and machine learning to understand and analyse phenomena through data.Data science encloses many break through technology like-AI, DL and e.t.c\n\nNow i will tell you about Learnbay courses-Learnbay provide IBM Certified data science courses. They designed their courses according to working professionals and their courses are also suitable for beginners and non-programmers. These courses are-\n\nData science and Artificial Intelligence course-\n\nFor the ones having 1 years of working experience.\n\nDuration is 6 months.\n\nArtificial Intelligence and Machine Learning Course-\n\nFor the ones having 5+ years of working experience\n\nDuration is 8 months.\n\nData science course for Managers and Leaders\n\nFor the ones having 9+ years of working experience\n\nDuration is 9 months.\n\n-They provide special support for Beginners and non-programmers\n\n- Their mentors are Industry\u2019s Experts\n\n-They provide a job assistance program with resume preparation and mock interviews.\n\n-They provide a Full -stack program\n\nThese programs are available in different cities like Mumbai, Hyderabad, Delhi, Pune, Chennai and Bangalore.\n\nHope it will help you!!",
                "In today's data-driven world, it's essential to understand the distinctions between various terms commonly used in the field of data and analytics. Let's explore the differences between Data Analytics, Data Analysis, Data Mining, Data Science, Machine Learning, and Big Data to gain a clearer understanding of these concepts.\n\nData Analytics: Data Analytics involves examining large datasets to uncover meaningful insights, trends, and patterns. It focuses on deriving actionable insights from data to make informed business decisions. For instance, a retail company might use data analytics to analyze customer purchasing patterns and optimize its inventory management strategies.\n\nData Analysis: Data Analysis is a broader term that encompasses various techniques used to examine data, draw conclusions, and solve problems. It involves inspecting and cleaning datasets, applying statistical methods, and employing visualization techniques to gain insights. An example of data analysis would be analyzing sales data to identify key performance indicators (KPIs) for a marketing campaign.\n\nData Mining: Data Mining refers to the process of extracting valuable information or knowledge from large datasets. It involves identifying patterns, relationships, or anomalies in the data that may not be immediately apparent. Data mining techniques can be used in various domains, such as customer segmentation, fraud detection, or predicting user behavior.\n\nData Science: Data Science is an interdisciplinary field that combines elements of statistics, mathematics, and computer science to extract knowledge and insights from data. It encompasses all stages of the data lifecycle, including data collection, cleaning, analysis, and interpretation. Data scientists utilize a wide range of techniques and tools to solve complex problems and make data-driven decisions. I found online resources such as Logicmojo, Upgrad, Great Learning and Udacity providing data science courses. I took the Logicmojo data science course to be incredibly helpful in acquiring these skills. They provided job assistance as well. At Logicmojo, they offer a comprehensive data science course that covers all the essential concepts and tools you need to succeed in this field. Whether you're just starting out or looking to advance your career, our course provides you with the knowledge and skills you need to become a successful data scientist.\n\nMachine Learning: Machine Learning is a subset of Artificial Intelligence (AI) that focuses on developing algorithms and models that enable computers to learn from data and make predictions or decisions without being explicitly programmed. It involves training models on historical data and using them to make accurate predictions on new, unseen data. A common example of machine learning is spam email filtering, where the model learns to distinguish between spam and legitimate emails based on past examples.\n\nBig Data: Big Data refers to extremely large and complex datasets that cannot be effectively processed using traditional data processing applications. It involves the storage, processing, and analysis of vast amounts of data to extract insights and value. Big Data technologies, such as Hadoop or Apache Spark, enable organizations to handle and derive value from these massive datasets.\n\nTo summarize:\n\n * Data Analytics focuses on extracting actionable insights from data for business decisions.\n * Data Analysis encompasses techniques used to examine and draw conclusions from data.\n * Data Mining involves discovering patterns and relationships in large datasets.\n * Data Science combines statistics, mathematics, and computer science to extract knowledge from data.\n * Machine Learning enables computers to learn from data and make predictions or decisions.\n * Big Data refers to the processing and analysis of large and complex datasets.\nAs we've seen, there are similarities and differences between data analysis, data science, data mining, machine learning, and big data. Despite the fact that the topics are closely related in many ways, each has its own implications, application, and areas of expertise. If you are interested in any of the subjects then I will say it's a great time to get hands-on experience as they have lucrative careers.\n\nAll the best!!",
                "I had been wanting to take a stab at this one since a few days, but it always looked like an enormous task, because this question has used too many words. In addition, this is a question on which a lot of people have their eyes, and a lot of others have already written elaborate answers.\n\nLet me first re-order all the important words:\n\n * Big data\n * Data mining\n * Data analysis\n * Analytics\n * Machine learning\n * Data science\nImagine that you want to become a data scientist, and work in a big organization like Amazon, Intel, Google, FB, Apple and so on.\n\nHow would that look like?\n\n * You would have to deal with big data, you would have to write computer programs in SQL, Python, R, C++, Java, Scala, Ruby\u2026and so on, to only maintain big-data databases. You would be called a database manager.\n * As an engineer working on process control, or someone wanting to streamline operations of the company, you would perform Data Mining, and Data Analysis; You may use simple software to do this where you would only run a lot of codes written by others, or you may be writing your elaborate codes in SQL, Python, R and you would be doing data mining, data cleaning, data analysis, modeling, predictive modeling and so on.\n * All this will be called Analytics. Several software exist to do this. One popular one is Tableau. Some others are JMP and SAS. Lot of people do everything online where a SAP based business intelligence setup can be used. Here, simple reporting can be done easily.\n * Further, you would then be able to use machine learning to derive conclusions, and come up with predictions, wherever analytical answers are not possible. Think of analytical answers as [If/then] type of computer programs, where all the input conditions are already known, and only a few parameters change.\n * Machine learning uses statistical analysis to partition data. An example would be this: Read the comments written by various people on Yelp, and predict from the comments whether the person would have marked a restaurant 4 star or 5 star.\n * If that is not enough, you would be able to use deep learning as well. Deep learning is used to process data such as musical files, images, even text data such as natural languages, where data are enormous, but their type is very diverse.\n * You would use everything to your advantage ~ analytical solutions, partitioning data, hacking mindset, automation by programming, reporting, deriving conclusions, making decisions, taking actions, and telling stories about your data.\n * Last but not the least, a part of this will be happening on cruise control, where you may not be there physically, but the programs you may have created would do most of the things themselves. Probably if you take it to the level of AI, one day it may get smarter than you, needless to say it would already be faster than you. One day it can go to the level that it can surprise you with the solutions that you may not even have imagined.\n * Now you are a data scientist, and what you would do is called Data-science.\n * Whatever you would do may or may not be seen by people outside your company such as people asking Alexa various questions if you work for Amazon, or people asking questions to ok Google if you work for Google. Or they may not be getting to see anything you do. Your functions would be helping the companies engineer things better.\n * To do all this, you may need lots of expertise in handling data and knowledge of a few programming languages.\n * One popular data science Venn Diagram I have seen on internet is here: Note that a data scientist is at the intersection of a lot of things. Communication, statistics, programming, and business.\n * Read also:\n * Rohit Malshe's answer to How do I learn machine learning? [ https://www.quora.com/How-do-I-learn-machine-learning-1/answer/Rohit-Malshe ]\n * Rohit Malshe's answer to How should I start learning Python? [ https://www.quora.com/How-should-I-start-learning-Python-1/answer/Rohit-Malshe ]\n * Rohit Malshe's answer to What is deep learning? Why is this a growing trend in machine learning? Why not use SVMs? [ https://www.quora.com/What-is-deep-learning-Why-is-this-a-growing-trend-in-machine-learning-Why-not-use-SVMs/answer/Rohit-Malshe ]\n * Rohit Malshe's answer to Are \u2018curated paths to a Data Science career\u2019 on Coursera worth the money and time? [ https://www.quora.com/Are-\u2018curated-paths-to-a-Data-Science-career\u2019-on-Coursera-worth-the-money-and-time/answer/Rohit-Malshe ]\n\nIn all the seriousness, if you want a elaborate documentation on all this, I would suggest, go ahead and read this McKinsey report to get a full understanding. I only extracted a few sections out of it conveniently because I only wanted to add on the top of someone else\u2019s knowledge, and put together these concepts like a story so as to inspire the people to think about this subject and begin their own journeys.\n\nBig data: The next frontier for innovation, competition, and productivity [ http://www.mckinsey.com/business-functions/digital-mckinsey/our-insights/big-data-the-next-frontier-for-innovation ]\n\nI will answer a few questions step by step, and wherever possible, I will give a few pictures, or plots to show you how things look like.\n\nMcKinsey consultants! You are amazing, so if you read things written in this answer that were typed by you at some point in time, I give full credit to you.\n\n * What do we mean by \"big data\"?\n * \u201cBig data\u201d refers to datasets whose size is beyond the ability of typical database software tools to capture, store, manage, and analyze. This definition is intentionally subjective and incorporates a moving definition of how big a dataset needs to be in order to be considered big data\u2014i.e., we need not define big data in terms of being larger than a certain number of terabytes (thousands of gigabytes). We assume that, as technology advances over time, the size of datasets that qualify as big data will also increase. Also note that the definition can vary by sector, depending on what kinds of software tools are commonly available and what sizes of datasets are common in a particular industry. With those caveats, big data in many sectors today will range from a few dozen terabytes to multiple petabytes (thousands of terabytes).\n * What is a typical size of data I may have to deal with? Sometimes GBs, sometimes just a few MBs, sometimes up to as high as 1TB. Sometimes the complexity is nothing. The data may be representing the same thing. Sometimes the complexity can be very high. I might have a giant file full of a lot of data and logs which can be structured or unstructured.\n * Think for example about Macy\u2019s. There are thousands of stores, selling thousands of items per day to millions of customers. If Macy\u2019s wants to derive a conclusion ~ should they rather diversify in shoes, or should they rather diversify in women\u2019s purses? How would they make this decision?\n * Well then, a natural question is: How do we measure the value of big data?\n * Measuring data Measuring volumes of data provokes a number of methodological questions. First, how can we distinguish data from information and from insight? Common definitions describe data as being raw indicators, information as the meaningful interpretation of those signals, and insight as an actionable piece of knowledge.\n * For example - In this chart, someone has plotted cost per student for various regions. It makes a few of them stand out.\nLet us now talk about analysis: This is big part of being a data scientist.\n\n * TECHNIQUES FOR ANALYZING BIG DATA\n * There are many techniques that draw on disciplines such as statistics and computer science (particularly machine learning) that can be used to analyze datasets. This list is by no means exhaustive. Indeed, researchers continue to develop new techniques and improve on existing ones, particularly in response to the need to analyze new combinations of data.\n * Also, note that not all of these techniques strictly require the use of big data\u2014some of them can be applied effectively to smaller datasets (e.g., A/B testing, regression analysis). However, all of the techniques listed here can be applied to big data and, in general, larger and more diverse datasets can be used to generate more numerous and insightful results than smaller, less diverse ones.\n * A/B testing. A technique in which a control group is compared with a variety of test groups in order to determine what treatments (i.e., changes) will improve a given objective variable, e.g., marketing response rate. This technique is also known as split testing or bucket testing. An example application is determining what copy text, layouts, images, or colors will improve conversion rates on an e-commerce Web site. Big data enables huge numbers of tests to be executed and analyzed, ensuring that groups are of sufficient size to detect meaningful (i.e., statistically significant) differences between the control 28 and treatment groups (see statistics). When more than one variable is simultaneously manipulated in the treatment, the multivariate generalization of this technique, which applies statistical modeling, is often called \u201cA/B/N\u201d testing. What would an example look like?\n * Imagine that Coke signs up with Facebook to work on marketing and sales. Facebook would put advertisements according to the customers. It can create versions of advertisements. Not all versions will suit to every geography. Some will suit to USA, some will suit to India. Some can suit to Indians living in USA. What Facebook can do is to choose a subset of people from a massive pool, and pass advertisements to them in their feed according to whether those people love food or not. For each advertisement, Facebook will collect the responses and accordingly determine which advertisement does better, and on a larger pool of people it will use a better one. Does data science let someone determine better what the answer should be? Absolutely!\n * Association rule learning. A set of techniques for discovering interesting relationships, i.e., \u201cassociation rules,\u201d among variables in large databases. These techniques consist of a variety of algorithms to generate and test possible rules. One application is market basket analysis, in which a retailer can determine which products are frequently bought together and use this information for marketing (a commonly cited example is the discovery that many supermarket shoppers who buy diapers also tend to buy beer).\n * Classification. A set of techniques to identify the categories in which new data points belong, based on a training set containing data points that have already been categorized. One application is the prediction of segment-specific customer behavior (e.g., buying decisions, churn rate, consumption rate) where there is a clear hypothesis or objective outcome. These techniques are often described as supervised learning because of the existence of a training set; they stand in contrast to cluster analysis, a type of unsupervised learning.\n * Cluster analysis. A statistical method for classifying objects that splits a diverse group into smaller groups of similar objects, whose characteristics of similarity are not known in advance. An example of cluster analysis is segmenting consumers into self-similar groups for targeted marketing. This is a type of unsupervised learning because training data are not used. This technique is in contrast to classification, a type of supervised learning.\n * Crowdsourcing. A technique for collecting data submitted by a large group of people or community (i.e., the \u201ccrowd\u201d) through an open call, usually through networked media such as the Web.28 This is a type of mass collaboration and an instance of using Web 2.0.29 Data fusion and data integration.\n * A set of techniques that integrate and analyze data from multiple sources in order to develop insights in ways that are more efficient and potentially more accurate than if they were developed by analyzing a single source of data.\n * Data mining. A set of techniques to extract patterns from large datasets by combining methods from statistics and machine learning with database management. These techniques include association rule learning, cluster analysis, classification, and regression. Applications include mining customer data to determine segments most likely to respond to an offer, mining human resources data to identify characteristics of most successful employees, or market basket analysis to model the purchase behavior of customers.\n * Ensemble learning. Using multiple predictive models (each developed using statistics and/or machine learning) to obtain better predictive performance than could be obtained from any of the constituent models. This is a type of supervised learning.\n * Genetic algorithms. A technique used for optimization that is inspired by the process of natural evolution or \u201csurvival of the fittest.\u201d In this technique, potential solutions are encoded as \u201cchromosomes\u201d that can combine and mutate. These individual chromosomes are selected for survival within a modeled \u201cenvironment\u201d that determines the fitness or performance of each individual in the population. Often described as a type of \u201cevolutionary algorithm,\u201d these algorithms are well-suited for solving nonlinear problems. Examples of applications include improving job scheduling in manufacturing and optimizing the performance of an investment portfolio.\n * Machine learning. A subspecialty of computer science (within a field historically called \u201cartificial intelligence\u201d) concerned with the design and development of algorithms that allow computers to evolve behaviors based on empirical data. A major focus of machine learning research is to automatically learn to recognize complex patterns and make intelligent decisions based on data. Natural language processing is an example of machine learning.\n * Natural language processing (NLP). A set of techniques from a sub-specialty of computer science (within a field historically called \u201cartificial intelligence\u201d) and linguistics that uses computer algorithms to analyze human (natural) language. Many NLP techniques are types of machine learning. One application of NLP is using sentiment analysis on social media to determine how prospective customers are reacting to a branding campaign. Data from social media, analyzed by natural language processing, can be combined with real-time sales data, in order to determine what effect a marketing campaign is having on customer sentiment and purchasing behavior.\n * Neural networks. Computational models, inspired by the structure and workings of biological neural networks (i.e., the cells and connections within a brain), that find patterns in data. Neural networks are well-suited for finding nonlinear patterns. They can be used for pattern recognition and optimization. Some neural network applications involve supervised learning and others involve unsupervised learning. Examples of applications include identifying high-value customers that are at risk of leaving a particular company and identifying fraudulent insurance claims.\n * Network analysis. A set of techniques used to characterize relationships among discrete nodes in a graph or a network. In social network analysis, connections between individuals in a community or organization are analyzed, e.g., how information travels, or who has the most influence over whom. Examples of applications include identifying key opinion leaders to target for marketing, and identifying bottlenecks in enterprise information flows.\n * Optimization. A portfolio of numerical techniques used to redesign complex systems and processes to improve their performance according to one or more objective measures (e.g., cost, speed, or reliability). Examples of applications include improving operational processes such as scheduling, routing, and floor layout, and making strategic decisions such as product range strategy, linked investment analysis, and R&D portfolio strategy. Genetic algorithms are an example of an optimization technique. Same way, mixed integer programming is another way.\n * Pattern recognition. A set of machine learning techniques that assign some sort of output value (or label) to a given input value (or instance) according to a specific algorithm. Classification techniques are an example.\n * Predictive modeling. A set of techniques in which a mathematical model is created or chosen to best predict the probability of an outcome. An example of an application in customer relationship management is the use of predictive models to estimate the likelihood that a customer will \u201cchurn\u201d (i.e., change providers) or the likelihood that a customer can be cross-sold another product. Regression is one example of the many predictive modeling techniques.\n * Regression. A set of statistical techniques to determine how the value of the dependent variable changes when one or more independent variables is modified. Often used for forecasting or prediction. Examples of applications include forecasting sales volumes based on various market and economic variables or determining what measurable manufacturing parameters most influence customer satisfaction. Used for data mining.\n * Sentiment analysis. Application of natural language processing and other analytic techniques to identify and extract subjective information from source text material. Key aspects of these analyses include identifying the feature, aspect, or product about which a sentiment is being expressed, and determining the type, \u201cpolarity\u201d (i.e., positive, negative, or neutral) and the degree and strength of the sentiment. Examples of applications include companies applying sentiment analysis to analyze social media (e.g., blogs, microblogs, and social networks) to determine how different customer segments and stakeholders are reacting to their products and actions.\n * Signal processing. A set of techniques from electrical engineering and applied mathematics originally developed to analyze discrete and continuous signals, i.e., representations of analog physical quantities (even if represented digitally) such as radio signals, sounds, and images. This category includes techniques from signal detection theory, which quantifies the ability to discern between signal and noise. Sample applications include modeling for time series analysis or implementing data fusion to determine a more precise reading by combining data from a set of less precise data sources (i.e., extracting the signal from the noise). Signal processing techniques can be used to implement some types of data fusion. One example of an application is sensor data from the Internet of Things being combined to develop an integrated perspective on the performance of a complex distributed system such as an oil refinery.\n * Spatial analysis. A set of techniques, some applied from statistics, which analyze the topological, geometric, or geographic properties encoded in a data set. Often the data for spatial analysis come from geographic information systems (GIS) that capture data including location information, e.g., addresses or latitude/longitude coordinates. Examples of applications include the incorporation of spatial data into spatial regressions (e.g., how is consumer willingness to purchase a product correlated with location?) or simulations (e.g., how would a manufacturing supply chain network perform with sites in different locations?).\n * Statistics. The science of the collection, organization, and interpretation of data, including the design of surveys and experiments. Statistical techniques are often used to make judgments about what relationships between variables could have occurred by chance (the \u201cnull hypothesis\u201d), and what relationships between variables likely result from some kind of underlying causal relationship (i.e., that are \u201cstatistically significant\u201d). Statistical techniques are also used to reduce the likelihood of Type I errors (\u201cfalse positives\u201d) and Type II errors (\u201cfalse negatives\u201d). An example of an application is A/B testing to determine what types of marketing material will most increase revenue.\n * Supervised learning. The set of machine learning techniques that infer a function or relationship from a set of training data. Examples include classification and support vector machines.30 This is different from unsupervised learning.\n * Simulation. Modeling the behavior of complex systems, often used for forecasting, predicting and scenario planning. Monte Carlo simulations, for example, are a class of algorithms that rely on repeated random sampling, i.e., running thousands of simulations, each based on different assumptions. The result is a histogram that gives a probability distribution of outcomes. One application is assessing the likelihood of meeting financial targets given uncertainties about the success of various initiatives.\n * Time series analysis. Set of techniques from both statistics and signal processing for analyzing sequences of data points, representing values at successive times, to extract meaningful characteristics from the data. Examples of time series analysis include the hourly value of a stock market index or the number of patients diagnosed with a given condition every day.\n * Time series forecasting. Time series forecasting is the use of a model to predict future values of a time series based on known past values of the same or other series. Some of these techniques, e.g., structural modeling, decompose a series into trend, seasonal, and residual components, which can be useful for identifying cyclical patterns in the data. Examples of applications include forecasting sales figures, or predicting the number of people who will be diagnosed with an infectious disease.\n * Unsupervised learning. A set of machine learning techniques that finds hidden structure in unlabeled data. Cluster analysis is an example of unsupervised learning (in contrast to supervised learning).\n * Visualization. Techniques used for creating images, diagrams, or animations to communicate, understand, and improve the results of big data analyses. This expands into creating dashboards, on web or desktop platforms.\n\nHope this somewhat elaborate write up gives you some inspiration to hold on to. Stay blessed and stay inspired!",
                "Lots of good answers already - however the question is such that I think perhaps a business rather than technical description might be warranted.\n\nFirst things first, doing stuff with data, whatever you want to call it is going to require some investment - fortunately the entry price has come right down and you can do pretty much all of this at home with a reasonably priced machine and online access to a host of free or purchased resources. Commercial organizations have realized that there is huge value hiding in the data and are employing the techniques you ask about to realize that value. Ultimately what all of this work produces is insights, things that you may not have known otherwise. Insights are the items of information that cause a change in behavior.\n\nLet's begin with a real world example, looking at a farm that is growing strawberries (here's a simple backgrounder The Secret Life Of California's World-Class Strawberries [ http://www.npr.org/sections/thesalt/2012/05/17/152522900/the-secret-life-of-californias-world-class-strawberries ], this High-Tech Greenhouse Yields Winter Strawberries [ http://www.laboratoryequipment.com/news/2013/12/high-tech-greenhouse-yields-winter-strawberries ] , and this Growing Strawberry Plants Commercially [ http://strawberryplants.org/2010/09/growing-strawberry-plants-commercially/ ])\n\nWhat would a farmer need to consider if they are growing strawberries? The farmer will be selecting the types of plants, fertilizers, pesticides. Also looking at machinery, transportation, storage and labor. Weather, water supply and pestlience are also likely concerns. Ultimately the farmer is also investigating the market price so supply and demand and timing of the harvest (which will determine the dates to prepare the soil, to plant, to thin out the crop, to nurture and to harvest) are also concerns.\n\nSo the objective of all the data work is to create insights that will help the farmer make a set of decisions that will optimize their commercial growing operation.\n\nLet's think about the data available to the farmer, here's a simplified breakdown:\n\n1. Historic weather patterns\n\n2. Plant breeding data and productivity for each strain\n\n3. Fertilizer specifications\n\n4. Pesticide specifications\n\n5. Soil productivity data\n\n6. Pest cycle data\n\n7. Machinery cost, reliability, fault and cost data\n\n8. Water supply data\n\n9. Historic supply and demand data\n\n10. Market spot price and futures data\n\nNow to explain the definitions in context (with some made-up insights, so if you're a strawberry farmer, this might not be the best set of examples):\n\nBig Data: Using all of the data available to provide new insights to a problem. Traditionally the farmer may have made their decisions based on only a few of the available data points, for example selecting the breeds of strawberries that had the highest yield for their soil and water table. The Big Data approach may show that the market price slightly earlier in the season is a lot higher and local weather patterns are such that a new breed variation of strawberry would do well. So the insight would be switching to a new breed would allow the farmer to take advantage of a higher prices earlier in the season, and the cost of labor, storage and transportation at that time would be slightly lower. There's another thing you might hear in the Big Data marketing hype: Volume, Velocity, Variety, Veracity - so there is a huge amount of data here, a lot of data is being generated each minute (so weather patterns, stock prices and machine sensors), and the data is liable to change at any time (e.g. a new source of social media data that is a great predictor for consumer demand),\n\nData Analysis: Analysis is really a heuristic activity, where scanning through all the data the analyst gains some insight. Looking at a single data set - say the one on machine reliability, I might be able to say that certain machines are expensive to purchase but have fewer general operational faults leading to less downtime and lower maintenance costs. There are other cheaper machines that are more costly in the long run. The farmer might not have enough working capital to afford the expensive machine and they would have to decide whether to purchase the cheaper machine and incur the additional maintenance costs and risk the downtime or to borrow money with the interest payment, to afford the expensive machine.\n\nData Analytics: Analytics is about applying a mechanical or algorithmic process to derive the insights for example running through various data sets looking for meaningful correlations between them. Looking at the weather data and pest data we see that there is a high correlation of a certain type of fungus when the humidity level reaches a certain point. The future weather projections for the next few months (during planting season) predict a low humidity level and therefore lowered risk of that fungus. For the farmer this might mean being able to plant a certain type of strawberry, higher yield, higher market price and not needing to purchase a certain fungicide.\n\nData Mining: this term was most widely used in the late 90's and early 00's when a business consolidated all of its data into an Enterprise Data Warehouse. All of that data was brought together to discover previously unknown trends, anomalies and correlations such as the famed 'beer and diapers' correlation (Diapers, Beer, and data science in retail [ http://canworksmart.com/diapers-beer-retail-predictive-analytics/ ]). Going back to the strawberries, assuming that our farmer was a large conglomerate like Cargill, then all of the data above would be sitting ready for analysis in the warehouse so questions such as this could be answered with relative ease: What is the best time to harvest strawberries to get the highest market price? Given certain soil conditions and rainfall patterns at a location, what are the highest yielding strawberry breeds that we should grow?\n\nData Science: a combination of mathematics, statistics, programming, the context of the problem being solved, ingenious ways of capturing data that may not be being captured right now plus the ability to look at things 'differently' (like this Why UPS Trucks Don't Turn Left [ http://priceonomics.com/why-ups-trucks-dont-turn-left/ ] ) and of course the significant and necessary activity of cleansing, preparing and aligning the data. So in the strawberry industry we're going to be building some models that tell us when the optimal time is to sell, which gives us the time to harvest which gives us a combination of breeds to plant at various times to maximize overall yield. We might be short of consumer demand data - so maybe we figure out that when strawberry recipes are published online or on television, then demand goes up - and Tweets and Instagram or Facebook likes provide an indicator of demand. Then we need to align demand data up with market price to give us the final insights and maybe to create a way to drive up demand by promoting certain social media activity.\n\nMachine Learning: this is one of the tools used by data scientist, where a model is created that mathematically describes a certain process and its outcomes, then the model provides recommendations and monitors the results once those recommendations are implemented and uses the results to improve the model. When Google provides a set of results for the search term \"strawberry\" people might click on the first 3 entries and ignore the 4th one - over time, that 4th entry will not appear as high in the results because the machine is learning what users are responding to. Applied to the farm, when the system creates recommendations for which breeds of strawberry to plant, and collects the results on the yeilds for each berry under various soil and weather conditions, machine learning will allow it to build a model that can make a better set of recommendations for the next growing season.\n\nI am adding this next one because there seems to be some popular misconceptions as to what this means. My belief is that 'predictive' is much overused and hyped.\n\nPredictive Analytics: Creating a quantitative model that allows an outcome to be predicted based on as much historical information as can be gathered. In this input data, there will be multiple variables to consider, some of which may be significant and others less significant in determining the outcome. The predictive model determines what signals in the data can be used to make an accurate prediction. The models become useful if there are certain variables than can be changed that will increase chances of a desired outcome. So what might be useful for our strawberry farmer to want to predict? Let's go back to the commercial strawberry grower who is selling product to grocery retailers and food manufacturers - the supply deals are in tens and hundreds of thousands of dollars and there is a large salesforce. How can they predict whether a deal is likely to close or not? To begin with, they could look at the history of that company and the quantities and frequencies of produce purchased over time, the most recent purchases being stronger indicators. They could then look at the salesperson's history of selling that product to those types of companies. Those are the obvious indicators. Less obvious ones would be the what competing growers are also bidding for the contract, perhaps certain competitors always win because they always undercut. How many visits the rep has paid to the prospective client over the year, how many emails and phone calls. How many product complaints has the prospective client made regarding product quality? Have all our deliveries been the correct quantity, delivered on time? All of these variables may contribute to the next deal being closed. If there is enough historical data, we can build a model that will predict that a deal will close or not. We can use a sample of the historic data set aside to test if the model works. If we are confident, then we can use it to predict the next deal\n\n[Update June 19, 2017 - just discovered: Farmers Business Network (FBN) [ https://www.farmersbusinessnetwork.com/ ] Farmers Business Network is proudly Farmers First SM. Created by farmers for farmers, FBN is an independent and unbiased farmer-to-farmer network of thousands of American farms. FBN democratizes farm information by making the power of anonymous aggregated analytics available to all FBN members. The FBN Network helps level the playing field for independent farmers with unbiased information, profit enhancing farm analysis, and network buying power.]",
                "Data Analytics, Data Analysis, Data Mining, Data Science, Machine Learning, and Big Data are all related terms that are often used interchangeably, but they have distinct differences.\n\nLet\u2019s check that out:\n\n1. Data Analytics: Data Analytics refers to the practice of analyzing data sets to gain insights and involves extracting meaningful patterns, trends, and correlations from data.\n2. Data Analysis: Data Analysis is a broader term that involves applying statistical techniques and tools to understand the data, identify patterns, and draw conclusions.\n3. Data Mining: Data Mining is a specific technique within data analysis that involves using algorithms and statistical models to uncover hidden patterns and make predictions.\n4. Data Science: Data Science is an interdisciplinary field that combines elements of statistics, mathematics, computer science, and domain expertise to extract knowledge and insights from data.\n5. Machine Learning: Machine Learning is a subset of artificial intelligence that involves training models on historical data and using them to make predictions on new data.\n6. Big Data: Big Data involves technologies and methodologies to store, process, and analyze vast amounts of data.\nSo, how can one study these concepts in the most effective manner?\n\n1. Start with foundational knowledge: It is crucial to build a strong foundation in the fundamentals before delving into more advanced topics. Familiarize yourself with basic statistical concepts, programming languages (such as Python or R), and data manipulation techniques. Online courses, tutorials, and textbooks can be great resources for acquiring this foundational knowledge.\n\nLet\u2019s see the better options:\n\nCoursera\n\n\"Introduction to Data Science in Python\" by the University of Michigan\u201d course is a great choice for beginners who have little or no programming experience. It covers the fundamentals of data manipulation, analysis, and visualization using Python, which is widely used in the data science field.\n\n2. Enroll in specialized courses: Once you have a solid foundation, consider enrolling in specialized master\u2019s degree courses or programs that focus on advanced learning and provide a premium value of learning even online. Platforms like Learnbay, edX, and Udemy offer a wide range of data-related courses, some of which are offered by top universities and industry experts.\n\nExpand your knowledge by taking online DS courses. You can practice by taking domain-specific courses from this platform.\n\n * \n * Online courses such as the Learnbay\u2019s Program in Data Science and AI.\n\nSpecific domain knowledge also helps experts identify relevant sectors such as healthcare, manufacturing, BFSI, oil and gas, energy, supply chain, retail, e-commerce, etc.\n\n3. Participate in collaborating on projects: Take part in projects that allow you to apply the concepts you have learned. This could involve working on personal projects, joining online community forums, or connecting with like-minded individuals who can provide valuable insights, opportunities for collaboration, and exposure to real-world challenges.\n\nJoin programming workshops, coding boot camps, or online tutorials to get hands-on experience on Learnbay. You can practice by working and acquiring theoretical and practical skills online and at experience centers. The institute offers online theory sessions and generally provides a structured curriculum as well as practical learning opportunities for offline sessions at the center. You can access offline centers in cities like Bangalore, Delhi, Pune, and Hyderabad.\n\n4. Job Placement Assistance: These services can help individuals identify relevant job opportunities, refine their resumes, prepare for interviews, and enhance their overall employability in the data science industry.\n\nGreat Learning\n\nOne of the most popular data science courses offered by Great Learning is the \"Data Science and Business Analytics\" program. By completing this course, professionals can enhance their career prospects and open doors to various job opportunities in industries such as finance, healthcare, retail, and more.\n\n * \n * Look for opportunities provided by Learnbay to excel in mock sessions, CV writing classes, one-on-one doubt resolution sessions, etc.\n\nYou can achieve this by always enrolling in online programs like the Data Science and AI program. The program gives you unlimited access to phone interviews, personalized doubt resolution sessions, and more. You can also access these services offline from their centers in Bangalore, Delhi, Pune, and Hyderabad.\n\nThe Institute uplifts national and international alumni success stories. The certificate awarded upon completion of a Program in Data Science and AI by Learnbay has great value both domestically and internationally. Students receive an IBM Project Experience certificate for completing your course.\n\nCONCLUSION\n\nWhile these concepts may overlap to some extent, they each have distinct focuses and applications. Understanding the differences between these terms is crucial for effectively utilizing them in various industries and problem-solving scenarios.\n\nThank you and all the best with your choices.",
                "I am working as a Data Scientist myself therefore it makes me qualified enough to answer your question.\n\nAlso I will make sure to include the tricks in my answer that worked for me.\n\nSo Let's begin, Shall we?\n\nI will be answering this question, keeping in mind that a bunch of readers could be complete newbies into programming.\n\nSo addressing non-computer science students. Firstly, you need to work a lot on your problem-solving skills which is going to help you code effortlessly. You can achieve this by learning Data structures & Algorithms and coding in it. Also, DS & Algo are the building block of computer science so it will definitely help you on your Journey towards excellence in coding.\n\nAfter you are comfortable with problem-solving, you should stick to the below mentioned points:\n\n1. Opt for a good course on Machine learning and study it thoroughly to become well versed with all it\u2019s concepts.\n2. Practice machine learning problems on Kaggle: Your Machine Learning and Data Science Community [ https://www.kaggle.com/ ] which will help you gain confidence and give you enough hands-on skills.\n3. Post your projects on GitHub, LinkedIn and also you can use youtube to showcase your skills\n4. Now it\u2019s Time to market yourself. Make a clean and creative online portfolio and a strong resume based on ML. Start applying to your desired companies and surely circumstances will bend in your favour and soon you will become something you have worked so hard for and that is \u201cData scientist\u201d\n5. you can connect with me on LinkedIn\nPs: I am attaching my photo, in which you can see me working from home, just in case you are interested to know how a data scientist looks?! \ud83d\ude1b\n\n",
                "The list below probably isn't exhaustive, but contains the first things I thought of. So hopefully, they're the most important/fundamental ones!  (Although I'm sure I missed some.)\n\nStatistical Learning Theory:\n\nOverfitting [ https://en.wikipedia.org/wiki/Overfitting ] -- A central concept in machine learning is that of overfitting.  Roughly speaking, overfitting happens when you train a model that captures the idiosyncrasies of your training data.  A model that overfits to training data cannot generalize well to new unseen test examples, which is ultimately what we want most machine learning models to do.\n\nGeneralization error [ https://en.wikipedia.org/wiki/Generalization_error ] -- One way to quantify overfitting is through generalization error.  Roughly speaking, generalization error measure the gap between the error on the training set versus the test set.  Thus, the larger the generalization error, the more the model is overfitting.\n\nBias\u2013variance tradeoff [ https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff ] -- Sometimes, it's OK if the model you train overfits so long as the generalization error is not too large.  For example, if you train a complex model that achieves 0.2 error on the training set and 0.5 error on the test set, that might be desirable to a simple model that achieves 0.5 error on the training set and 0.6 error on the test set.  Even though the simple model overfits less, it was so simple that it still performs worse on the test set compared to a complex model that overfits more.  The bias-variance tradeoff is a way of reasoning about this issue: when does it make sense to use a more complex model even though it overfits more?\n\nEmpirical risk minimization [ https://en.wikipedia.org/wiki/Empirical_risk_minimization ] -- When most people think of machine learning, they're probably thinking of empirical risk minimization.  That is, they want a model that achieves low error on some training set.  However, it is important to keep in mind what the assumptions are of empirical risk minimization.  Most notably, that the training set is sampled independently from the test distribution you really care about.  If this assumption is violated, you can get machine learning models that don't behave in the way you want (cf. Algorithms and Bias: Q. and A. With Cynthia Dwork [ http://www.nytimes.com/2015/08/11/upshot/algorithms-and-bias-q-and-a-with-cynthia-dwork.html?_r=0 ]).\n\nCross-validation (statistics) [ https://en.wikipedia.org/wiki/Cross-validation_(statistics) ] -- Typically, one cannot test on data that one trained on, one must split the existing data into training and test sets. However, this is statistically wasteful and also increases the variability since you aren't testing on every data point at your disposal.  Cross validation is a way of getting around that by rotating what's in the training vs test sets. \n\nConfidence interval [ https://en.wikipedia.org/wiki/Confidence_interval ] -- The most direct quantitative way to compare two models is by looking at their respective test errors (e.g., via cross validation).  However, how do we know if two numbers actually reflect meaningful differences between the two models or are just due to some spurious effects caused by a finite sample size?  Confidence intervals are the most common way to deal with this issue.\n\nStatistical hypothesis testing [ https://en.wikipedia.org/wiki/Statistical_hypothesis_testing- ] -- A related concept to confidence intervals is statistical hypothesis testing.  The most common thing to use this for is answer whether two models have statistically distinguishable accuracies.  The way statistical hypothesis testing is typically implemented involves using confidence intervals and setting the size of the confidence intervals at an appropriate width w.r.t. the desired statistical significance level.\n\nBootstrapping (statistics) [ https://en.wikipedia.org/wiki/Bootstrapping_(statistics) ] -- Another way of evaluating the variability of the model is via bootstrapping, which effectively samples from the training set with replacement to generate new training sets that are statistically similar to the original training set.\n\n\nStatistical Modeling:\n\nMetrics | Kaggle [ https://www.kaggle.com/wiki/Metrics ] -- It's important to understand what your metric of choice is for whatever modeling problem you're solving.  For some tasks, you only care that your model can make good predictions at the top (e.g., ranking in web search), so a metric like precision@10 is appropriate there.\n\nRegularization (mathematics) [ https://en.wikipedia.org/wiki/Regularization_(mathematics)#Regularization_in_statistics_and_machine_learning ] --  Regularization serves two purposes.  First, it is commonly used to control for overfitting so that the learned model is not too complex.  Second, different choices of regularization reflect different assumptions about what \"simple\" means.  For instance, using L1 regularization encourages sparsity in the trained model, and interprets simple as having few non-zero parameters.  On the other hand, using L2 regularization encourages the norm of the learned model to be low, and interprets simple as having a small magnitude.\n\nMachine learning Types and Tasks [ https://en.wikipedia.org/wiki/Machine_learning#Types_of_problems_and_tasks ] --  People often think of supervised learning when they think machine learning (and in fact most of the topics listed here are described through the lens of supervised learning).  But supervised learning is not the only learning setup.  Others include unsupervised learning, semi-supervised learning, transductive learning, etc.  It's important to understand what kind of learning problem you're dealing with.  If you would rather deal with the supervised learning problem, that often means throwing away data for which you do not have labels for.  Sometimes that's good, and sometimes that's not so good.\n\nCorrelation vs. causation [ https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation ] -- It's important to keep in mind when inspecting a learned model that many things learned by the model are purely correlation and should not be interpreted causally. \n\n\nOptimization:\n\nStochastic gradient descent [ https://en.wikipedia.org/wiki/Stochastic_gradient_descent ] -- Most machine learning models are trained via some form of stochastic gradient descent.  It's generally useful to understand when different methods work well, so you can train your models more efficiently.\n\nNesterov\u2019s Accelerated Gradient Descent [ https://blogs.princeton.edu/imabandit/2013/04/01/acceleratedgradientdescent/ ] -- One important concept in gradient descent is momentum, of which Nesterov's method is arguably the most beautiful instance.  Momentum is typically extremely useful for speeding up training.\n \nConvex analysis [ https://en.wikipedia.org/wiki/Convex_analysis ] --  It's important to understand when your learning problem is convex versus non-convex.  Convex learning problems always converge to the same optimal model, so you don't have to be too careful about how you train (apart for speed considerations).  Non-convex learning problems can get stuck in local optima, and so the model you get back can vary greatly.  As such, it's often important to be careful about how you initialize the non-convex learning problem.\n\n\nLinear Algebra:\n\nNorm (mathematics) [ https://en.wikipedia.org/wiki/Norm_(mathematics) ] -- Norms are used a lot in machine learning.  For instance, many regularization formulations are written as norms.  Understanding the behavior of different norms will help you in deciding which kind of regularization you want to impose.\n\nMatrix_(mathematics) [ https://en.wikipedia.org/wiki/Matrix_(mathematics) ] -- A lot of times, data and models are expressed using matrices.  Sometimes, you can save a lot of computation time by being clever about how you order the matrix operations.  Other times, you can figure out how to transform your data into the format that some learning toolkit uses by using matrix transforms.  \n\nThe Statistical Whitening Transform [ https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/ ] --  One particularly useful approach for standardizing your data is whitening.  It's good to understand what assumptions are built into whitening, so that you'll have a good sense of when whitening will and won't work.\n\n\nOutlook:\nThese days, there are a lot of tools being developed that can automate away a lot of the issues described above, and will thus make machine learning more intuitive & easier to use for more people.  For example, many machine learning packages already do cross validation automatically.  However, those tools are far from perfect, and so having a solid grasp of the theoretical fundamentals will be very beneficial in the long run, because it'll allow you to more intelligently use and compose the existing tools to achieve whatever data modeling task you're trying to solve.",
                "Here are the 100 most popular Machine Learning talks at http://VideoLectures.Net\n(October 22, 2012)\n\n(NB: to download videos from VideoLectures you can use StreamTransport [ http://softwarerecs.stackexchange.com/q/7186/903 ] on Windows)\n \n1. 26971 views, 1:00:45,  Gaussian Process Basics [ http://videolectures.net/gpip06_mackay_gpb/ ], David MacKay, 8 comments\n2. 7799 views, 3:08:32, Introduction to Machine Learning [ http://videolectures.net/bootcamp2010_murray_iml/ ], Iain Murray\n3. 16092 views, 1:28:05, Introduction to Support Vector Machines [ http://videolectures.net/epsrcws08_campbell_isvm/ ], Colin Campbell,22 comments\n4. 5755 views, 2:53:54, Probability and Mathematical Needs [ http://videolectures.net/bootcamp2010_anthoine_pmn/ ], Sandrine Anthoine, 2 comments\n5. 7960 views, 3:06:47, A tutorial on Deep Learning [ http://videolectures.net/jul09_hinton_deeplearn/ ], Geoffrey E. Hinto\n6. 3858 views, 2:45:25, Introduction to Machine Learning [ http://videolectures.net/aibootcamp2011_quinn_iml/ ], John Quinn, 1 comment\n7. 13758 views, 5:40:10, Statistical Learning Theory [ http://videolectures.net/mlss04_taylor_slt/ ], John Shawe-Taylor [ http://en.wikipedia.org/wiki/John_Shawe-Taylor ], 3 comments\n8. 12226 views, 1:01:20, Semisupervised Learning Approaches [ http://videolectures.net/mlas06_mitchell_sla/ ], Tom Mitchell,8 comments\n9. 1596 views, 1:04:23, Why Bayesian nonparametrics? [ http://videolectures.net/nipsworkshops2011_ghahramani_nonparametrics/ ], Zoubin Ghahramani [ http://en.wikipedia.org/wiki/Zoubin_Ghahramani ],  1 comment\n10. 11390 views, 3:52:22, Markov Chain Monte Carlo Methods, [ http://videolectures.net/mlss04_robert_mcmcm/ ] Christian P. Robert,5 comments\n11. 3153 views, 2:15:00, Data mining and Machine learning algorithms [ http://videolectures.net/aibootcamp2011_balcazar_dmml/ ], Jos\u00e9 L. Balc\u00e1zar, 1 comment\n12. 10322 views, 5:15:43, Graphical models [ http://videolectures.net/mlss07_ghahramani_grafm/ ], Zoubin Ghahramani, 23 comments\n13. 11071 views, 1:05:40, Dirichlet Processes, Chinese Restaurant Processes, and all that, [ http://videolectures.net/icml05_jordan_dpcrp/ ]Michael I. Jordan [ http://en.wikipedia.org/wiki/Michael_I._Jordan ], 7 comments\n\n14. 10550 views, 1:06:55, Generative Models for Visual Objects and Object Recognition via Bayesian Inference [ http://videolectures.net/mlas06_li_gmvoo/ ], Fei-Fei Li, 11 comments\n15. 9312 views, 03:21, K-nearest neighbor classification [ http://videolectures.net/aaai07_bosch_knnc/ ], Antal van den Bosch,7 comments\n16. 4800 views, 2:07:31, Patterns in Vector Spaces [ http://videolectures.net/aop09_ricci_pivs/ ], Elisa Ricci, 1 comment\n17. 736 views, 16:55, Twitter Sentiment  in Financial Domain [ http://videolectures.net/firstworkshop2012_grcar_twitter/ ], Miha Gr\u010dar, 1 comment\n18. 6789 views, 2:06:40, Introduction to kernel methods [ http://videolectures.net/mlss07_scholkopf_intkmet/ ], Bernhard Sch\u00f6lkopf,  5 comments\n19. 6849 views, 2:54:37, Some Mathematical Tools for Machine Learning, [ http://videolectures.net/mlss03_burges_smtml/ ] Chris Burges, 6 comments\n20. 6792 views, 1:24:46, Bayesian Learning [ http://videolectures.net/mlss05us_ghahramani_bl/ ], Zoubin Ghahramani,  9 comments\n21. 6689 views, 4:33:48, Graphical Models and Variational Methods [ http://videolectures.net/mlss04_bishop_gmvm/ ], Christopher Bishop, 11 comments\n22. 844 views, 17:05, High-Dimensional Graphical Model Selection [ http://videolectures.net/nips2011_anandkumar_conditions/ ], Animashree Anandkumar\n23. 5862 views, 57:16, Introduction to feature selection [ http://videolectures.net/bootcamp07_guyon_ifs/ ], Isabelle Guyon,  1 comment\n24. 5541 views, 2:14:21, Introduction to kernel methods [ http://videolectures.net/mlss07_smola_intkmet/ ], Alexander J. Smola,  8 comments\n25. 2304 views, 3:22:46, Introduction to Kernel Methods [ http://videolectures.net/bootcamp2010_ralaivola_ikm/ ], Liva Ralaivola,  1 comment \n26. 723 views, 16:26, Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries [ http://videolectures.net/nips2011_xiang_dictionaries/ ], Zhen James Xiang\n27. 1628 views, 23:12, Gradient Boosted Decision Trees on Hadoop [ http://videolectures.net/nipsworkshops2010_ye_gbd/ ], Jerry Ye\n28. 5169 views, 4:16:53, Learning with Kernels [ http://videolectures.net/mlss03_scholkopf_lk/ ],4 comments\n29. 2038 views, 03:18, Scikitlearn [ http://videolectures.net/icml2010_varaquaux_scik/ ], Gael Varoquaux\n30. 4965 views, 32:36, The Dynamics of AdaBoost, [ http://videolectures.net/mlss05us_rudin_da/ ] Cynthia Rudin,  3 comments\n31. 4433 views, 2:16:17, Sequential Monte Carlo methods [ http://videolectures.net/mlss07_doucet_smcm/ ], Arnaud Doucet, 9 comments\n32. 4859 views, 1:37:46, Online Learning and Game Theory [ http://videolectures.net/mlss05us_kalai_olgt/ ], Adam Kalai,  3 comments\n33. 4237 views, 20:36, Learning to align: a statistical approach, [ http://videolectures.net/ida07_ricci_lta/ ] Elisa Ricci, 1 comment \n34. 2645 views, 21:49, Online Dictionary Learning for Sparse Coding [ http://videolectures.net/icml09_mairal_odlsc/ ], Julien Mairal, 1 comment\n\n35. 4727 views, 3:13:52, Bayesian Inference: Principles and Practice [ http://videolectures.net/mlss03_tipping_pp/ ], Mike Tipping, 6 comments\n36. 1419 views, 2:49:30, Online Learning [ http://videolectures.net/mlss2010au_bartlett_onlinelearning/ ], Peter L. Bartlett\n37. 2973 views, 21:01, Training a Binary Classifier with the Quantum Adiabatic Algorithm [ http://videolectures.net/opt08_neven_tabcwt/ ], Hartmut Neven, 1 comment \n38. 3973 views, 08:55, Machine Learning for Stock Selection [ http://videolectures.net/kdd07_ling_mlfss/ ], Charles X. Ling,3 comments\n39. 3900 views, 2:56:35, Machine learning and finance [ http://videolectures.net/mlss07_gyorfi_mlaf/ ], L\u00e1szl\u00f3 Gy\u00f6rfi, 3 comments\n40. 3517 views, 2:10:19, Learning with Gaussian Processes [ http://videolectures.net/epsrcws08_rasmussen_lgp/ ], Carl Edward Rasmussen,7 comments\n41. 222 views, 29:03, Generating Possible Interpretations for Statistics from Linked Open Data [ http://videolectures.net/eswc2012_paulheim_linked_data/ ],Heiko Paulheim\n42. 4089 views, 2:32:26, Graph Matching Algorithms [ http://videolectures.net/mlss05au_caelli_gma/ ], Terry Caelli,  6 comments\n43. 3948 views, 3:39:05, Clustering \u2013 An overview [ http://videolectures.net/mlss06tw_meila_co/ ], Marina Meila,  1 comment\n44. 3903 views, 2:11:59, An Introduction to Pattern Classification [ http://videolectures.net/mlss03_tov_ipc/ ], Elad Yom Tov,1 comment\n45. 3896 views, 5:18:05, Statistical Learning Theory [ http://videolectures.net/mlss03_bousquet_slt/ ], Olivier Bousquet, 3 comments \n46. 1541 views, 38:10, Learning with similarity functions [ http://videolectures.net/icml2010_balcan_lwsf/ ], Maria Balcan\n47. 51 views, 1:00:30, A Flexible Model for Count Data: The COM-Poisson Distribution [ http://videolectures.net/solomon_shmueli_count_data/ ], Galit Shmu\u00e9l\n48. 331 views, 41:53, Automatic Discovery of Patterns in News Content [ http://videolectures.net/workshops2012_cristianini_news_content/ ], Nello Cristianini,2 comments\n49. 1132 views, 2:31:35, Gaussian Processes [ http://videolectures.net/mlss2010au_bonilla_gausianprocesses/ ], Edwin V. Bonilla\n50. 2256 views, 1:08:39, Lecture 1 \u2013 The Motivation & Applications of Machine Learning [ http://videolectures.net/stanfordcs229f08_ng_lec01/ ], Andrew Ng\n51. 666 views, 21:47, On the Usefulness of Similarity based Projection Spaces for Transfer Learning [ http://videolectures.net/simbad2011_morvant_transfer/ ], Emilie Morvant\n52. 1112 views, 36:35, Robust PCA and Collaborative Filtering: Rejecting Outliers, Identifying Manipulators [ http://videolectures.net/nipsworkshops2010_caramanis_rcf/ ], Constantine Caramanis\n53. 3294 views, 2:01:49, The EM algorithm and Mixtures of Gaussians [ http://videolectures.net/bootcamp07_quinonero_emal/ ], Joaquin Qui\u00f1onero Candela, 4 comments\n54. 3444 views, 5:35:17, Independent Component Analysis [ http://videolectures.net/mlss03_cardoso_ica/ ], Jean-Fran\u00e7ois Cardoso, 2 comments\n55. 1918 views, 19:47, Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations [ http://videolectures.net/icml09_lee_cdb/ ], Honglak Lee\n56. 790 views, 1:00:20, Classification and Clustering in Large Complex Networks [ http://videolectures.net/solomon_eliassi_rad_classification/ ], Ina Eliasi-Rad\n57. 986 views, 2:44:35, Restricted Boltzmann Machines and Deep Belief Nets [ http://videolectures.net/mlss2010au_frean_deepbeliefnets/ ], Marcus Frean\n58. 23 views, 17:29, Improved Initialisation and Gaussian Mixture Pairwise Terms for Dense Random Fields with Mean-field Inference [ http://videolectures.net/bmvc2012_vineet_mean_field/ ], Vibhav Vineet\n59. 1915 views, 1:22:16, Lecture 11 \u2013 Bayesian Statistics and Regularization [ http://videolectures.net/stanfordcs229f08_ng_lec11/ ], Andrew Ng\n60. 3129 views, 4:31:39, Kernel Methods [ http://videolectures.net/mlss06tw_smola_km/ ], Alexander J. Smola 2 comments\n61. 2577 views, 1:21:29, Graphical models [ http://videolectures.net/epsrcws08_ghahramani_gm/ ], Zoubin Ghahramani\n62. 2160 views, 1:00:37, Should all Machine Learning be Bayesian? Should all Bayesian models be non-parametric? [ http://videolectures.net/bark08_ghahramani_samlbb/ ], Zoubin Ghahramani,  2 comments\n\n63. 3018 views, 4:35:51, Graphical Models, Variational Methods, and Message-Passing [ http://videolectures.net/mlss06tw_wainwright_gmvmm/ ], Martin J. Wainwright, 6 comments\n64. 3017 views, 3:43:43, Introduction to Kernel Methods [ http://videolectures.net/mlss04_scholkopf_ikm/ ], Bernhard Sch\u00f6lkopf, 1 comment\n65. 1257 views, 1:24:39, Reinforcement learning: Tutorial + Rethinking State, Action & Reward [ http://videolectures.net/mlss2010_singh_rlt/ ], Satinder Singh\n66. 1044 views, 18:34, On the stability and interpretability of prognosis signatures in breast cancer [ http://videolectures.net/mlsb2010_haury_ots/ ], Anne-Claire Haury,1 comment \n67. 2827 views, 00:58, Artificial intelligence: An instance of Aibo ingenuity [ http://videolectures.net/aaai07_littman_ai/ ], Michael Littman,2 comments\n68. 163 views, 22:35, Exploiting Information Extraction, Reasoning and Machine Learning for Relation Prediction [ http://videolectures.net/eswc2012_jiang_relation_prediction/ ], Xueyan Jiang,2 comments\n69. 1704 views, 2:42:22, Theory and Applications of Boosting [ http://videolectures.net/mlss09us_schapire_tab/ ], Robert Schapire,1 comment\n70. 387 views, 18:48, High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity [ http://videolectures.net/nips2011_loh_nonconvexity/ ], Po-Ling Loh\n71. 1912 views, 38:30, Machine learning and kernel methods for computer vision [ http://videolectures.net/etvc08_bach_mlakm/ ], Francis R. Bach\n72. 2755 views, 32:18, Neighbourhood Components Analysis [ http://videolectures.net/mlss06tw_roweis_nca/ ], Sam Roweis,1 comment\n73. 2295 views, 28:18, Learning an Outlier-Robust Kalman Filter [ http://videolectures.net/ecml07_ting_lor/ ], Jo-Anne Ting,1 comment\n74. 1308 views, 25:08, Probabilistic Machine Learning in Computational Advertising [ http://videolectures.net/nipsworkshops09_graepel_pmlca/ ], Thore Graepel\n75. 2670 views, 4:22:31, Gaussian Processes [ http://videolectures.net/mlss03_rasmussen_gp/ ], Carl Edward Rasmussen, 2 comments\n76. 1772 views, 58:42, Probabilistic Decision-Making Under Model Uncertainty [ http://videolectures.net/cmulls08_pineau_pdm/ ],  Joelle Pineau\n77. 2198 views, 58:51, Who is Afraid of Non-Convex Loss Functions? [ http://videolectures.net/eml07_lecun_wia/ ],  Yann LeCun\n78.  339 views, 54:15, Machine Learning Markets [ http://videolectures.net/nipsworkshops2011_storkey_markets/ ], Amos Storkey\n79. 2560 views, 1:49:01, Generalized Principal Component Analysis (GPCA) [ http://videolectures.net/mlss05au_vidal_gpca/ ], Rene Vidal,8 comments\n80. 1247 views, 25:00, FPGA-based MapReduce Framework for Machine Learning [ http://videolectures.net/nipsworkshops09_xu_fpga/ ], Ningyi Xu\n81. 2527 views, 58:39, Latent Semantic Variable Models [ http://videolectures.net/slsfs05_hofmann_lsvm/ ], Thomas Hofmann,3 comments\n82. 324 views, 18:31, k-NN Regression Adapts to Local Intrinsic Dimension [ http://videolectures.net/nips2011_kpotufe_intrinsic/ ], Samory Kpotufe\n83. 1485 views, 1:20:37, Lecture 14 \u2013 The Factor Analysis Model [ http://videolectures.net/stanfordcs229f08_ng_lec14/ ], Andrew Ng\n84. 2000 views, 1:11:49, Hierarchical Clustering [ http://videolectures.net/epsrcws08_teh_hc/ ], Yee Whye Teh\n85. 316 views, 16:38, Discussion of Erik Sudderth\u2019s talk: NPB Hype or Hope? [ http://videolectures.net/nipsworkshops2011_lecun_discussant/ ], Yann LeCun\n86. 309 views, 16:15, A Collaborative Mechanism for Crowdsourcing Prediction Problems [ http://videolectures.net/nips2011_abernethy_prediction/ ], Jacob Aberneth\n87. 1993 views, 39:15, Speeding Up Stochastic Gradient Descent [ http://videolectures.net/eml07_bengio_ssg/ ], Yoshua Bengio\n88. 126 views, 24:42, LODifier: Generating Linked Data from Unstructured Text [ http://videolectures.net/eswc2012_augenstein_lodifier/ ], Isabelle Augenstein\n89. 304 views, 19:47, Iterative Learning for Reliable Crowdsourcing Systems [ http://videolectures.net/nips2011_oh_crowdsourcing/ ], Sewoong Oh\n90. 1246 views, 24:03, Collaborative Filtering with Temporal Dynamics [ http://videolectures.net/kdd09_koren_cftd/ ], Yehuda Koren\n91. 714 views, 21:56, HIV-Haplotype Inference using a Constraintbased Dirichlet Process Mixture Model [ http://videolectures.net/nipsworkshops2010_prabhakaran_rey_hiv/ ], Sandhya Prabhakaran, Melanie Rey\n92. 1272 views, 22:40, Modeling the S&P 500 Index using the Kalman Filter and the LagLasso [ http://videolectures.net/amlcf09_mahler_mtsp/ ], Nicolas Mahle\n93. 2064 views, 10:47, Ten problems for the next 10 years [ http://videolectures.net/icml07_domingos_tpf/ ], Pedro Domingos, 1 comment\n94.  2097 views, 23:15, Best Paper \u2013 Information-Theoretic Metric Learning [ http://videolectures.net/icml07_kulis_itml/ ], Brian Kulis\n95.  926 views, 1:10:31, Neuroscience, cognitive science and machine learning [ http://videolectures.net/mlss2010_kording_ncsam/ ], Konrad K\u00f6rding\n96.  2210 views, 1:21:57, Introduction to Kernel Methods [ http://videolectures.net/mlss05us_niyogi_ikm/ ], Partha Niyogi, 5 comments\n97.  291 views, 12:00, Fast and Accurate k-means For Large Datasets [ http://videolectures.net/nips2011_shindler_largedatasets/ ], Michael Shindler\n98. 2203 views, 2:56:16, Probabilistic and Bayesian Modelling I [ http://videolectures.net/sscs06_opper_pbm/ ], Manfred Opper, 1 comment\n99. 2198 views, 1:00:00, Nonparametric Bayesian Models in Machine Learning [ http://videolectures.net/mlws04_ghahramani_nbmml/ ], Zoubin Ghahramani\n100. 1901 views, 48:34, Machine Learning for Intrusion Detection [ http://videolectures.net/mmdss07_laskov_mlit/ ], Pavel Laskov\n\nRelated articles\n * Scalable Machine Learning Course [ http://datascience101.wordpress.com/2012/08/20/scalable-machine-learning-course/ ](Data Science 101 [ http://datascience101.wordpress.com ])\n * 7th Annual Machine Learning Symposium [ http://hunch.net/?p=2586 ](Page on hunch.net [ http://hunch.net ])\n * Message-Oriented Object Design and Machine Learning in JavaScript [ http://architects.dzone.com/articles/message-oriented-object-design ](Architectural Design Patterns & Best Practices [ http://architects.dzone.com ])\n * Large Scale (Machine) Learning at Twitter [ http://videolectures.net/site/news/kolcz_twitterlargescale/ ](VideoLectures.NET - VideoLectures.NET [ http://videolectures.net ])\n * Hidden Benefits of Online Learning [ http://www.i-programmer.info/news/150-training-a-education/4951-hidden-benefits-of-online-learning.html ] (programming, reviews and projects [ http://i-programmer.info ])\n * Model checking and model understanding in machine learning [ http://andrewgelman.com/2012/09/model-checking-and-model-understanding-in-machine-learning/ ] (Statistical Modeling, Causal Inference, and Social Science - [ http://andrewgelman.com ])\n * Machine Learning Book for Students and Researchers [ http://googleresearch.blogspot.com/2012/08/machine-learning-book-for-students-and.html ] (Research Blog [ http://googleresearch.blogspot.com ])\n * Wagstaff (2012, jpl) machine learning that matters [ http://www.slideshare.net/daniel_bilar/wagstaff-2012-jpl-machine-learning-that-matters ] (Slideshare.net [ http://slideshare.net ])\n * Why becoming a data scientist might be easier than you think [ http://gigaom.com/data/why-becoming-a-data-scientist-might-be-easier-than-you-think/ ] (Gigaom [ http://gigaom.com ])\n * The Man-Machine Framework: How to Build Machine-Learning Applications the Right Way [ http://www.forbes.com/sites/danwoods/2012/10/18/the-man-machine-framework-how-to-build-machine-learning-applications-the-right-way/ ] (Information for the World's Business Leaders [ http://forbes.com ])\n",
                "If you are getting started with machine learning I would recommend starting with the (now very popular) videos from Andrew Ng's course, which you can find at Coursera [ https://class.coursera.org/ml-003/lecture ] or in several other versions (e.g. Stanford Engineering Everywhere CS229 - Machine Learning [ http://videolectures.net/stanfordcs229f07_machine_learning/ ]). \n\nAnother great introductory material is the set of videos for Hastie and Tibshirani's Introduction to Statistical Learning [ http://www-bcf.usc.edu/~gareth/ISL/ ] (you can find direct links to the videos in this blog post (In-depth introduction to machine learning in 15 hours of expert videos [ http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/ ]). \n\nFinally, I would also recommend going through some of the videos posted to Machine Learning Summer School's from recent year. Here is a playlist including some of the videos from the 2014 School at CMU (including my own) Machine Learning Summer School 2014 [ https://www.youtube.com/playlist?list=PLZSO_6-bSqHQCIYxE3ycGLXHMjK3XV7Iz ]",
                "Lectures by Tom Mitchell & Thorsten Jochims\nMachine Learning [ http://www.cs.cmu.edu/~tom/10701_sp11/lectures.shtml ]\nCS4780 Machine Learning Course, Thorsten Joachims, Cornell University [ http://machine-learning-course.joachims.org/ ]\nSome Learning material on Adv ML\nCS281: Advanced Machine Learning [ http://www.seas.harvard.edu/courses/cs281/ ]\nTalks by Christopher bishop.\nChristopher M. Bishop | Talks [ http://research.microsoft.com/en-us/um/people/cmbishop/talks.htm ]",
                "Adding a very recent talk by Yann LeCun (published today April 29 2017, actual talk was in March 2017) to the comprehensive list of links already present here in the other answers.\n\nPerhaps one of the best talks on the subject, particularly for its breadth and detailed analysis/evaluation of the state of art.\n\nFew highlights from the talk (1hr and 48 mins including Q&A at end)\n\n * a sketch of the breakthroughs that led to the resurgence of machine learning, in part from his own personal contribution (convnets) - may be informative even for those of us who have heard/read it many times (e.g. SVMs and convnets originated at the same time with its authors separated by a few rooms).\n * a candid admission, with examples, of why we are still a long way of when it comes to creating true intelligence - i.e. machines with world models that go beyond just answering questions at a children\u2019s book level. Reference. Tracking the world state with recurrent entity networks [ https://openreview.net/pdf?id=rJTKKKqeg ]. The need for a reasonable understanding of the world or \u201ccommon sense\u201d which is virtually non-existent in current models. Reference. What are the most promising consumer product areas for ML? [ https://www.quora.com/What-are-the-most-promising-consumer-product-areas-for-ML-Where-will-we-see-most-impressive-breakthroughs-in-next-2-3-yrs-cars-computer-vision-etc/answer/Ajit-Rajasekharan ]\n * His view that key to intelligence is predictive or unsupervised learning - i.e. a model that can be trained by just observing the world without being fed labeled data by humans.\n * \n * He cites Hinton\u2019s argument about the necessity for unsupervised learning in humans.\n * \n * Our brains have about 10^14 synapses (learnable parameters) and we live for about 10^9 seconds. So we have lot more parameters than data. This motivates the idea that we must do a lot of unsupervised learning since the perceptual input (including proprioception - the sense of relative positions of various parts of body) is the only place where we can get 10^5 dimensions of constraint per second - AMA Geoffrey Hinton \u2022 MachineLearning [ https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/ ]\n * Only our unsupervised perception of our environment can account for 10^5 dimensions of constraint per second - the information we get from language or teachers (our labeled data) is not enough.\n * To give a very crude reference point to the potential capacity of the brain - 10^5 dimensions /second - an unsupervised learning model, word2vec captures semantic relatedness of words in under 500 dimensions for a substantially large corpus (much larger than entire wikipedia).\n\n\n * His believes reinforcement learning has its value in certain tasks like games (e.g. AlphGo) but it doesn\u2019t work so well in real life scenarios, where learning has to happen in real time - a model cannot afford to learn from many trials in real life. Towards the end of the talk in Q&A he explains in detail how AlphaGo really works and the role reinforcement learning played in its success.\n * He then introduces Adversarial learning as the most exciting idea in machine learning in last 10 years. Reference. Attacking Machine Learning with Adversarial Examples [ https://blog.openai.com/adversarial-example-research/ ]\n * \n * The power of these learning models is the ability to predict the future in uncertain conditions.\n * They do reasonably well in some prediction tasks (predicting 5 frames into future given an input video stream - e.g. filling up the remaining view of an apartment given initial view. Things fall apart when prediction extends to 50 frames - indicating we still have ways to go).\n\n * Then in Q&A, questions range from broad borderline philosophical topics (what is consciousness, is the concern of machines taking over the world really warranted etc.) to very pragmatic ones like how power efficient current models are compared to our brains.\n * \n * Yann says they are terrible compared to human brain - a typical GPU card\u2019s compute power is about 10 Teraflops at ~200 watts power consumption. The brain on the other hand ( a low estimate) is 100,000 Teraflops at 25 watts power consumption. He suggests some engineering trade-offs that can bring down power consumption and still perform well for certain tasks (lower precision, avoiding movement of data to extent possible etc.) and are being pursued by many startups.\n * This particular analysis by Yann is reminiscent of an analysis/comparison Von Neumann dating back over 50 years ago, except Von Neumann compared computing units made up of vacuum tubes to neurons in the human brain Reference. Theory of self replicating automata - Von Neumann and Arthur brooks (completed after Von Neumann\u2019s death) 1966 [ http://cba.mit.edu/events/03.11.ASE/docs/VonNeumann.pdf ]\n\n________________________________________________\n\n\n * In his response to the last question, he discusses the current (dismal) status of virtual assistants and poses the question himself, \u201cwhen will we have truly intelligent virtual assistants(not ones like Siri where it throws funny scripted answers when things get slightly complicated) that we can ask questions and not be frustrated because we can sense it truly understands\u201d -\n * \n * he mentions an intelligent virtual assistant experiment in Facebook (partly automated but largely handled by humans behind the scene) which revealed the fact that there is a long tail of very complicated things that people want out of virtual assistants, for which the technology doesn\u2019t exist yet and his prediction is that it would at least take one decade if not more to get there.\n\nVideo link to Yann\u2019s talk.\n\nhttps://www.youtube.com/watch?v=qjI8lJn8Mss\n",
                "I enjoy talks by Dr. D. J. Patil [ https://www.quora.com/profile/DJ-2833 ], Data Scientist in Residence at Greylock (ex-LinkedIn). They tend to be insightful and engaging. Several of them are available on YouTube.\n \nHere are just two examples:\nhttp://www.youtube.com/watch?v=OSQNn83UZBk\n \nhttp://www.youtube.com/watch?v=J_CYKk8q1Ao\n",
                "I'll try to give a very crude overview of how the pieces fit in together, because the details span multiple books. Please forgive me for some oversimplifications.\n\n * MapReduce is the Google paper that started it all (Page on googleusercontent.com [ http://static.googleusercontent.com/media/research.google.com/en/us/archive/mapreduce-osdi04.pdf ]). It's a paradigm for writing distributed code inspired by some elements of functional programming. You don't have to do things this way, but it neatly fits a lot of problems we try to solve in a distributed way. The Google internal implementation is called MapReduce and Hadoop is it's open-source implementation. Amazon's Hadoop instance is called Elastic MapReduce (EMR) and has plugins for multiple languages.\n * HDFS is an implementation inspired by the Google File System [ http://research.google.com/archive/gfs.html ] (GFS) to store files across a bunch of machines when it's too big for one. Hadoop consumes data in HDFS (Hadoop Distributed File System).\n * Apache Spark is an emerging platform that has more flexibility than MapReduce but more structure than a basic message passing interface. It relies on the concept of distributed data structures (what it calls RDDs) and operators. See this page for more: The Apache Software Foundation [ http://apache.org ] \n * Because Spark is a lower level thing that sits on top of a message passing interface, it has higher level libraries to make it more accessible to data scientists. The Machine Learning library built on top of it is called MLib and there's a distributed graph library called GraphX.\n * Pregel and it's open source twin Giraph is a way to do graph algorithms on billions of nodes and trillions of edges over a cluster of machines. Notably, the MapReduce model is not well suited to graph processing so Hadoop/MapReduce are avoided in this model, but HDFS/GFS is still used as a data store.\n * Zookeeper is a coordination and synchronization service that a distributed set of computer make decisions by consensus, handles failure, etc.\n * Flume and Scribe are logging services, Flume is an Apache project and Scribe is an open-source Facebook project. Both aim to make it easy to collect tons of logged data, analyze it, tail it, move it around and store it to a distributed store.\n * Google BigTable and it's open source twin HBase were meant to be read-write distributed databases, originally built for the Google Crawler that sit on top of GFS/HDFS and MapReduce/Hadoop. Google Research Publication: BigTable [ http://research.google.com/archive/bigtable.html ]\n * Hive and Pig are abstractions on top of Hadoop designed to help analysis of tabular data stored in a distributed file system (think of excel sheets too big to store on one machine). They operate on top of a data warehouse, so the high level idea is to dump data once and analyze it by reading and processing it instead of updating cells and rows and columns individually much. Hive has a language similar to SQL while Pig is inspired by Google's Sawzall - Google Research Publication: Sawzall [ http://research.google.com/archive/sawzall.html ]. You generally don't update a single cell in a table when processing it with Hive or Pig.\n * Hive and Pig turned out to be slow because they were built on Hadoop which optimizes for the volume of data moved around, not latency. To get around this, engineers bypassed and went straight to HDFS. They also threw in some memory and caching and this resulted in Google's Dremel (Dremel: Interactive Analysis of Web-Scale Datasets [ http://research.google.com/pubs/pub36632.html ]), F1 (F1 - The Fault-Tolerant Distributed RDBMS Supporting Google's Ad Business [ http://research.google.com/pubs/pub38125.html ]), Facebook's Presto (Presto | Distributed SQL Query Engine for Big Data [ http://prestodb.io/ ]), Apache Spark SQL (Page on apache.org [ http://spark.incubator.apache.org/and ] ), Cloudera Impala (Cloudera Impala: Real-Time Queries in Apache Hadoop, For Real [ http://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/ ]), Amazon's Redshift, etc. They all have slightly different semantics but are essentially meant to be programmer or analyst friendly abstractions to analyze tabular data stored in distributed data warehouses.\n * Mahout (Scalable machine learning and data mining [ https://mahout.apache.org/ ]) is a collection of machine learning libraries written in the MapReduce paradigm, specifically for Hadoop. Google has it's own internal version but they haven't published a paper on it as far as I know.\n\n * Oozie is a workflow scheduler. The oversimplified description would be that it's something that puts together a pipeline of the tools described above. For example, you can write an Oozie script that will scrape your production HBase data to a Hive warehouse nightly, then a Mahout script will train with this data. At the same time, you might use pig to pull in the test set into another file and when Mahout is done creating a model you can pass the testing data through the model and get results. You specify the dependency graph of these tasks through Oozie (I may be messing up terminology since I've never used Oozie but have used the Facebook equivalent).\n\n * Lucene is a bunch of search-related and NLP tools but it's core feature is being a search index and retrieval system. It takes data from a store like HBase and indexes it for fast retrieval from a search query. Solr uses Lucene under the hood to provide a convenient REST API for indexing and searching data. ElasticSearch is similar to Solr.\n * Sqoop is a command-line interface to back SQL data to a distributed warehouse. It's what you might use to snapshot and copy your database tables to a Hive warehouse every night.\n * Hue is a web-based GUI to a subset of the above tools - http://gethue.com/\n",
                "The author of this content is: Mr. Praveen Kumar.\n\nA few years ago, when I was in college, I attended a workshop organized by FOSSEE [ http://python.fossee.in/ ] and it was in this workshop that I met \"Python\" and fell in love with her at first sight. I was amazed at how easy it was to write simple solutions to complex problems in Python. One of the utility I learnt at the workshop was an image to ASCII art generator.\n\nHow it works?\n\nWe scale a given image to a standard resolution that suitably represents the ASCII version of a given image. The scaled version is then converted to a grayscale image. In a grayscale image, there are 256 shades of gray, or in other words, each pixel carries only the intensity information which is represented by an 8 bit value. A pixel with a value of 0 is assumed to be black and the one with 255 is assumed to be white. We divide the whole range of 0-255 into 11 smaller ranges of 25 pixels each and then assign each pixel a character according to the range it falls in. The point is to assign a group of pixels with slightly varying intensity the same ASCII char. We use the PIL library to play with the images. The code given below is almost self explanatory. The default char mapping and resolution doesn't render good ASCII arts for every image size and so you should try modifying the char mapping and image size to the one that best represents the given image.\n\nHere are some ASCII arts:\n\nDependencies:\n\nPIL(Python Imaging Library)\n\nUnder Ubuntu\n\n[code]$ sudo pip install Pillow\n[/code]Code:\n\n[code]ASCII_CHARS = [ '#', '?', '%', '.', 'S', '+', '.', '*', ':', ',', '@']\n\ndef scale_image(image, new_width=100):\n    \"\"\"Resizes an image preserving the aspect ratio.\n    \"\"\"\n    (original_width, original_height) = image.size\n    aspect_ratio = original_height/float(original_width)\n    new_height = int(aspect_ratio * new_width)\n\n    new_image = image.resize((new_width, new_height))\n    return new_image\n\ndef convert_to_grayscale(image):\n    return image.convert('L')\n\ndef map_pixels_to_ascii_chars(image, range_width=25):\n    \"\"\"Maps each pixel to an ascii char based on the range\n    in which it lies.\n\n    0-255 is divided into 11 ranges of 25 pixels each.\n    \"\"\"\n\n    pixels_in_image = list(image.getdata())\n    pixels_to_chars = [ASCII_CHARS[pixel_value/range_width] for pixel_value in\n            pixels_in_image]\n\n    return \"\".join(pixels_to_chars)\n\ndef convert_image_to_ascii(image, new_width=100):\n    image = scale_image(image)\n    image = convert_to_grayscale(image)\n\n    pixels_to_chars = map_pixels_to_ascii_chars(image)\n    len_pixels_to_chars = len(pixels_to_chars)\n\n    image_ascii = [pixels_to_chars[index: index + new_width] for index in\n            xrange(0, len_pixels_to_chars, new_width)]\n\n    return \"\\n\".join(image_ascii)\n\ndef handle_image_conversion(image_filepath):\n    image = None\n    try:\n        image = Image.open(image_filepath)\n    except Exception, e:\n        print \"Unable to open image file {image_filepath}.\".format(image_filepath=image_filepath)\n        print e\n        return\n\n    image_ascii = convert_image_to_ascii(image)\n    print image_ascii\n\nif __name__=='__main__':\n    import sys\n\n    image_file_path = sys.argv[1]\n    handle_image_conversion(image_file_path)Source : https://www.hackerearth.com/notes/beautiful-python-a-simple-ascii-art-generator-from-images/\n[/code]Source: Beautiful Python: A Simple ASCII Art Generator from Images - Praveen Kumar [ https://www.hackerearth.com/notes/beautiful-python-a-simple-ascii-art-generator-from-images/ ]\n\nThank you\u2026. :)",
                "I read a lot of Cool tricks here,I wonder if anyone mentioned this, anyways! I am going to share You this Little trick which i find very pythonic!\n\nUnderstanding the underscore( _ ) of Python!\n\nThere are 5 cases for using the underscore in Python.\n\n1. For storing the value of last expression in interpreter.\n2. For ignoring the specific values. (so-called \u201cI don\u2019t care\u201d)\n3. To give special meanings and functions to name of vartiables or functions.\n4. To use as \u2018Internationalization(i18n)\u2019 or \u2018Localization(l10n)\u2019 functions.\n5. To separate the digits of number literal value.\nLet\u2019s look at each case.\n\nWhen used in interpreter\n\nThe python interpreter stores the last expression value to the special variable called \u2018_\u2019. This feature has been used in standard CPython interpreter first and you could use it in other Python interpreters too.\n\nFor Ignoring the values\n\nThe underscore is also used for ignoring the specific values. If you don\u2019t need the specific values or the values are not used, just assign the values to underscore.\n\nGive special meanings to name of variables and functions\n\nThe underscore may be most used in \u2018naming\u2019. The PEP8 which is Python convention guideline introduces the following 4 naming cases.\n\n_single_leading_underscore\nThis convention is used for declaring private variables, functions, methods and classes in a module. Anything with this convention are ignored in [code ]from module import *[/code]. \nHowever, of course, Python does not supports truly private, so we can not force somethings private ones and also can call it directly from other modules. So sometimes we say it \u201cweak internal use indicator\u201d.\n\nsingle_trailing_underscore_\nThis convention could be used for avoiding conflict with Python keywords or built-ins. You might not use it often.\n\n__double_leading_underscore\nThis is about syntax rather than a convention. double underscore will mangle the attribute names of a class to avoid conflicts of attribute names between classes. (so-called \u201cmangling\u201d that means that the compiler or interpreter modify the variables or function names with some rules, not use as it is) \nThe mangling rule of Python is adding the \u201c_ClassName\u201d to front of attribute names are declared with double underscore. \nThat is, if you write method named \u201c__method\u201d in a class, the name will be mangled in \u201c_ClassName__method\u201d form.\n\nAnd the best One is this:\n\nTo separate the digits of number literal value\n\nThis feature was added in Python 3.6. It is used for separating digits of numbers using underscore for readability.\n\nEdit 1\u2192 Extending Unpacking ,*_ is available in Python 3 Only!\n\nsrc\u2192Understanding the underscore( _ ) of Python \u2013 Hacker Noon [ https://hackernoon.com/understanding-the-underscore-of-python-309d1a029edc ] !\n\nHala Python!",
                "No. I wrote my first ML program waaay back in 1982, before there was Internet, Google, GPU computing, laptops, cellphones, digital cameras, desktop PCs, heck before there was almost anything remotely resembling what you see in the tech world around you today.\n\nHow did I even discover the existence of such a field? Back then, to educate oneself, you read books. Of course, you had to either go to a library, or in my case, a quaint event called a book fair. I attended a large book fair in New Delhi, India\u2019s capital, and picked up this 800 page tome, a fairly massive affair. Why, I don\u2019t know. After all, I was at the Indian Institute of Technology, Kanpur, studying to become an electrical engineer.\n\nHofstadter\u2019s first book, and in my opinion, still his best, was an utter revelation to me. It opened up a whole new world of imagination of what deep links there are between art, music and abstract math, realized by the three central characters \u2014 Johann Sebastian Bach, Maurice Escher, Kurt G\u00f6del \u2014 and computing, including of course AI and ML.\n\nThe book featured an intriguing set of visual puzzles from a Russian researcher named Bongard, where the task is to discover a rule that separates the six figures on the left from those on the right. This is an elementary problem in ML called classification. It\u2019s analogous to distinguishing email from spam or deciding if an image contains a face. As humans, we classify sensory stimuli billions of times through our lives, and our very survival depends on it. As you cross the road, is the object approaching you a person or a Fedex truck? Get the answer wrong and your life may indeed be over. Not surprisingly, we solve such problems amazingly well.\n\nI\u2019ll leave you to work this one out, but with absolutely no training in this field, I nonetheless decided to foolishly make this the core of my Masters thesis project. Somehow I plodded through and worked out a solution, however naively it seems in retrospect. That experience made me realize that AI and ML was my life\u2019s goal, and I decided to come to the United States in 1983, where I was incredibly fortunate to work with this brilliant Stanford educated researcher, Thomas Mitchell, now the Dean of the School of Computer Science at Carnegie Mellon University.\n\nFrom Tom, I learned the most important lesson of all, which no book can teach you. Research is *fun*. He simply embodied the spirit of a researcher who bubbled with enthusiasm for the field of ML. He worked harder than anyone I had met, yet seemed to be having a ball. That lesson made a huge imprint on me and stayed with me ever since.\n\nAfter getting my PhD, I joined IBM Watson Research in New York in late 1989, where they couldn\u2019t figure out what to do with an ML researcher. So they threw me into a newly formed robotics group, even though I had absolutely no background in this field. I had never ever programmed a robot. Amazingly enough, I seemed to thrive in this somewhat challenging situation, and ended up writing some of my most highly cited papers, exploring how robots can acquire new behaviors using the newly emerging field of reinforcement learning. I also published in 1993 perhaps the first book on robot learning, which featured research from all over the world in this new area of AI. Despite having no background in robotics, I still managed to make a name for myself.\n\nMany years later, I was elected a Fellow of AAAI, the leading international professional society for AI researchers. Each year, a small handful of researchers are selected and the competition is fierce. This year\u2019s AAAI Fellows include some of the founders of the deep learning revolution: Yoshua Bengio, and Yann Le Cun.\n\nThe list of AAAI Fellows include some of the most amazing researchers in AI and ML, and I\u2019m humbled to be listed in such distinguished company. None of this would have happened if back in 1982, I thought doing ML with no formal training in this field, with primitive computing, or doing robot learning at IBM in 1990 with no training in robotics, was \u201cdifficult\u201d.\n\nFor those aspiring young researchers reading this on Quora, the best advice I can give you is that nothing is \u201cdifficult\u201d if you set yourself the challenge of working on it. Above all, remember: research is fun! It\u2019s an exploration into the unknown.\n\nFor many years, from 2001\u20132018, I was privileged to co-direct the Autonomous Learning Laboratory at the University of Massachusetts, Amherst, with one of reinforcement learning\u2019s true pioneers, Andrew Barto. Andy and his former PhD student, Rich Sutton, helped establish the modern field of RL, the area that gave rise to Deep Mind and Alpha Go Zero. Andy and Rich embodied the true spirit of researchers having fun and working with them was the best professional experience of my career.\n\nPhD students at the lab hung up a sign on the main door that was a quote from one of the most distinguished scientists of all time, Albert Einstein.\n\nThat sums it all. Research doesn\u2019t need expertise. Einstein in fact hated textbook knowledge. Above all, he prized imagination, the ability to dream. If you want to make your children smart, he told parents, teach them fairy stories.\n\nAs we battle the latest pandemic, the Wuhan coronavirus, the biggest weapon at our disposal is our ability to sequence its genome. The biggest breakthrough in biology of the 20th century came from Watson and Crick, two brash biologists who upturned the world of biology by having fun! Watson went on to write a highly popular account of their discovery called the Double Helix. In it he tells the story of how they scandalized established researchers, like Oswald Avery of Columbia University, when he realized they didn\u2019t know elementary biochemistry. Yet, by playing with 3D models and in effect stealing from Rosalind Franklin\u2019s carefully gathered data sets, they cracked the secret of life. They were simply having fun!\n\nSo, again, my answer is, no, ML is not difficult. It is fun!",
                "As someone who often finds himself explaining machine learning to non-experts, I offer the following list as a public service announcement.\n\n1. Machine learning means learning from data [ https://work.caltech.edu/telecourse ]; AI is a buzzword. Machine learning lives up to the hype: there are an incredible number of problems that you can solve by providing the right training data to the right learning algorithms. Call it AI if that helps you sell it, but know that AI, at least as used outside of academia, is often a buzzword that can mean whatever people want it to mean.\n2. Machine learning is about data and algorithms, but mostly data. There\u2019s a lot of excitement about advances in machine learning algorithms, and particularly about deep learning [ https://en.wikipedia.org/wiki/Deep_learning ]. But data is the key ingredient that makes machine learning possible. You can have machine learning without sophisticated algorithms, but not without good data. [ https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf ]\n3. Unless you have a lot of data, you should stick to simple models. Machine learning trains a model from patterns in your data, exploring a space of possible models defined by parameters. If your parameter space is too big, you\u2019ll overfit [ https://en.wikipedia.org/wiki/Overfitting ] to your training data and train a model that doesn\u2019t generalize [ https://en.wikipedia.org/wiki/Generalization_error ] beyond it. A detailed explanation requires more math [ https://en.wikipedia.org/wiki/VC_dimension ], but as a rule you should keep your models as simple as possible.\n4. Machine learning can only be as good as the data you use to train it. The phrase \u201cgarbage in, garbage out [ https://en.wikipedia.org/wiki/Garbage_in,_garbage_out ]\u201d predates machine learning, but it aptly characterizes a key limitation of machine learning. Machine learning can only discover patterns that are present in your training data. For supervised machine learning [ https://en.wikipedia.org/wiki/Supervised_learning ] tasks like classification [ https://en.wikipedia.org/wiki/Statistical_classification ], you\u2019ll need a robust collection of correctly labeled, richly featured training data.\n5. Machine learning only works if your training data is representative. Just as a fund prospectus warns that \u201cpast performance is no guarantee of future results\u201d, machine learning should warn that it\u2019s only guaranteed to work for data generated by the same distribution that generated its training data. Be vigilant of skews between training data and production data, and retrain your models frequently so they don\u2019t become stale.\n6. Most of the hard work for machine learning is data transformation. From reading the hype about new machine learning techniques, you might think that machine learning is mostly about selecting and tuning algorithms. The reality is more prosaic: most of your time and effort goes into data cleansing [ https://en.wikipedia.org/wiki/Data_cleansing ] and feature engineering [ https://en.wikipedia.org/wiki/Feature_engineering ]\u200a\u2014\u200athat is, transforming raw features [ https://en.wikipedia.org/wiki/Feature_%28machine_learning%29 ] into features that better represent the signal in your data.\n7. Deep learning is a revolutionary advance, but it isn\u2019t a magic bullet. Deep learning has earned its hype by delivering advances across a broad range of machine learning application areas. Moreover, deep learning automates some of the work traditionally performed through feature engineering, especially for image and video data. But deep learning isn\u2019t a silver bullet. You can\u2019t just use it out of the box, and you\u2019ll still need to invest significant effort in data cleansing and transformation.\n8. Machine learning systems are highly vulnerable to operator error. With apologies to the NRA, \u201cMachine learning algorithms don\u2019t kill people; people kill people.\u201d When machine learning systems fail, it\u2019s rarely because of problems with the machine learning algorithm. More likely, you\u2019ve introduced human error into the training data, creating bias or some other systematic error. Always be skeptical, and approach machine learning with the discipline you apply to software engineering.\n9. Machine learning can inadvertently create a self-fulfilling prophecy. In many applications of machine learning, the decisions you make today affect the training data you collect tomorrow. Once your machine learning system embeds biases into its model, it can continue generating new training data that reinforces those biases. And some biases can ruin people\u2019s lives. [ https://medium.com/@dtunkelang/getting-uncomfortable-with-data-7339e27adf6f ] Be responsible: don\u2019t create self-fulfilling prophecies.\n10. AI is not going to become self-aware, rise up, and destroy humanity. A surprising number of people (cough [ https://www.recode.net/2017/7/15/15976744/elon-musk-artificial-intelligence-regulations-ai ]) seem to be getting their ideas about artificial intelligence from science fiction movies. We should be inspired by science fiction, but not so credulous that we mistake it for reality. There are enough real and present dangers to worry about, from consciously evil human beings to unconsciously biased machine learning models. So you can stop worrying about SkyNet [ https://en.wikipedia.org/wiki/Skynet_%28Terminator%29 ] and \u201csuperintelligence [ https://en.wikipedia.org/wiki/Superintelligence ]\u201d.\nThere\u2019s far more to machine learning than I can explain in a top-10 list. But hopefully this serves as a useful introduction for non-experts.",
                "Great question! How indeed does one prepare oneself for a (research or otherwise) career in machine learning, in particular in terms of familiarizing oneself with the underlying mathematics? I\u2019m going to resist the temptation of trotting out some standard books, and instead, focus on giving broad advice.\n\nThere\u2019s some bad news on this front, and it\u2019s best to get this out of the way as quickly as possible. Having spent 35+ years studying machine learning, let me put this in the most direct way possible: no matter how much time and effort you devote to it, you can never know enough math to read through all the ML literature. Different parts of ML use a variety of esoteric math. There\u2019s just no way one person can know all of this math, so it\u2019s good to be forewarned.\n\nOK, with that out of the way, how does one prepare oneself? Think of the process analogous to conditioning your mind and body to run a marathon. It\u2019s a gradual process, of improving your fitness, your ability to run for longer and longer distances, your breathing technique, your mental focus, and dozens of other dimensions. Working in ML is not like running a 100 meter sprint, where the race is pretty much over in a single breath. It\u2019s much more of an endurance sport, where you have to constantly work at it to remain in shape, and there\u2019s no point at which you can relax and say: OK, I know it all! Because no one does!\n\nAn example from my recent work will clarify the issues involved. One of the major challenges in machine learning is that there\u2019s never enough training data to tackle every ML problem that presents itself. Humans are especially adept in solving this challenge. I can get on a flight from San Francisco and within a few short hours find myself in a dizzying diversity of new environments, from the glitzy subways of Tokyo and the bleak winter in Scandinavia to an arid savannah in Africa, or a swampy rainforest in Brazil. There\u2019s no way I can ever hope to collect training samples from every possible environment that I can encounter in life. So, what do we do? We transfer our learned knowledge from places we\u2019ve been \u2014 so, having taken the BART subway in San Francisco, and subways in New York and London, I can try to handle the complexity of the subway in Tokyo by drawing upon my previous experience. Of course, it doesn\u2019t quite match \u2014 the language is completely different, the tone and texture of the visual experience is completely different (attendants in gloved hands show you the way in Tokyo \u2014 no such luxury is available in the US!). Yet, we somehow manage, and plod our way through new experiences. We even cherish the prospect of finding ourselves in some alien new culture, where we don\u2019t speak the language and can\u2019t ask for directions. It opens up our mind to new horizons, all part of the charm of travel.\n\nSo, what\u2019s the mathematics involved in implementing a transfer learning algorithm? It varies a lot depending on what type of approach you investigate. Let\u2019s review some approaches from computer vision over the past few years. One class of approaches are so-called subspace methods, where the training data from a collection of images in the \u201csource\u201d domain (which conveniently has labels given to us) is to be compared with a collection of unlabeled images from a \u201ctarget\u201d domain (e.g., \u201csource\u201d \u2192 NY subway, \u201ctarget\u201d \u2192 Tokyo subway).\n\nOne can take a collection of images of size NxN and using a variety of different methods find the smallest subspace that the source images lie in (treating each image as a vector in N^2 dimensions). Now, to understand this body of work, you obviously need to know some linear algebra. So, if you don\u2019t understand linear algebra, or you took a class way back when and forgot it all, it\u2019s time to refresh your memory or learn anew. Fortunately, there are excellent textbooks (Strang is usually a good place to start) and also something like MATLAB will let you explore linear algebraic ML methods without having to implement things like eigenvalue or singular value decomposition. As I usually told my students, keep the motto \u201ceigen do it if I try\u201d in mind. Persevere, and keep the focus on why you are learning this math. Because it is important and essential to understand much of modern ML.\n\nOK, great, you\u2019ve managed to learn some linear algebra. Are you done? Ummm, not quite. So, back to our transfer learning example. You construct a source subspace from the source images, and a target subspace from the target images. Umm, how does one do that. OK, you can use a garden variety dimensionality reduction method like principal components analysis (PCA), which just computes the dominant eigenvectors of the covariance matrices of the source and target images. This is one subroutine call in MATLAB. But, PCA is 100 years old. How about something new and cool, like a ooh la la subspace tracking method like GOUDA, which uses the fancier math of Lie groups. Oops, now you need to learn some group theory, the mathematics of symmetry. As it turns out, matrices of certain types, like all invertible matrices, or all positive definite matrices, are not just linear algebraic objects, they are also of interest in group theory, a particularly important subfield of which is Lie groups (Lie \u2192 \u201cLee\u201d).\n\nOK, great, you have a smattering of knowledge of group theory and Lie groups. Are you done? Hmmm\u2026actually not, because it turns out Lie groups are not just groups, but they are also continuous manifolds. What in the blazes is a \u201cmanifold\u201d? If you google this, you are likely to encounter web pages describing engine parts! No, a manifold is something entirely different in machine learning, where it means a non-Euclidean space that has curvature. It turns out the set of all probability distributions (e.g., 1 dimensional Gaussians with a scalar variance dimension and a scalar mean dimension) are not Euclidean, but rather, describe a curved space. So, the set of all positive definite matrices form a Lie group, with a certain curvature. What this implies is that obvious operations like taking the average have to done with considerable care. So, off you go, learning all there is to know about manifolds, Riemannian manifolds, tangent spaces, covariant derivatives, exp and log mappings, etc. Oh, what fun!\n\nGetting back to our transfer learning method, if you compute the source covariance matrix C_s and the target covariance matrix C_t, then there is a simple method called CORAL (for correlational alignment) that figures out how to transform C_s into C_t using some invertible mapping A. CORAL is popular as a transfer learning method in computer vision. But, CORAL does not actually use the knowledge that the space of positive definite matrices (or covariance matrices) forms a manifold. In fact, it forms something called a cone in convex analysis. If you subtract one covariance matrix from another, the result is not a covariance matrix. So, they do not form a vector space, but rather something else entirely. Oops, it turns out the study of cones is important in convex analysis, so there you go again, you need to learn about convex sets and functions, projections onto convex sets, etc. The dividing line between tractable and intractable optimization is not linear vs. nonlinear, but rather, convex vs. non-convex.\n\nI hope the pattern is becoming clear. Like one of those legendary Russian dolls, where each time you open one, you find it is not the end, but there\u2019s another one inside it, so it is with learning math in machine learning. Each time you learn a bit of math, you find it opens the door to an entirely new field of math, which you need to know something about as well. For my most recent paper, I had to digest a whole book devoted entirely to the topic of positive definite matrices (it\u2019s like the old joke, where the deeper you go, the more you know about a specialized topic, until you know everything about \u2014- nothing!).\n\nAny given problem in machine learning, like transfer learning, can be formulated as a convex optimization problem, as a manifold learning problem, as a multivariate statistical estimation problem, as a nonlinear gradient based deep learning problem, etc. etc. Each of these requires learning a bit about the underlying math involved.\n\nIf you feel discouraged, and feel like tearing your hair out at this point, I sympathize with you. But, on the other hand, you can look on the positive side, and realize that in terms of our analogy of running a marathon, you are steadily becoming better at running the long race, building your mathematical muscle as you go along, and gradually things start falling into place. Things start to make sense, and different subfields start connecting with each other. Something strange happens. You start liking it! Of course, there\u2019s a drawback. Someone who doesn't understand any of the math you get good at using asks you to explain your work, and you realize that it\u2019s impossible to do that without writing equations.\n\nMost researchers find their comfort zone and try to stay within it, since otherwise, it takes a great deal of time and effort to master the dozens of mathematical subfields that modern ML uses. But, this strategy eventually fails, and one is always forced to get outside one\u2019s comfort zone and learn some new math, since otherwise, a whole area of the field becomes alien to you.\n\nFortunately, the human brain is an amazing instrument, and provides decades and decades of trouble-free operation, allowing us to continually learn over 40,50, 60, years or more. How precisely it does that without zeroing out all prior learning is one of the greatest unsolved mysteries in science!",
                "Great question! How indeed does one prepare oneself for a (research or otherwise) career in machine learning, in particular in terms of familiarizing oneself with the underlying mathematics? I\u2019m going to resist the temptation of trotting out some standard books, and instead, focus on giving broad advice.\n\nThere\u2019s some bad news on this front, and it\u2019s best to get this out of the way as quickly as possible. Having spent 35+ years studying machine learning, let me put this in the most direct way possible: no matter how much time and effort you devote to it, you can never know enough math to read through all the ML literature. Different parts of ML use a variety of esoteric math. There\u2019s just no way one person can know all of this math, so it\u2019s good to be forewarned.\n\nOK, with that out of the way, how does one prepare oneself? Think of the process analogous to conditioning your mind and body to run a marathon. It\u2019s a gradual process, of improving your fitness, your ability to run for longer and longer distances, your breathing technique, your mental focus, and dozens of other dimensions. Working in ML is not like running a 100 meter sprint, where the race is pretty much over in a single breath. It\u2019s much more of an endurance sport, where you have to constantly work at it to remain in shape, and there\u2019s no point at which you can relax and say: OK, I know it all! Because no one does!\n\nAn example from my recent work will clarify the issues involved. One of the major challenges in machine learning is that there\u2019s never enough training data to tackle every ML problem that presents itself. Humans are especially adept in solving this challenge. I can get on a flight from San Francisco and within a few short hours find myself in a dizzying diversity of new environments, from the glitzy subways of Tokyo and the bleak winter in Scandinavia to an arid savannah in Africa, or a swampy rainforest in Brazil. There\u2019s no way I can ever hope to collect training samples from every possible environment that I can encounter in life. So, what do we do? We transfer our learned knowledge from places we\u2019ve been \u2014 so, having taken the BART subway in San Francisco, and subways in New York and London, I can try to handle the complexity of the subway in Tokyo by drawing upon my previous experience. Of course, it doesn\u2019t quite match \u2014 the language is completely different, the tone and texture of the visual experience is completely different (attendants in gloved hands show you the way in Tokyo \u2014 no such luxury is available in the US!). Yet, we somehow manage, and plod our way through new experiences. We even cherish the prospect of finding ourselves in some alien new culture, where we don\u2019t speak the language and can\u2019t ask for directions. It opens up our mind to new horizons, all part of the charm of travel.\n\nSo, what\u2019s the mathematics involved in implementing a transfer learning algorithm? It varies a lot depending on what type of approach you investigate. Let\u2019s review some approaches from computer vision over the past few years. One class of approaches are so-called subspace methods, where the training data from a collection of images in the \u201csource\u201d domain (which conveniently has labels given to us) is to be compared with a collection of unlabeled images from a \u201ctarget\u201d domain (e.g., \u201csource\u201d \u2192 NY subway, \u201ctarget\u201d \u2192 Tokyo subway).\n\nOne can take a collection of images of size NxN and using a variety of different methods find the smallest subspace that the source images lie in (treating each image as a vector in N^2 dimensions). Now, to understand this body of work, you obviously need to know some linear algebra. So, if you don\u2019t understand linear algebra, or you took a class way back when and forgot it all, it\u2019s time to refresh your memory or learn anew. Fortunately, there are excellent textbooks (Strang is usually a good place to start) and also something like MATLAB will let you explore linear algebraic ML methods without having to implement things like eigenvalue or singular value decomposition. As I usually told my students, keep the motto \u201ceigen do it if I try\u201d in mind. Persevere, and keep the focus on why you are learning this math. Because it is important and essential to understand much of modern ML.\n\nOK, great, you\u2019ve managed to learn some linear algebra. Are you done? Ummm, not quite. So, back to our transfer learning example. You construct a source subspace from the source images, and a target subspace from the target images. Umm, how does one do that. OK, you can use a garden variety dimensionality reduction method like principal components analysis (PCA), which just computes the dominant eigenvectors of the covariance matrices of the source and target images. This is one subroutine call in MATLAB. But, PCA is 100 years old. How about something new and cool, like a ooh la la subspace tracking method like GOUDA, which uses the fancier math of Lie groups. Oops, now you need to learn some group theory, the mathematics of symmetry. As it turns out, matrices of certain types, like all invertible matrices, or all positive definite matrices, are not just linear algebraic objects, they are also of interest in group theory, a particularly important subfield of which is Lie groups (Lie \u2192 \u201cLee\u201d).\n\nOK, great, you have a smattering of knowledge of group theory and Lie groups. Are you done? Hmmm\u2026actually not, because it turns out Lie groups are not just groups, but they are also continuous manifolds. What in the blazes is a \u201cmanifold\u201d? If you google this, you are likely to encounter web pages describing engine parts! No, a manifold is something entirely different in machine learning, where it means a non-Euclidean space that has curvature. It turns out the set of all probability distributions (e.g., 1 dimensional Gaussians with a scalar variance dimension and a scalar mean dimension) are not Euclidean, but rather, describe a curved space. So, the set of all positive definite matrices form a Lie group, with a certain curvature. What this implies is that obvious operations like taking the average have to done with considerable care. So, off you go, learning all there is to know about manifolds, Riemannian manifolds, tangent spaces, covariant derivatives, exp and log mappings, etc. Oh, what fun!\n\nGetting back to our transfer learning method, if you compute the source covariance matrix C_s and the target covariance matrix C_t, then there is a simple method called CORAL (for correlational alignment) that figures out how to transform C_s into C_t using some invertible mapping A. CORAL is popular as a transfer learning method in computer vision. But, CORAL does not actually use the knowledge that the space of positive definite matrices (or covariance matrices) forms a manifold. In fact, it forms something called a cone in convex analysis. If you subtract one covariance matrix from another, the result is not a covariance matrix. So, they do not form a vector space, but rather something else entirely. Oops, it turns out the study of cones is important in convex analysis, so there you go again, you need to learn about convex sets and functions, projections onto convex sets, etc. The dividing line between tractable and intractable optimization is not linear vs. nonlinear, but rather, convex vs. non-convex.\n\nI hope the pattern is becoming clear. Like one of those legendary Russian dolls, where each time you open one, you find it is not the end, but there\u2019s another one inside it, so it is with learning math in machine learning. Each time you learn a bit of math, you find it opens the door to an entirely new field of math, which you need to know something about as well. For my most recent paper, I had to digest a whole book devoted entirely to the topic of positive definite matrices (it\u2019s like the old joke, where the deeper you go, the more you know about a specialized topic, until you know everything about \u2014- nothing!).\n\nAny given problem in machine learning, like transfer learning, can be formulated as a convex optimization problem, as a manifold learning problem, as a multivariate statistical estimation problem, as a nonlinear gradient based deep learning problem, etc. etc. Each of these requires learning a bit about the underlying math involved.\n\nIf you feel discouraged, and feel like tearing your hair out at this point, I sympathize with you. But, on the other hand, you can look on the positive side, and realize that in terms of our analogy of running a marathon, you are steadily becoming better at running the long race, building your mathematical muscle as you go along, and gradually things start falling into place. Things start to make sense, and different subfields start connecting with each other. Something strange happens. You start liking it! Of course, there\u2019s a drawback. Someone who doesn't understand any of the math you get good at using asks you to explain your work, and you realize that it\u2019s impossible to do that without writing equations.\n\nMost researchers find their comfort zone and try to stay within it, since otherwise, it takes a great deal of time and effort to master the dozens of mathematical subfields that modern ML uses. But, this strategy eventually fails, and one is always forced to get outside one\u2019s comfort zone and learn some new math, since otherwise, a whole area of the field becomes alien to you.\n\nFortunately, the human brain is an amazing instrument, and provides decades and decades of trouble-free operation, allowing us to continually learn over 40,50, 60, years or more. How precisely it does that without zeroing out all prior learning is one of the greatest unsolved mysteries in science!",
                "As others mentioned before, it will mostly boil down to Statistics and Linear Algebra. However, I am very surprised how vague answers are.\n\nThere are lots of different techniques that you can specialise in so I\u2019ll try mentioning some places where you can start. Hopefully, it will make you realise, that there is a lot to cover and how you can start doing that.\n\nLinear Algebra\n\nTopics:\n\n * Vectors & Matrices\n * Matrix Multiplication & Transformations\n * Eigenvector analysis\n * Linear Equation Systems\nGood place to learn:\n\n * Precalculus | Khan Academy [ https://www.khanacademy.org/math/precalculus/ ]\n * Linear Algebra | Khan Academy [ https://www.khanacademy.org/math/linear-algebra ]\n * Linear Algebra [ https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/index.htm ]\n * http://cs229.stanford.edu/section/cs229-linalg.pdf\nCalculus\n\nBasically, you need to be able to do derivative and integration inside out.\n\nTopics:\n\n * Differentiation\n * Chain Rule\n * Partial Derivatives\n * Integrals\nGood places to learn:\n\n * AP Calculus AB | Khan Academy [ https://www.khanacademy.org/math/ap-calculus-ab ]\n * Multivariable Calculus | Khan Academy [ https://www.khanacademy.org/math/multivariable-calculus ]\nStatistics\n\nProbably, the most important thing is to have a pretty decent grasp on probability density and mass functions (PDFs and PMFs) for most of the common distributions (like Gaussian, Binomial, or Exponential family).\n\nIn the end, if you know statistics really well, you can make any machine learning problem into a statistics problem. I would even claim, that the current machine learning developments (apart from deep learning) are just rediscovery of statistics with the computation power of modern computers.\n\nAlso, I will skip things like expected value, variance, and etc. because that\u2019s basics.\n\nTopics:\n\n * Distributions and their CDFs and PDFs:\n * \n * Binomial\n * Poisson\n * Exponential family\n * \n * Gaussian distribution in particular\n\n\n * Calculating CDFs from PDFs\n * Bayes Rule\n * Naive Bayes\n * Markov Chains\n * Belief Networks (or Graphical Models in General)\nGood places to learn:\n\n * Statistics and probability [ https://www.khanacademy.org/math/statistics-probability ]\n * Probability | Statistics and probability | Math | Khan Academy [ https://www.khanacademy.org/math/statistics-probability/probability-library ]\n * Journey into information theory [ https://www.khanacademy.org/computing/computer-science/informationtheory ]\n * http://cs229.stanford.edu/section/cs229-prob.pdf\nOptimisation\n\nAnd finally optimisation. I would say it\u2019s just a part of Calculus (and it would look like that Khan agrees) but whatever. Here you want to be able to find the minimum or either the maximum of some function.\n\nTopics:\n\n * Gradient Descend\n * Stochastic (Online) Gradient Descend\n * Expectation Maximization\nGood places to learn:\n\n * Derivative applications [ https://www.khanacademy.org/math/calculus-home/derivative-applications-calc ]\n * Machine Learning - Stanford University | Coursera [ https://www.coursera.org/learn/machine-learning ] (note: it seems that Andrew Ng is in love with gradient descend so it is covered there pretty well)\nWhat\u2019s next?\n\nThere is a lot of material online for machine learning online but the problem with it is that most of it is very shallow and, honestly, it isn\u2019t worth your time.\n\nHowever, I would go with the following courses:\n\nMachine Learning - Stanford University | Coursera [ https://www.coursera.org/learn/machine-learning ] - A great introductory course to Machine Learning by Andrew Ng. It will cover some statistics and fair amount of linear algebra. Additionally, it exposes you to some decent optimisation basics.\n\nProbabilistic Graphical Models | Coursera [ https://www.coursera.org/specializations/probabilistic-graphical-models ] - Pretty tough course by Daphne Koller. You might also want to check out a book, that the course is based on Probabilistic Graphical Models [ http://pgm.stanford.edu/ ] . The course will give you some decent intro to Bayesian Probabilistic Models.\n\nNeural Networks for Machine Learning - University of Toronto | Coursera [ https://www.coursera.org/learn/neural-networks ] - Neural Nets by Jeff Hinton. If this course is anything as it was before, take it only if you are really interested in neural nets. It makes you do some proper maths, to realise how do those nets actually work. I wouldn\u2019t take it as a beginner.\n\nHowever, that course doesn\u2019t cover some newer development in DeepNets so you might want to see CS224d: Deep Learning for Natural Language Processing [ http://cs224d.stanford.edu/syllabus.html ] or CS231n Convolutional Neural Networks for Visual Recognition [ http://cs231n.github.io ] to cover things like RNNs and LSTMs.\n\nA quick peek\n\nWhat it would take you to solve these?\n\n1. Solve [math]Xw=b [/math] for w where X is data matrix and w is a weight vector and b is a target value (constant).\n2. Calculate Kullback-Leibler divergence between [math]p(x)=N(x|\\mu,\\sigma^2) [/math]and [math]q(x)=N(x|m,s^2)[/math].\n3. Prove [math]p(x,y|z) = p(x|z)p(y|x,z) [/math]\nGiven all of this, you should be good enough to know where to go next.\n\nGood Luck\n\nI am pretty sure I\u2019ve missed some important stuff but I am also sure that this will set you on the right track.\n\nGood luck!",
                "\ud83e\uddd1\ud83c\udffb\u200d\ud83e\uddb1For students or adults aiming to learn mathematics for machine learning, a foundational practice like mastering the abacus can be beneficial.\n\n * \ud83d\udcccThe abacus helps develop a strong numerical foundation and enhances mental calculation skills, which are crucial for understanding mathematical concepts in machine learning.\n * \ud83d\udcccAdditionally, complement your abacus practice with learning key mathematical principles such as algebra, calculus, and linear algebra, as they form the basis for many machine learning algorithms.\n * \ud83d\udccc Online courses, textbooks, and practical applications through coding exercises can further deepen your understanding of the mathematical aspects relevant to machine learning.\nTo ease the process, you can try online courses like:\n\n\n%3E  * Udacity: Intro to Machine Learning with PyTorch\n * Learnbay: Advanced Artificial Intelligence and ML Certification course\n * Great Learning: Python for Machine Learning\n\nWhy go for these courses? How do they help in mathematics?\ud83d\ude1f\n\n\ud83d\udc4cAdaptability: You do not have to travel, pay less fees compared to college courses, and learn from top faculty at your own pace. ML and AI technologies continue to evolve rapidly. Students who acquire skills in these areas are better positioned to adapt to technological changes and stay relevant in a dynamic job market.\n\n\ud83d\udc4cExcellent Job Opportunities: Students gain access to diverse career paths, including data science, machine learning engineering, AI research, and more. This diversity allows them to explore areas aligned with their interests and strengths. Plus, they land a good job in their nation or abroad.\n\nLet\u2019s explore how these courses will help you. Check these courses based on projects, certifications, domain, duration, and more.\n\n\ud83d\ude1fWhat are their Projects? How do they help in mathematics learning?\n\nWell, all these courses have unique projects that will enhance your deep thinking, calculations, and mathematical skills. You will find that Udacity\u2019s course real-world projects include learning how to use recurrent neural networks to learn from sequential data, building a network that can classify images of dogs & cats, and more.\n\n\ud83e\udd29Accreditation: A course completion certificate from Udacity\n\nGreat Learning course provides projects on Python analytical research, analytical modeling, statistical modeling, coding structure in Python, etc.\n\n\ud83e\udd29Accreditation: A course completion certificate from Great Learning\n\nThese certificates & accreditations help students prove their mastery in particular subjects and help them to grab good jobs.\n\nIf you look at Learnbay\u2019s course, it offers simulated real-time & capstone projects on career progression planning of employees with workforce defections and efficiency, forecasting future sales with trends and price, etc.\n\n\ud83e\udd29Accreditation: Two-course completion certificates from IBM & Microsoft\n\nMore: An IBM Capstone Project certification\n\n\ud83e\ude99Plus: It also offers you a unique feature of domain Electives where you can choose a specific domain like BSFI, healthcare, manufacturing, retail, & more.\n\nSum-up\n\nFinally, the best way to learn mathematics & determine which career assistance program is right for you is to research the specific services and resources that are offered by each provider. You should also consider your own career goals and needs when making your decision.\n\nBest Wishes!\u2b50\u2b50",
                "I am working as a Data Scientist myself therefore it makes me qualified enough to answer your question.\n\nAlso I will make sure to include the tricks in my answer that worked for me.\n\nSo Let's begin, Shall we?\n\nI will be answering this question, keeping in mind that a bunch of readers could be complete newbies into programming.\n\nSo addressing non-computer science students. Firstly, you need to work a lot on your problem-solving skills which is going to help you code effortlessly. You can achieve this by learning Data structures & Algorithms and coding in it. Also, DS & Algo are the building block of computer science so it will definitely help you on your Journey towards excellence in coding.\n\nAfter you are comfortable with problem-solving, you should stick to the below mentioned points:\n\n1. Opt for a good course on Machine learning and study it thoroughly to become well versed with all it\u2019s concepts.\n2. Practice machine learning problems on Kaggle: Your Machine Learning and Data Science Community [ https://www.kaggle.com/ ] which will help you gain confidence and give you enough hands-on skills.\n3. Post your projects on GitHub, LinkedIn and also you can use youtube to showcase your skills\n4. Now it\u2019s Time to market yourself. Make a clean and creative online portfolio and a strong resume based on ML. Start applying to your desired companies and surely circumstances will bend in your favour and soon you will become something you have worked so hard for and that is \u201cData scientist\u201d\n5. you can connect with me on LinkedIn\nPs: I am attaching my photo, in which you can see me working from home, just in case you are interested to know how a data scientist looks?! \ud83d\ude1b\n\n",
                "The list below probably isn't exhaustive, but contains the first things I thought of. So hopefully, they're the most important/fundamental ones!  (Although I'm sure I missed some.)\n\nStatistical Learning Theory:\n\nOverfitting [ https://en.wikipedia.org/wiki/Overfitting ] -- A central concept in machine learning is that of overfitting.  Roughly speaking, overfitting happens when you train a model that captures the idiosyncrasies of your training data.  A model that overfits to training data cannot generalize well to new unseen test examples, which is ultimately what we want most machine learning models to do.\n\nGeneralization error [ https://en.wikipedia.org/wiki/Generalization_error ] -- One way to quantify overfitting is through generalization error.  Roughly speaking, generalization error measure the gap between the error on the training set versus the test set.  Thus, the larger the generalization error, the more the model is overfitting.\n\nBias\u2013variance tradeoff [ https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff ] -- Sometimes, it's OK if the model you train overfits so long as the generalization error is not too large.  For example, if you train a complex model that achieves 0.2 error on the training set and 0.5 error on the test set, that might be desirable to a simple model that achieves 0.5 error on the training set and 0.6 error on the test set.  Even though the simple model overfits less, it was so simple that it still performs worse on the test set compared to a complex model that overfits more.  The bias-variance tradeoff is a way of reasoning about this issue: when does it make sense to use a more complex model even though it overfits more?\n\nEmpirical risk minimization [ https://en.wikipedia.org/wiki/Empirical_risk_minimization ] -- When most people think of machine learning, they're probably thinking of empirical risk minimization.  That is, they want a model that achieves low error on some training set.  However, it is important to keep in mind what the assumptions are of empirical risk minimization.  Most notably, that the training set is sampled independently from the test distribution you really care about.  If this assumption is violated, you can get machine learning models that don't behave in the way you want (cf. Algorithms and Bias: Q. and A. With Cynthia Dwork [ http://www.nytimes.com/2015/08/11/upshot/algorithms-and-bias-q-and-a-with-cynthia-dwork.html?_r=0 ]).\n\nCross-validation (statistics) [ https://en.wikipedia.org/wiki/Cross-validation_(statistics) ] -- Typically, one cannot test on data that one trained on, one must split the existing data into training and test sets. However, this is statistically wasteful and also increases the variability since you aren't testing on every data point at your disposal.  Cross validation is a way of getting around that by rotating what's in the training vs test sets. \n\nConfidence interval [ https://en.wikipedia.org/wiki/Confidence_interval ] -- The most direct quantitative way to compare two models is by looking at their respective test errors (e.g., via cross validation).  However, how do we know if two numbers actually reflect meaningful differences between the two models or are just due to some spurious effects caused by a finite sample size?  Confidence intervals are the most common way to deal with this issue.\n\nStatistical hypothesis testing [ https://en.wikipedia.org/wiki/Statistical_hypothesis_testing- ] -- A related concept to confidence intervals is statistical hypothesis testing.  The most common thing to use this for is answer whether two models have statistically distinguishable accuracies.  The way statistical hypothesis testing is typically implemented involves using confidence intervals and setting the size of the confidence intervals at an appropriate width w.r.t. the desired statistical significance level.\n\nBootstrapping (statistics) [ https://en.wikipedia.org/wiki/Bootstrapping_(statistics) ] -- Another way of evaluating the variability of the model is via bootstrapping, which effectively samples from the training set with replacement to generate new training sets that are statistically similar to the original training set.\n\n\nStatistical Modeling:\n\nMetrics | Kaggle [ https://www.kaggle.com/wiki/Metrics ] -- It's important to understand what your metric of choice is for whatever modeling problem you're solving.  For some tasks, you only care that your model can make good predictions at the top (e.g., ranking in web search), so a metric like precision@10 is appropriate there.\n\nRegularization (mathematics) [ https://en.wikipedia.org/wiki/Regularization_(mathematics)#Regularization_in_statistics_and_machine_learning ] --  Regularization serves two purposes.  First, it is commonly used to control for overfitting so that the learned model is not too complex.  Second, different choices of regularization reflect different assumptions about what \"simple\" means.  For instance, using L1 regularization encourages sparsity in the trained model, and interprets simple as having few non-zero parameters.  On the other hand, using L2 regularization encourages the norm of the learned model to be low, and interprets simple as having a small magnitude.\n\nMachine learning Types and Tasks [ https://en.wikipedia.org/wiki/Machine_learning#Types_of_problems_and_tasks ] --  People often think of supervised learning when they think machine learning (and in fact most of the topics listed here are described through the lens of supervised learning).  But supervised learning is not the only learning setup.  Others include unsupervised learning, semi-supervised learning, transductive learning, etc.  It's important to understand what kind of learning problem you're dealing with.  If you would rather deal with the supervised learning problem, that often means throwing away data for which you do not have labels for.  Sometimes that's good, and sometimes that's not so good.\n\nCorrelation vs. causation [ https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation ] -- It's important to keep in mind when inspecting a learned model that many things learned by the model are purely correlation and should not be interpreted causally. \n\n\nOptimization:\n\nStochastic gradient descent [ https://en.wikipedia.org/wiki/Stochastic_gradient_descent ] -- Most machine learning models are trained via some form of stochastic gradient descent.  It's generally useful to understand when different methods work well, so you can train your models more efficiently.\n\nNesterov\u2019s Accelerated Gradient Descent [ https://blogs.princeton.edu/imabandit/2013/04/01/acceleratedgradientdescent/ ] -- One important concept in gradient descent is momentum, of which Nesterov's method is arguably the most beautiful instance.  Momentum is typically extremely useful for speeding up training.\n \nConvex analysis [ https://en.wikipedia.org/wiki/Convex_analysis ] --  It's important to understand when your learning problem is convex versus non-convex.  Convex learning problems always converge to the same optimal model, so you don't have to be too careful about how you train (apart for speed considerations).  Non-convex learning problems can get stuck in local optima, and so the model you get back can vary greatly.  As such, it's often important to be careful about how you initialize the non-convex learning problem.\n\n\nLinear Algebra:\n\nNorm (mathematics) [ https://en.wikipedia.org/wiki/Norm_(mathematics) ] -- Norms are used a lot in machine learning.  For instance, many regularization formulations are written as norms.  Understanding the behavior of different norms will help you in deciding which kind of regularization you want to impose.\n\nMatrix_(mathematics) [ https://en.wikipedia.org/wiki/Matrix_(mathematics) ] -- A lot of times, data and models are expressed using matrices.  Sometimes, you can save a lot of computation time by being clever about how you order the matrix operations.  Other times, you can figure out how to transform your data into the format that some learning toolkit uses by using matrix transforms.  \n\nThe Statistical Whitening Transform [ https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/ ] --  One particularly useful approach for standardizing your data is whitening.  It's good to understand what assumptions are built into whitening, so that you'll have a good sense of when whitening will and won't work.\n\n\nOutlook:\nThese days, there are a lot of tools being developed that can automate away a lot of the issues described above, and will thus make machine learning more intuitive & easier to use for more people.  For example, many machine learning packages already do cross validation automatically.  However, those tools are far from perfect, and so having a solid grasp of the theoretical fundamentals will be very beneficial in the long run, because it'll allow you to more intelligently use and compose the existing tools to achieve whatever data modeling task you're trying to solve.",
                "The label of being the \u201cbest career\u201d that is attributed to Machine Learning is definitely undeserved. It all stems from one word; hype.\n\n\u201cWHAT!\u201d scream the millions of deep learning enthusiasts and ML engineers waiting in the wings.\n\nLet me explain before pitchforks are raised.\n\n\nThere is no doubt Machine Learning is an interesting field to be in. It\u2019s not only intellectually stimulating, its results appear near magic to the general public. Machines beating the world\u2019s best at Go, self driving cars, Snapchats face filters. It\u2019s sexy to be a Machine Learning engineer, to be involved in the very real wave of new technologies.\n\n\nBut there comes a point where the public greatly over values a profession. News articles such as the New York Times\u2019 listing salaries of 300 - 500k for AI professionals [1] lead young graduates to believe Machine Learning is the best way to make big money. The problem begins when people start to pursue a career in Machine Learning because it\u2019s the supposed \u201cbest career\u201d. Combine this with the fact the field is so vaguely defined, encompassing everything from Data Engineers to statisticians to quantitative analysts with Python and you have a misinformed public and a ballooned valuation.\n\n\nSince the field is so young, there are massive shortages in graduates educated with the relevant skills. As a result, salaries have of course rocketed. However, just a few years behind, the Universities of the world are starting more and more \u201cData Science\u201d and \u201cMachine Learning\u201d degree programmes. Surely, this is the answer to the job shortage problem? Not quite. Because the field\u2019s value is so distorted universities across the globe are starting sub par programmes which receive buckets of applications from (genuinely interested) students. These programmes make nice money for the university but only make the problem worse. Thousands of graduates now qualified as \u201cMachine Learning experts\u201d are under equipped for the task necessary, further clouding the definition of Machine Learning field. You can bet that in the years to come, when ML has stabilised, so too will the salaries and only those who are truly interested in the field will find their jobs fulfilling.\n\n\nThe real message I want to communicate in this answer is not that Machine Learning is overly hyped. My main message is that there is no such thing as \u201cthe best career\u201d. There is only \u201cthe best career for you\u201d. The most important thing to remember (just like for any job, however publicly valuable) is to honestly assess whether you have a genuine interest in the subject. Do you love playing with data and ML algorithms? Do you find the underlying math beautiful? Machine Learning is a fascinating field and, if it's right for you, it's rewarding, fulfilling and you can make some decent money doing it. But it's important to isolate the publics opinions from your own.\n\n\n1. Tech Giants Are Paying Huge Salaries for Scarce A.I. Talent [ https://www.nytimes.com/2017/10/22/technology/artificial-intelligence-experts-salaries.html ]",
                "Are you serious?\n\nHere are a few reasons I love my job.\n\n1. Make more than most doctors.\n2. Work remotely 100% of the time.\n3. Work with really smart people.\n4. Not a lot of human interaction. Very few meetings.\n5. A total package like C level execs. (Bonus, sick leave, paid time off)\n6. For the most part, I make my own hours. I\u2019m not a morning person so I block off my calendar so other\u2019s can\u2019t set up meetings at 8am. I\u2019m still sleeping then and don\u2019t want to be interrupted.\n7. Tons of jobs. If I don\u2019t like my job for any reason, I move on.\n8. A career for life. At 50, I\u2019ll never not have job in this space. Ever.\n9. Highly respected. This career is one of the most technical and respected in all of IT.\nThose are a few reasons that make machine learning a great career.\n\nNow, don\u2019t kid yourself. I\u2019ve been working with data and BI for 20 years. You won\u2019t be in a MLE role in a few months and not likely in a few years but I wouldn\u2019t change a thing.\n\nReady to learn real-world ML? Here you go.\n\nhttps://logikbot.quora.com/\n",
                "Great answers here already:\n\nThe foundation of machine learning (ML) is maths and not data science. So start by polishing up your maths skills. ML currently is a very hot area with many more people trying to learn it but most don't understand that the underlying principles in machine learning are that of optimization theory in maths.\n\nThus to give you a boost revisit the prerequisites.\n\n1. Maths:\n2. \n1. Linear algebra: Make sure you are comfortable with matrices, vectors and singular value decomposition (SVD).\n2. Calculus: Especially differential calculus, become comfortable with evaluation of derivatives of any function and learn chain rule.\n3. Numerical optimization: Like I said above ML is currently more related to numerical optimization than anything else. Most optimization methods are variants of gradient descent such as stochastic gradient descent (SGD). So learn about first order and second order optimization methods.\n4. Statistics and probability: Bayes theorem, random variables, probability distribution functions such as the Binomial and Gaussian distributions. Sigmoid and softmax functions are motivated by probability.\n\n3. Programming: In any of the following:\n4. \n1. \n1. Python: Is an easy to learn scripted language that you can quickly pick up for the purpose of practicing what you are learning. Python has a lot of ML libraries supporting it thus it is ideal not only for beginners but also for experts.\n2. Java: is a fairly high-level language that is fairly easy to learn but not as easy as Python. It also has a lot of ML libraries supporting it.\n3. C/C++: Don't mess with this one at beginner level but start getting used to writing ML code from scratch as you advance so that you can learn more details about most ML algorithms. It is recommended to build your own mini-ML library at some point in your learning journey using such low-level high-performance languages.\n\n\nWith that said, there are several roadmaps to reaching your destination. I can split them up into three stages.\n\n1. Beginner stage.\n2. Intermediary stage.\n3. Advanced (expert) stage.\nBeginner stage:\n\nThe beginning part has been partially covered above, begin with maths. And if you are not good at maths you will need to make sure you are good. Thus make sure to sharpen the axe before starting to chop down the tree. Maths is very important for learning ML as most of the times, systems are expressed in mathematical terms. So before starting, please go through the basics. Don't worry about forgetting something along the way, you can always go back to revisit the stuff you have forgotten, this is not an exam.\n\nThis is where you also have to set yourself up for success in ML, so I suggest you skim through the basic concepts of ML and the book [1] by Ian Goodfellow and others is an excellent book to introduce you to ML and the current hot area in ML called deep learning (DL). Go through the book in any order you like but make sure to read the introduction first.\n\nAt beginner level is also the time to sharpen up another axe, programming. Practice coding on problems that are not even related to ML so that you can learn the syntax of that language. You can't learn coding by reading but through practice. Python being English-like is very easy to pick up and there are a lot of resources out there teaching Python coding. Just Google and you will find high quality tutorials on Python programming. In fact your best way to learn coding is to just jump directly to coding and just Google and Stackoverflow your way through learning the syntax of the language. Every modern programmer owes their project completions to Google and Stackoverflow, let no one lie to you, we are in an era of powerful \u201ccheating\u201d tools like Google search. Don't just copy and paste though, understand the solutions you find online and code your own versions afterwards. If you can't code a matrix operation in Python just Google and try to understand what others did. Sometimes very basic code can be just copied and pasted.\n\nAlso go through the TensorFlow (TF) tutorials as well. They will not only introduce you to TF but also to the concepts of ML such as linear and logistic regression.\n\nIntermediary stage:\n\nAt this stage you should have already read several online sources such as articles, books and watched several YouTube videos on ML and you should have coded up some models at high-level using libraries like TF. Now it is time to start moving forward, you need to make sure that that knowledge does not slip away, you need to consolidate that knowledge in your mind.\n\nThis is where things start to get interesting because you need to now ask yourself, can I implement backpropagation algorithm from scratch? You will notice that you probably wouldn't, that's okay. You need at this point to start pursuing proofs like deriving mathematical expressions you find while reading ML literature. Try to derive backprop for simple neural networks yourself from your own perspective and understanding and see if you can translate that maths to actual working code. Try to also implement convolutional neural networks (CNN) and recurrent neural networks (RNN) from scratch.\n\nThis is where you start to build your mini-ML library. You will be able to learn a great deal of detail about ML this way. Implement stochastic gradient descent and train your own ML models using your own mini-ML library and debug until the models work comparable to those from mature libraries like TF. Open source the mini-ML library afterwards to further show off your skills to potential employers or for the purpose of getting into the Google Brain residency program for example.\n\nYou also need to start reading journal after journal at this point. At first it will be hard but with more reading and rereading you will start to understand even complex journals from the likes of DeepMind, OpenAI, Microsoft, Facebook and Google. If you start to understand journals it means you are advancing well towards your goal.\n\nAdvanced stage:\n\nYes coding your own mini-ML library is sort of like reinventing the wheel but it is essential for learning the details of the most important underlying concepts in machine learning but not sufficient to make you an expert. This advanced stage comes after spending a few years in the intermediary stage.\n\nIn the advanced stage you need to start paying attention to your own intuitions and ideas. Build or start working on actual novel ML algorithms. You will need to empirically or theoretically validate your ideas by implementing them and doing lots of experiments. It is somewhat hard to design a novel ML algorithm as the ideas come, in form of a eureka moment, after years or months of thinking and lots of research work.\n\nThis is why being at an advanced level requires that you have built up a strong mental model of the field of ML from a variety of angles. You don't have to be an expert programmer though, as the coding skills only need to help you implement your models.\n\nAt this stage start asking and pursuing deep questions to which there are no answers yet. Focus on areas that are counterintuitive and try to come up with more novel intuitive solutions to those areas. It is like a PhD program.\n\n\nThat is a possible roadmap for someone trying to learn ML. It is also important to practice by explaining complex ML systems to someone else. Quora is a great place for answering ML related questions, that way you will be able to consolidate your knowledge when explaining the ML algorithms to others, it's a win-win situation, you help others while you gain and consolidate knowledge yourself.\n\nThe other thing worth noting is that you really need to be passionate about the ML field otherwise it won\u2019t be easy. You also need to have tenacity because some concepts take long to understand fully, you may think that you get a concept up until it's time to implement it yourself. Thus learning ML especially by yourself requires serious discipline and focus. You can only maintain that focus if you are passionate and determined to learn ML.\n\nWith that said the journey itself is quite fun, fulfilling and challenging, not hard but challenging just keep going and read anything that interests you concerning ML.\n\nBe passionately curious, you will get there.\n\nHope this helps.\n\n\n1. Deep Learning [ http://www.deeplearningbook.org ]",
                "Hmm\u2026this reminds me of a joke that was on a coaster in my house, as I was growing up many decades ago in India. The coaster read:\n\n\u201cWho says nothing is impossible? I\u2019ve been doing nothing for years!\u201d\n\nWell, I\u2019ve been doing ML for almost 40 years (wrote my first ML program waaay back in 1982 on a Digital Equipment Corporation (DEC) 10 mainframe computer using Common LISP, which back then was the Python of AI, and everyone in AI \u2014 a small bunch of diehards then \u2014 coded in LISP).\n\nGetting to your question, I\u2019m reminded of an anecdote from Michael Jordan (no, not the basketball player!), a well known ML professor at nearby U.C. Berkeley, who said the best definition of the ML field was that it is the activity of the peculiar constellation of folks who attend the NeurIPS conference. I suppose in that sense, yes, ML is certainly a \u201cmade-up profession\u201d. It has neuroscientists, biologists, statisticians, computer scientists, mathematicians, philosophers, physicists, psychologists etc. It\u2019s a complete hodgepodge, and I love that about ML. It has never been a boring field in the past 40 odd years that I\u2019ve been at it. Whenever you get bored, you can always switch to some other topic. My work over the past decades has ranged from very theoretical math-oriented research, collaborations with biologists, psychologists and planetary geochemists, ruminations on philosophy, and everything in between.\n\nI think Geoff Hinton, the \u201cfather\u201d of deep learning, made a revealing statement to me in a personal conversation many years ago (I knew him since the mid-1980s, when he taught me neural networks in a class at CMU). He said, and I\u2019m paraphrasing a bit: \u201cI started in physics and failed in that field, did a bit of psychology and dropped out of that field, and finally ended up in a field with no standards!\u201d (deep learning!). It worked great for him! He was the winner of the Turing award, computer science\u2019s equivalent of the Nobel prize! He\u2019s truly deserving of that, of course, having spent his entire career on trying to develop computational neuroscience-inspired models.\n\nThere\u2019s no scientific unity in ML at all that I can discern, and I should know, have written papers in the field for going on 35 years (first two papers published in IJCAI 1985, and the most recent paper in ICML 2020!). Some people adhere to the view that ML is statistics in high dimensions. Some people view ML as the engineering of neuroscience (e.g. deep learning). Some people view ML as computational psychology (e.g., reinforcement learning). Others view it as building embodied AI agents (e.g., roboticists interested in ML). There\u2019s absolutely no shared sense of a common problem formulation, or theoretical framework. That\u2019s both a great source of strength \u2014 it\u2019s a blending of many points of view \u2014 but also a fundamental weakness.\n\nMy wife is a statistician, and we\u2019ve been happily married for just as long as I\u2019ve been writing papers in ML! She calls ML the \u201cgosh and golly\u201d profession. What she means is that the vast majority of ML papers are about some \u201cdemo\u201d, which essentially says, wow, we trained this deep neural net, and look what it produced (generating face images, NLP chat bot, Atari game player etc.). Take any statistical journal of repute (e.g. JASA, Biometrika, Journal of the Royal Statistical Society etc.). I guarantee you that you will not find an article that says, look, wow, we trained a deep neural net that plays Atari video games! That\u2019s not what a true scientific field like statistics works. Statistics is the one true data science, because the questions it asked and successfully answered are truly as deep as Newton\u2019s laws.\n\nTake the simple question of how to abstract data without losing any information. In statistics, this is formalized as the concept of \u201csufficient statistics\u201d. A sufficient statistic is a function T(X) of some dataset X that retains all the information in X about a class of models M(\\theta), such that P(M(\\theta) | X, T(X)) = P(M(\\theta) | T(X)).\n\nWhat this says in English is that the distribution of models that explain the dataset X given the sufficient statistic T(X) is conditionally independent of the dataset X. It tells you how to abstract data so you don\u2019t need to store it anymore. There\u2019s a vast amount of theory on sufficient statistics that you learn in graduate courses on statistics. For example, the famous Rao-Blackwell theorem states that any (unbiased) estimator of a model that is conditioned on a sufficient statistic T(X) has variance that is lower than any other estimator, unless that other estimator is itself a function of a sufficient statistic. So, this is how statistics works. You formalize the problem using ideas like sufficient statistics, and then you work out the ramifications of the concepts using very rigorous theory. Virtually all the major ideas in statistics, including design of experiments, randomized trials, sampling etc. has a rigorous body of theory, which has deep measure-theoretic foundations.\n\nWith a small number of exceptions, virtually none of the modern work in ML that has achieved a lot of popular press has any deep theory behind it (e.g., deep learning, deep RL, deep NLP models, GAN models etc.). Like my wife says, it\u2019s all \u201cgosh and golly\u201d.\n\nYears ago, Richard Feynman, the well-known physicist who taught for many years at Caltech, gave a commencement ceremony at that august institution where he tried to explain the reasons physics is a science, but in his view, psychology is not. He called fields like psychology \u201ccargo cult sciences\u201d.\n\nCargo Cult Science [ https://calteches.library.caltech.edu/51/2/CargoCult.htm ]\n\nFeynman related an experiment done in psychology, one of a very small number in his view, where they tried to understand how rats were able to run a maze and do some task involving finding food. Instead of simply reporting on the results, which is what the vast majority of such experiments usually did, like ML does today, the experimenters tried every single way to fool the rats, and then finally discovered the one condition that made the rats not be able to succeed. When they changed that, the rats failed! To Feynman, this is true science. It\u2019s not a demo, or what the famous AI pioneer John McCarthy, who set up the Stanford AI Lab, called the \u201cLook, ma, no hands!\u201d syndrome. For Feynman, science is the process of trying every which way in which to disprove your theory, to find faults with your approach.\n\nVirtually no paper in NeurIPS today says, look, we tried every which way to make our methods fail, and here\u2019s the ways in which our method fails! No, they all say, look how great our technique is, it works so well on all these (carefully assembled and hand-tuned) datasets with these parameters.\n\nBut, I have great hopes that even with all its faults, the \u201cmade-up profession\u201d of ML will finally find its savior, someone who will finally put our field on more solid scientific grounds. To see why, all we need to do is understand how the field of statistics became a true science. Ronald Fisher, the \u201cfather\u201d of modern statistics, was hired to work at a provincial agricultural station.\n\nSir Ronald Aylmer Fisher | British geneticist and statistician [ https://www.britannica.com/biography/Ronald-Aylmer-Fisher ]\n\nAt the agricultural station, Fisher was shown a huge room where they had the early 20th century version of a Kaggle dataset, a massive collection of leather bound volumes of agricultural data that the station had patiently collected on soil samples, fertilizer, crop yields and so on, but had no idea how to extract insights from this massive 90 year dataset. Fisher, to his great credit, not only showed how to get insights from this data, he invented the major techniques, from likelihood analysis to randomized trials, that are the bedrock of almost every modern empirical science today, a hundred years later!\n\nFisher faced great challenges in doing his work, not the least of which was that he had no vast AWS or Azure or Google cloud service, all he had was a hand calculator that he affectionately called \u201cThe Millionaire\u201d, because that was the largest number it could store! Doing any calculation with the Millionaire involved a lot of cranking of its ponderous mechanisms. Fisher was also incredibly near sighted, and suffered from very poor vision since childhood (a disability that he later prized, as he said he was forced to conceptualize and work out problems in his mind from a very young age).\n\nFisher wrote a series of now legendary papers that changed the field of statistics, and ushered in modern empirical science, for Fisher\u2019s techniques are used everywhere, from large-scale astronomy, such as the investigation of ripples in the big bang afterglow of residual energy, which earned Berkeley physicist Smoot, the Nobel prize, to the latest vaccines that have been designed for the COVID-19 pandemic.\n\nFisher faced the additional hurdle that the most prestigious journal in the field then was Biometrika, but its editor-in-chief, the rival statistician Pearson, hated Fisher, and personally saw to it that few if any of Fisher\u2019s great papers were published in Biometrika. Fisher was forced to publish his work in then more obscure journals. None of this mattered. Ultimately, Fisher\u2019s insights became the stuff of legend, and the world is a far better place a hundred years later because of Fisher and his laborious hand-calculations with the Millionaire. Each of Fisher\u2019s many early papers had dozens of numerical tables, each one of which involved hundreds of hours of manual hand cranking with the Millionaire. Despite all these handicaps, Fisher succeeded, changing our world for the better.\n\nSo, in a sense, the science of statistics started with a Kaggle like dataset, and it took a genius like Fisher to show how to turn datasets into a true science. There\u2019s hope for ML yet! We may see a true \u201cML Fisher\u201d emerge one day, and perhaps he or she would develop their ideas from looking at Kaggle datasets! I can\u2019t think of a better New Year\u2019s wish, as we close out this truly horrible 2020, and look forward to a brighter 2021.\n\nHappy new year, everyone! May your dreams be realized in the coming year(s)!",
                "Hi there,\n\nFirst of all welcome to the world of data science. I would share my experience when I was at your stage. First will talk about the mistakes and then, how to fix them.\n\na. If you are at a stage where your just completed few tutorials in R programming, or data science courses in in Udemy or Coursera, stop before you feel like let's participate in Kaggle or other such competitions. Mistake 1. Kaggle competition in most cases doesn't teach you data science. Yes it does tell \"it's home for data science, but it's not a good host\", just kidding.\n\nb. Do not tell yourself that if you know how to apply logistic regression, or PCA or K-means clustering et.al [ http://et.al ]. machine learning (ML) algorithms, you will become Data Scientist. No. Stop. Mistake 2.\n\nC. Not trying to ask questions(high level, theoretical and oriented towards business problem and how doing data science would bring answers to those questions. If you are not starting from this way, guess what, you are doing mistake 3. and we can go on with this list.\n\nNow, let's fix them.\n\na. Yes, you definitely need a tool to experiment, you can pick R which is fine. Others pick Python or Clojure or Weka etc, and some still use Java and Javascript. It depends. So, make sure you are comfortable with R functions such as lapply, tapply, mapply, sapply, as you are going to need them daily. You also practice how to read data from thousand of files sequentially, or from SQL database or from CSVs and try some NoSQL databases. This is totally \"not data science\". This is just an exercise to use R (any tool you pick) tool for data ingestion, processing and make them ready for your analysis. I initially started with Python, and after two years too R and just in a month I fell in love with R programming. I now prefer to use R.\n\nb. Start reading a book which teaches you Statistics using R (again pick any tool you like). Why, because even before you get to apply those fancy ML algos, you will need to understand what your data represents, how are they distributed, how do they relate to each other, do they have missing values, how do you find or evaluate whether those variables are of any use to you, should you remove them or if you remove them are you going to lose any information, etc. You will get even more questions. So statistics. Very important. Still \"no data science.\"\n\nC. Once you have basic understanding of R(or any tool) and Statistics, pick the most simple machine learning algorithm i.e. \u201cLinear Regression\u201d. Read it first from statistical point of view, and then how to apply in R and why and when would you use Linear Regression. This will give your first glimpse of how to apply a ML algo to a data set. Then add one more algorithm, may be logistic regression, and so on. Do not rush, as you would need to make sure you very well know what are the strong and weak points of your algorithm. This is still \"not data science\"\n\nD. Once you have reached this stage. Pick a case study. Now this could be scary, but its ok. Here you immediately notice mix of statistics, ML algos, programming and function been built to several things. Just stick to it, and cover one topic or section at a time.\n\nYou will find tons of case studies online. I would suggest pick a case study on Fraud detection or recommendation systems. This is an area, which every industry has a problem with in this digital world.\n\nNow comes, data science. Say you work for an eCommerce company and your colleague or managers asks you, can you detect fraudulent transactions from the entire transaction history and would you be able to predict it, given that historical transactions are already reviewed by \"domain experts\" and termed them either fraud transaction or non fraud. Well in real world, not all data would be clearly termed. Now, given such a scenario, you would need to do data processing first format the data in such a way that you ML algo understand, in the processing stage you do not want to lose any useful information so you have put extra measure, manual analysis and data analysis utilizing your statistics knowledge, work experience, domain experience etc. Hence you would spend 80% of your time cleaning the data and preparing them. Rest 20% would be to apply ML and deploying the solution.\n\nAfter all of these, if you can answer the question above based on the data, which could save your company/clients/customers millions of dollars from the fraudulent transactions or getting new clients based on the products you have recommended, that's when you have applied all your knowledge and expertise to do data science.\n\nI thought this might help you see what to expect from this path. Hope this answers your question.",
                "Machine Learning is an excellent choice for a good career. Top positions in Machine Learning provides great salaries and make it an excellent field for a lifelong career.\n\nThe best Machine Learning Institutes in India are as follows:\n\n * Analytixlabs\n * Imarticus\n * Great Learning\n * ExcelR\n * Simplilearn\n * TalentSprint\n * EDX\n * Upgrad\n * BITS Pilani\n * Edureka\n",
                "The role of a machine learning engineer is actually much better defined than that of a data scientist.\n\nWhy? Because the companies that use that job title are the ones that have a very clear idea about how and why they want to utilize machine learning. In addition, these companies almost invariously have data scientists as well, so they have defined the distinction between the two.\n\nData scientist is a job title that\u2019s often poorly defined. It\u2019s usually an analyst that knows some programming and machine learning. A machine learning engineer is a full-blown software engineer that has specialized in machine learning.\n\nThe most important responsibilities of a machine learning engineer are roughly these:\n\n * Running machine learning experiments using a programming language with machine learning libraries.\n * Deploying machine learning solutions into production.\n * Optimizing solutions for performance and scalability.\n * Data engineering, i.e. ensuring a good data flow between database and backend systems.\n * Implementing custom machine learning code.\n * Data science, i.e. analyzing data and coming up with use cases.\n",
                "Finally, the hard-hitting question that everyone has been waiting for someone to ask.\n\nYes, the ML profession is completely made up\u2026of people who develop and implement models and algorithms that allow computers to learn and extract patterns from data.\n\nI\u2019m glad that\u2019s finally settled. I was beginning to wonder where that paycheck was coming from every 2 weeks.",
                "The label of being the \u201cbest career\u201d that is attributed to Machine Learning is definitely undeserved. It all stems from one word; hype.\n\n\u201cWHAT!\u201d scream the millions of deep learning enthusiasts and ML engineers waiting in the wings.\n\nLet me explain before pitchforks are raised.\n\n\nThere is no doubt Machine Learning is an interesting field to be in. It\u2019s not only intellectually stimulating, its results appear near magic to the general public. Machines beating the world\u2019s best at Go, self driving cars, Snapchats face filters. It\u2019s sexy to be a Machine Learning engineer, to be involved in the very real wave of new technologies.\n\n\nBut there comes a point where the public greatly over values a profession. News articles such as the New York Times\u2019 listing salaries of 300 - 500k for AI professionals [1] lead young graduates to believe Machine Learning is the best way to make big money. The problem begins when people start to pursue a career in Machine Learning because it\u2019s the supposed \u201cbest career\u201d. Combine this with the fact the field is so vaguely defined, encompassing everything from Data Engineers to statisticians to quantitative analysts with Python and you have a misinformed public and a ballooned valuation.\n\n\nSince the field is so young, there are massive shortages in graduates educated with the relevant skills. As a result, salaries have of course rocketed. However, just a few years behind, the Universities of the world are starting more and more \u201cData Science\u201d and \u201cMachine Learning\u201d degree programmes. Surely, this is the answer to the job shortage problem? Not quite. Because the field\u2019s value is so distorted universities across the globe are starting sub par programmes which receive buckets of applications from (genuinely interested) students. These programmes make nice money for the university but only make the problem worse. Thousands of graduates now qualified as \u201cMachine Learning experts\u201d are under equipped for the task necessary, further clouding the definition of Machine Learning field. You can bet that in the years to come, when ML has stabilised, so too will the salaries and only those who are truly interested in the field will find their jobs fulfilling.\n\n\nThe real message I want to communicate in this answer is not that Machine Learning is overly hyped. My main message is that there is no such thing as \u201cthe best career\u201d. There is only \u201cthe best career for you\u201d. The most important thing to remember (just like for any job, however publicly valuable) is to honestly assess whether you have a genuine interest in the subject. Do you love playing with data and ML algorithms? Do you find the underlying math beautiful? Machine Learning is a fascinating field and, if it's right for you, it's rewarding, fulfilling and you can make some decent money doing it. But it's important to isolate the publics opinions from your own.\n\n\n1. Tech Giants Are Paying Huge Salaries for Scarce A.I. Talent [ https://www.nytimes.com/2017/10/22/technology/artificial-intelligence-experts-salaries.html ]",
                "The role of a machine learning engineer is actually much better defined than that of a data scientist.\n\nWhy? Because the companies that use that job title are the ones that have a very clear idea about how and why they want to utilize machine learning. In addition, these companies almost invariously have data scientists as well, so they have defined the distinction between the two.\n\nData scientist is a job title that\u2019s often poorly defined. It\u2019s usually an analyst that knows some programming and machine learning. A machine learning engineer is a full-blown software engineer that has specialized in machine learning.\n\nThe most important responsibilities of a machine learning engineer are roughly these:\n\n * Running machine learning experiments using a programming language with machine learning libraries.\n * Deploying machine learning solutions into production.\n * Optimizing solutions for performance and scalability.\n * Data engineering, i.e. ensuring a good data flow between database and backend systems.\n * Implementing custom machine learning code.\n * Data science, i.e. analyzing data and coming up with use cases.\n",
                "Basically, it\u2019s:\n\n * Find solutions to complex problems, often using machine learning but not always - if there is a solution that is faster, better, simpler etc.\n * Find how to exploit dormant unused datasets to make value for the company.\n * Improve the models build during previous work on a project.\n * Request data that could be valuable for the company based on what you or other teams might need.\n * Be a geek about artificial intelligence\n * Transform Machine Learning prototypes into working products, ready to be scaled.\n * Much more!\n",
                "Are you serious?\n\nHere are a few reasons I love my job.\n\n1. Make more than most doctors.\n2. Work remotely 100% of the time.\n3. Work with really smart people.\n4. Not a lot of human interaction. Very few meetings.\n5. A total package like C level execs. (Bonus, sick leave, paid time off)\n6. For the most part, I make my own hours. I\u2019m not a morning person so I block off my calendar so other\u2019s can\u2019t set up meetings at 8am. I\u2019m still sleeping then and don\u2019t want to be interrupted.\n7. Tons of jobs. If I don\u2019t like my job for any reason, I move on.\n8. A career for life. At 50, I\u2019ll never not have job in this space. Ever.\n9. Highly respected. This career is one of the most technical and respected in all of IT.\nThose are a few reasons that make machine learning a great career.\n\nNow, don\u2019t kid yourself. I\u2019ve been working with data and BI for 20 years. You won\u2019t be in a MLE role in a few months and not likely in a few years but I wouldn\u2019t change a thing.\n\nReady to learn real-world ML? Here you go.\n\nhttps://logikbot.quora.com/\n",
                "Basically, it\u2019s:\n\n * Find solutions to complex problems, often using machine learning but not always - if there is a solution that is faster, better, simpler etc.\n * Find how to exploit dormant unused datasets to make value for the company.\n * Improve the models build during previous work on a project.\n * Request data that could be valuable for the company based on what you or other teams might need.\n * Be a geek about artificial intelligence\n * Transform Machine Learning prototypes into working products, ready to be scaled.\n * Much more!\n",
                "The role of a machine learning engineer is actually much better defined than that of a data scientist.\n\nWhy? Because the companies that use that job title are the ones that have a very clear idea about how and why they want to utilize machine learning. In addition, these companies almost invariously have data scientists as well, so they have defined the distinction between the two.\n\nData scientist is a job title that\u2019s often poorly defined. It\u2019s usually an analyst that knows some programming and machine learning. A machine learning engineer is a full-blown software engineer that has specialized in machine learning.\n\nThe most important responsibilities of a machine learning engineer are roughly these:\n\n * Running machine learning experiments using a programming language with machine learning libraries.\n * Deploying machine learning solutions into production.\n * Optimizing solutions for performance and scalability.\n * Data engineering, i.e. ensuring a good data flow between database and backend systems.\n * Implementing custom machine learning code.\n * Data science, i.e. analyzing data and coming up with use cases.\n",
                "Hi, ML engineer here.\n\nCurrently working on NLP, for the most part, including intent classification and entity extraction.\n\nHere\u2019s a typical day for me:\n\nGet to work, pull up GitHub and check on the ZenHub board (kind of like Jira, except way cooler). I had some models that were training last night on our servers and I should have gotten an email that they finished. I did!\n\nI\u2019ll probably spend a few minutes testing those new models and then tweak some parameters, then restart the training process.\n\nThe rest of the day I\u2019m usually head-down coding, either working on a back-end Python application that will supply the AI for one of our products, or implementing a new algorithm that I want to try out.\n\nFor example, recently I read a paper on coupled simulated annealing (CSA), [ http://ftp://ftp.esat.kuleuven.be/sista/sdesouza/papers/CSA2009accepted.pdf ] and I wanted to try it out on tuning the parameters for XGBoost as an alternative to a grid search. CSA is a generalized form of simulated annealing (SA), which is an algorithm for optimizing a function that doesn\u2019t use any information on the derivative of the function.\n\nUnfortunately I couldn\u2019t find an implementation in Python, so I decided to write my own. Two days later, I had submitted my first package to PyPI!\n\nAnyways\u2026 I\u2019ve actually never met another ML engineer, so I have no idea if my daily experience is typical ;-) Did I miss anything? Bagel Fridays are awesome because I\u2019m the only who likes everything bagels. Must be a New York thing? (Currently in the middle of Iowa, not to be confused with Idaho. We have CORN, not the potatoes).",
                "The pic below is the machine learning pipeline.\n\nEverything I do is within the confines of this pic.\n\nNow, most of my time is spent on this step.\n\nSurveys and studies have shown that most of the time of a MLE is spent sourcing, wrangling and cleansing data.",
                "There is no single definition that everyone agrees on and every project is different. But in general engineers design and build things. So in this case they design and build software systems that use machine learning as a core technology.\n\nDo not forget that machine learning is just \u201csoftware\u201d and it is created using a keyboard and a computer so what these people do is write software. ML may be at the core but most usable systems do a lot more and have interfaces with other systems",
                "Day 1\n\nCoffee\n\nWork with shitty data.\n\nWork with shitty data.\n\nAnswer emails.\n\nWork with shitty data.\n\nWork with shitty data.\n\nWork with shitty data.\n\nDay 2\n\nCoffee\n\nWork with shitty data.\n\nWork with shitty data.\n\nAnswer emails.\n\nWork with shitty data.\n\nWork with shitty data.\n\nWork with shitty data.\n\nDay 3\n\nCoffee\n\nWork with shitty data.\n\nWork with shitty data.\n\nAnswer emails.\n\nWork with shitty data.\n\nWork with shitty data.\n\nWork with shitty data.\n\nThe real world is about data, not modeling, reading papers\u2026 etc.",
                "Original Question: What does a machine learning engineer\u2019s work entail?\n\nProbably I\u2019m behind the times, but working on Machine Learning (ML) projects isn\u2019t much different than any other code: Input(s) -%3E process -%3E output(s) -%3E repeat. There have been programs written whose operation has relied on prior or collected data for many decades.\n\nUnlike Deep Learning (DL), where the \u201cexperience\u201d data is determined ahead of applying it to the application code, ML mostly gathers the \u201cprior\u201d data while the application is running.\n\nEngineering wise, that means designing the hardware and writing the software to be able to collect, analyze, integrate and act upon the appropriate data as the application is running.\n\nMy experience has been in designing supervised ML (human set up) embedded (microcontroller based) industrial control subsystems and test equipment. Once installed, the ML code would automatically compensate for wear and improve operation with use. What I\u2019ve written here is generally applicable to ML projects embedded or not.\n\nCollecting data is a combination of hardware and software. The ML engineer has to determine what inputs (sensors, serial communication, user buttons, etc. are needed to do the job. Some input sources and/or sensor(s) have to provide measures to the ML agent for improving/maintaining operation. Building an ML thermostat would certainly require a temperature sensor and some user input button(s).\n\nAnalyzing the data means authenticating what has been input. For serial or parallel data reception that may include parity, checksum, etc. For reading an analog input that may mean range and rate-of-change testing. Some way to determine if the current data is for real, then signaling, logging and ignoring if it isn\u2019t.\n\nIntegrating the current data is where the Artificial Intelligence (AI) can come in (although not always needed). Often Bayesian in origin, your program code will compare the prior data collected against the current input(s). If needed it will then change some internal offset/factor to normalize operation. In some applications, a novel input is added to a table as new \u201cprior\u201d data to compare to.\n\nAction will then be taken to change an output device(s) state to improve the system\u2019s operation. That may occur when a threshold is reached or due to a specific combination of input readings.\n\nWell, that is my take on ML from the embedded engineer\u2019s perspective. Here is an application example of ML and DL combined:\n\nBob Mikkelson's answer to Is there a difference in the implementation of artificial intelligence projects and the implementation of software projects? [ https://www.quora.com/Is-there-a-difference-in-the-implementation-of-artificial-intelligence-projects-and-the-implementation-of-software-projects/answer/Bob-Mikkelson ]",
                "The role of a machine learning engineer is actually much better defined than that of a data scientist.\n\nWhy? Because the companies that use that job title are the ones that have a very clear idea about how and why they want to utilize machine learning. In addition, these companies almost invariously have data scientists as well, so they have defined the distinction between the two.\n\nData scientist is a job title that\u2019s often poorly defined. It\u2019s usually an analyst that knows some programming and machine learning. A machine learning engineer is a full-blown software engineer that has specialized in machine learning.\n\nThe most important responsibilities of a machine learning engineer are roughly these:\n\n * Running machine learning experiments using a programming language with machine learning libraries.\n * Deploying machine learning solutions into production.\n * Optimizing solutions for performance and scalability.\n * Data engineering, i.e. ensuring a good data flow between database and backend systems.\n * Implementing custom machine learning code.\n * Data science, i.e. analyzing data and coming up with use cases.\n",
                "General working process consists of continuous iterations between:\n\n1. Data mining - gathering and pre-processing\n2. Model design - development of ML algorithms\n3. Testing - evaluation how well current model is doing\n4. Deployment - unfolding trained model in production environment\n",
                "Basically, it\u2019s:\n\n * Find solutions to complex problems, often using machine learning but not always - if there is a solution that is faster, better, simpler etc.\n * Find how to exploit dormant unused datasets to make value for the company.\n * Improve the models build during previous work on a project.\n * Request data that could be valuable for the company based on what you or other teams might need.\n * Be a geek about artificial intelligence\n * Transform Machine Learning prototypes into working products, ready to be scaled.\n * Much more!\n",
                "It\u2019s not a complicated process.\n\nI sit in meetings.\n\nThe business defines a problem.\n\nI determine if we have the data for the problem.\n\nIf yes, I source, cleanse and model that data. The majority of my work is data sourcing and data cleansing. (90% of the job)\n\nOnce the model works, I put in prod or work with the web heads to create a front end for it.\n\nI then set up more meetings to walk the business through how to use what I\u2019ve created.\n\nOnce that project is completed, I do the same process over again.",
                "Basically, it\u2019s:\n\n * Find solutions to complex problems, often using machine learning but not always - if there is a solution that is faster, better, simpler etc.\n * Find how to exploit dormant unused datasets to make value for the company.\n * Improve the models build during previous work on a project.\n * Request data that could be valuable for the company based on what you or other teams might need.\n * Be a geek about artificial intelligence\n * Transform Machine Learning prototypes into working products, ready to be scaled.\n * Much more!\n",
                "The role of a machine learning engineer is actually much better defined than that of a data scientist.\n\nWhy? Because the companies that use that job title are the ones that have a very clear idea about how and why they want to utilize machine learning. In addition, these companies almost invariously have data scientists as well, so they have defined the distinction between the two.\n\nData scientist is a job title that\u2019s often poorly defined. It\u2019s usually an analyst that knows some programming and machine learning. A machine learning engineer is a full-blown software engineer that has specialized in machine learning.\n\nThe most important responsibilities of a machine learning engineer are roughly these:\n\n * Running machine learning experiments using a programming language with machine learning libraries.\n * Deploying machine learning solutions into production.\n * Optimizing solutions for performance and scalability.\n * Data engineering, i.e. ensuring a good data flow between database and backend systems.\n * Implementing custom machine learning code.\n * Data science, i.e. analyzing data and coming up with use cases.\n",
                "Hi, ML engineer here.\n\nCurrently working on NLP, for the most part, including intent classification and entity extraction.\n\nHere\u2019s a typical day for me:\n\nGet to work, pull up GitHub and check on the ZenHub board (kind of like Jira, except way cooler). I had some models that were training last night on our servers and I should have gotten an email that they finished. I did!\n\nI\u2019ll probably spend a few minutes testing those new models and then tweak some parameters, then restart the training process.\n\nThe rest of the day I\u2019m usually head-down coding, either working on a back-end Python application that will supply the AI for one of our products, or implementing a new algorithm that I want to try out.\n\nFor example, recently I read a paper on coupled simulated annealing (CSA), [ http://ftp://ftp.esat.kuleuven.be/sista/sdesouza/papers/CSA2009accepted.pdf ] and I wanted to try it out on tuning the parameters for XGBoost as an alternative to a grid search. CSA is a generalized form of simulated annealing (SA), which is an algorithm for optimizing a function that doesn\u2019t use any information on the derivative of the function.\n\nUnfortunately I couldn\u2019t find an implementation in Python, so I decided to write my own. Two days later, I had submitted my first package to PyPI!\n\nAnyways\u2026 I\u2019ve actually never met another ML engineer, so I have no idea if my daily experience is typical ;-) Did I miss anything? Bagel Fridays are awesome because I\u2019m the only who likes everything bagels. Must be a New York thing? (Currently in the middle of Iowa, not to be confused with Idaho. We have CORN, not the potatoes).",
                "The pic below is the machine learning pipeline.\n\nEverything I do is within the confines of this pic.\n\nNow, most of my time is spent on this step.\n\nSurveys and studies have shown that most of the time of a MLE is spent sourcing, wrangling and cleansing data.",
                "There is no single definition that everyone agrees on and every project is different. But in general engineers design and build things. So in this case they design and build software systems that use machine learning as a core technology.\n\nDo not forget that machine learning is just \u201csoftware\u201d and it is created using a keyboard and a computer so what these people do is write software. ML may be at the core but most usable systems do a lot more and have interfaces with other systems",
                "Day 1\n\nCoffee\n\nWork with shitty data.\n\nWork with shitty data.\n\nAnswer emails.\n\nWork with shitty data.\n\nWork with shitty data.\n\nWork with shitty data.\n\nDay 2\n\nCoffee\n\nWork with shitty data.\n\nWork with shitty data.\n\nAnswer emails.\n\nWork with shitty data.\n\nWork with shitty data.\n\nWork with shitty data.\n\nDay 3\n\nCoffee\n\nWork with shitty data.\n\nWork with shitty data.\n\nAnswer emails.\n\nWork with shitty data.\n\nWork with shitty data.\n\nWork with shitty data.\n\nThe real world is about data, not modeling, reading papers\u2026 etc.",
                "Original Question: What does a machine learning engineer\u2019s work entail?\n\nProbably I\u2019m behind the times, but working on Machine Learning (ML) projects isn\u2019t much different than any other code: Input(s) -%3E process -%3E output(s) -%3E repeat. There have been programs written whose operation has relied on prior or collected data for many decades.\n\nUnlike Deep Learning (DL), where the \u201cexperience\u201d data is determined ahead of applying it to the application code, ML mostly gathers the \u201cprior\u201d data while the application is running.\n\nEngineering wise, that means designing the hardware and writing the software to be able to collect, analyze, integrate and act upon the appropriate data as the application is running.\n\nMy experience has been in designing supervised ML (human set up) embedded (microcontroller based) industrial control subsystems and test equipment. Once installed, the ML code would automatically compensate for wear and improve operation with use. What I\u2019ve written here is generally applicable to ML projects embedded or not.\n\nCollecting data is a combination of hardware and software. The ML engineer has to determine what inputs (sensors, serial communication, user buttons, etc. are needed to do the job. Some input sources and/or sensor(s) have to provide measures to the ML agent for improving/maintaining operation. Building an ML thermostat would certainly require a temperature sensor and some user input button(s).\n\nAnalyzing the data means authenticating what has been input. For serial or parallel data reception that may include parity, checksum, etc. For reading an analog input that may mean range and rate-of-change testing. Some way to determine if the current data is for real, then signaling, logging and ignoring if it isn\u2019t.\n\nIntegrating the current data is where the Artificial Intelligence (AI) can come in (although not always needed). Often Bayesian in origin, your program code will compare the prior data collected against the current input(s). If needed it will then change some internal offset/factor to normalize operation. In some applications, a novel input is added to a table as new \u201cprior\u201d data to compare to.\n\nAction will then be taken to change an output device(s) state to improve the system\u2019s operation. That may occur when a threshold is reached or due to a specific combination of input readings.\n\nWell, that is my take on ML from the embedded engineer\u2019s perspective. Here is an application example of ML and DL combined:\n\nBob Mikkelson's answer to Is there a difference in the implementation of artificial intelligence projects and the implementation of software projects? [ https://www.quora.com/Is-there-a-difference-in-the-implementation-of-artificial-intelligence-projects-and-the-implementation-of-software-projects/answer/Bob-Mikkelson ]",
                "Day 1\n\nCoffee\n\nWork with shitty data.\n\nWork with shitty data.\n\nAnswer emails.\n\nWork with shitty data.\n\nWork with shitty data.\n\nWork with shitty data.\n\nDay 2\n\nCoffee\n\nWork with shitty data.\n\nWork with shitty data.\n\nAnswer emails.\n\nWork with shitty data.\n\nWork with shitty data.\n\nWork with shitty data.\n\nDay 3\n\nCoffee\n\nWork with shitty data.\n\nWork with shitty data.\n\nAnswer emails.\n\nWork with shitty data.\n\nWork with shitty data.\n\nWork with shitty data.\n\nThe real world is about data, not modeling, reading papers\u2026 etc.",
                "Basically, it\u2019s:\n\n * Find solutions to complex problems, often using machine learning but not always - if there is a solution that is faster, better, simpler etc.\n * Find how to exploit dormant unused datasets to make value for the company.\n * Improve the models build during previous work on a project.\n * Request data that could be valuable for the company based on what you or other teams might need.\n * Be a geek about artificial intelligence\n * Transform Machine Learning prototypes into working products, ready to be scaled.\n * Much more!\n",
                "The role of a machine learning engineer is actually much better defined than that of a data scientist.\n\nWhy? Because the companies that use that job title are the ones that have a very clear idea about how and why they want to utilize machine learning. In addition, these companies almost invariously have data scientists as well, so they have defined the distinction between the two.\n\nData scientist is a job title that\u2019s often poorly defined. It\u2019s usually an analyst that knows some programming and machine learning. A machine learning engineer is a full-blown software engineer that has specialized in machine learning.\n\nThe most important responsibilities of a machine learning engineer are roughly these:\n\n * Running machine learning experiments using a programming language with machine learning libraries.\n * Deploying machine learning solutions into production.\n * Optimizing solutions for performance and scalability.\n * Data engineering, i.e. ensuring a good data flow between database and backend systems.\n * Implementing custom machine learning code.\n * Data science, i.e. analyzing data and coming up with use cases.\n",
                "while role == ml_engineer:\n   Wake up. \n   Solve that tricky part that was bugging you yesterday.\n   Spend a couple hours integrating it into a service.\n   Spend some more time building useful features.\n   Commit, push.\n   Get the feedback and fix some bugs.\n   Start tackling a new task, stumble upon some tricky part. \n   Enough for today.\n\n\nOptionally you can include reading articles about some obscure topic.",
                "The other answers do a great job covering the technical aspects of this question, such as the fundamentals you need to learn, the courses you should take, the papers you should read etc. I'm going to cover this question from another angle and talk about the traits you need to have or be able to cultivate in order to be a good machine learning engineer. \n\nHere is a list of must-have traits, in no particular order:\n\n * You need to enjoy an iterative process of development. If you want to build a machine learning system, you need to be able to build a version 0.1 using a very simple model quickly. Then iterate on getting it better at every successive stage.\n * You also need to have a good intuition for when to stop. In any machine learning system, you can always improve the accuracy by iterating on it more. But at some point, the effort you put into it exceeds the value you derive from it. You need to be able to identify that point.\n * You should be comfortable with failure. A lot of your models and experiments will fail. And that\u2019s ok.\n * You should be driven by curiosity. The best people are the ones who are genuinely curious about the world around them and channel that curiosity when working on machine learning.\n * You need to have a good data intuition. You should be good at identifying patterns in the data. Being able to create quick data visualizations (using R, Python, Matlab or Excel etc.) helps.\n * You need to have a good sense of metrics and be metrics-driven. You should to be able to establish metrics that define success or failure of your system. You should feel comfortable with blind experiments [1]  and terms like precision, recall, accuracy, ROC, conversion rates, NDCG etc.\n * Metrics are great at giving a high level view of how your system is doing. But at the same time, you should never stop directly looking at individual examples. Manually looking at your biggest wins and your biggest losses (e.g. worst performing queries for a search engine), as well as random samples of your data, teaches you a lot about your machine learning system that raw metrics don\u2019t.\n * You should be able to develop a generalized approach to fixing the bugs/misclassifications in your models. Fixing individual bugs will only let you attain a local maxima. More often that not, individual fixes will also make your models more complex and harder to work with. Gathering all the issues together and identifying common themes will let you focus on the biggest impact issues that you can fix in your next round of iteration.\n * When developing your models, you should be able to put yourself in the minds of the users / customers of your system. It\u2019s easy to build something that\u2019s good enough for you. But your current and future users matter way more than you do. There are also a lot of biases [2] that affect individual decision making, and you should try to be aware of and account for as many of them as possible.\nThis list was adapted from a blog post I wrote a while ago: What makes a good data scientist/engineer? [ https://medium.com/@nikhilbd/what-makes-a-good-data-scientist-engineer-a8b4d7948a86#.ylky30l9h ] \n\n\n1. Blind experiment [ https://en.wikipedia.org/wiki/Blind_experiment ]\n2. List of cognitive biases [ https://en.wikipedia.org/wiki/List_of_cognitive_biases ]",
                "The other answers do a great job covering the technical aspects of this question, such as the fundamentals you need to learn, the courses you should take, the papers you should read etc. I'm going to cover this question from another angle and talk about the traits you need to have or be able to cultivate in order to be a good machine learning engineer. \n\nHere is a list of must-have traits, in no particular order:\n\n * You need to enjoy an iterative process of development. If you want to build a machine learning system, you need to be able to build a version 0.1 using a very simple model quickly. Then iterate on getting it better at every successive stage.\n * You also need to have a good intuition for when to stop. In any machine learning system, you can always improve the accuracy by iterating on it more. But at some point, the effort you put into it exceeds the value you derive from it. You need to be able to identify that point.\n * You should be comfortable with failure. A lot of your models and experiments will fail. And that\u2019s ok.\n * You should be driven by curiosity. The best people are the ones who are genuinely curious about the world around them and channel that curiosity when working on machine learning.\n * You need to have a good data intuition. You should be good at identifying patterns in the data. Being able to create quick data visualizations (using R, Python, Matlab or Excel etc.) helps.\n * You need to have a good sense of metrics and be metrics-driven. You should to be able to establish metrics that define success or failure of your system. You should feel comfortable with blind experiments [1]  and terms like precision, recall, accuracy, ROC, conversion rates, NDCG etc.\n * Metrics are great at giving a high level view of how your system is doing. But at the same time, you should never stop directly looking at individual examples. Manually looking at your biggest wins and your biggest losses (e.g. worst performing queries for a search engine), as well as random samples of your data, teaches you a lot about your machine learning system that raw metrics don\u2019t.\n * You should be able to develop a generalized approach to fixing the bugs/misclassifications in your models. Fixing individual bugs will only let you attain a local maxima. More often that not, individual fixes will also make your models more complex and harder to work with. Gathering all the issues together and identifying common themes will let you focus on the biggest impact issues that you can fix in your next round of iteration.\n * When developing your models, you should be able to put yourself in the minds of the users / customers of your system. It\u2019s easy to build something that\u2019s good enough for you. But your current and future users matter way more than you do. There are also a lot of biases [2] that affect individual decision making, and you should try to be aware of and account for as many of them as possible.\nThis list was adapted from a blog post I wrote a while ago: What makes a good data scientist/engineer? [ https://medium.com/@nikhilbd/what-makes-a-good-data-scientist-engineer-a8b4d7948a86#.ylky30l9h ] \n\n\n1. Blind experiment [ https://en.wikipedia.org/wiki/Blind_experiment ]\n2. List of cognitive biases [ https://en.wikipedia.org/wiki/List_of_cognitive_biases ]",
                "That will depend on the individuals knowledge and skill.\n\nIf you happen to be a Python programmer with really solid SQL skill then you\u2019ll learning curve is going to be short. Why? Because what most of us do all day is work with SQL and Python.\n\nIf you know nothing about this space or data then you\u2019re looking at 5\u20136 years before you\u2019re doing entry level machine learning in the real-world.\n\nLook at this job posting below. That first requirement took me years to learn and become comfortable working with in the real-world. I know people on Quora find SQL easy, that is\u2026 until the interview\u2026 where the failure rate is around 90%.\n\nThat\u2019s just one bullet point!!\n\nReal-world machine learning is not modeling, it\u2019s data cleansing. If you can\u2019t source and clean your own data then you won\u2019t be working in the real-world.\n\nAround 80% of all applied machine learning is data cleansing. I didn\u2019t make this up, this comes from studies and surveys from those in the real-world.\n\nThere\u2019s a reason why the salaries and packages are big in this space, it\u2019s because the knowledge, skills and experience are big.\n\nThere is no easy path and most will need to start working in a lesser role, like a data analyst or business analyst in order to make the jump to machine learning.\n\nNow, this shouldn\u2019t discourage you. It should help you set some realistic expectations of what\u2019s really required to work in this space. When you\u2019re ready to dig in and learn this platform will get you there [ https://www.logikbot.com/ ]. It\u2019s not an easy path and most simply won\u2019t make it.\n\nhttps://youtu.be/xKYDdk4r88w\nTake it from those in the real-world.\n\n",
                "If you already have some knowledge in machine learning, that will be a great start. If not, then check out on internet what are the recruiters looking for in an ideal candidate for such job positions. Learn as much of these qualities as you can. Do some online course(s) on it. Earn some certificates for yourself, if possible. It will barely take a few weeks. Once you have some concrete knowledge (just some will work, believe me), apply for an internship by signing up on job portals, especially on LinkedIn. Internship will be the perfect place to start for any nervous beginner. Go for paid internships. There are plenty of paid internship offers in ML or related field these days. Do an internship for 2\u20133 or 6 month. By the 6th month you will be ready for entry level job position in quality companies with a good salary.\n\nBe patient. Keep learning. Keep applying. These are the key to any person in this field.\n\nBest of luck!\n\nThanks for A2A.",
                "To get started with machine learning, your basics should be sound in at least the following subjects\n\n1. Probability and Statistics\n2. Calculus\n3. Linear Algebra\nIn addition, you should be fluent in programming in at least one language. Languages like C, C++ as well as scripting languages like Python, Matlab, R are popularly used to code machine learning algorithms.\n\nIf your university has a computer science or some other department that offers a course in machine learning or related areas like data mining, artificial intelligence, then you should try to credit those courses or at least attend lectures. Crediting helps keeping motivation levels high. Make sure to pay attention to the course project if there is one. You may learn much more from working on the project than from the lectures.\n\nIn addition, there are a number of online resources (course notes, online courses) for machine learning courses that you can use to learn about the subject. In fact there are Quora questions that explicitly point to some good courses out there. However, you will have to keep your self motivated so that you do not lose focus and leave midway. There are plenty of challenge problems on platforms like Kaggle that you can use as a proxy for a project in this area.\n\nHappy Learning!",
                "The answer is both \u2018YES\u2019 and \u2018NO\u2019.\n\n\u2018NO\u2019 if you don\u2019t understand the mathematics and the logic behind Machine Learning.\n\nSource:Google\n\nIn today\u2019s world, more than your degree, \u201cSKILLS\u201c matter. You may or may not have a Master or PhD in CS yet you can become a ML expert. The condition is you\u2019re good in Mathematics and Statistics. In this era of free MOOCs upgrading yourself is not difficult. You can have a Bachelors in Economics/Mathematics/Statistics or even Literature but you have to be quite good with basic mathematics to get a good grasp of ML models. There are several courses in Coursera, Analytics Vidhya, Edureka, DataQuest, KDNuggets, GeeksforGeeks which one can use to learn.\n\nParticipate in hackathons and competitions in Kaggle. Make mistakes and learn from it. That is the best way to teach yourself any skill in any domain.\n\nIt\u2019s better to do Masters in AI and DataScience than just CS if the career to be pursued is in Machine Learning.\n\nPrerequisites of a ML expert - Knowledge in Python/R, Good knowledge in SQL, Basic knowledge of Linear Algebra, Probability, Stochastic Calculus, Inferential Statistics. If it\u2019s more niche role, Graph theory or Discreet mathematics will also be required above them.\n\nMostly employers prefer candidates with bachelors who know how to work using built-in ML models without writing complex formulas or equations. But in certain companies where problems require algorithms to be tweaked or even built from scratch - a MSc/PhD will be of more importance.",
                "That will depend on the individuals knowledge and skill.\n\nIf you happen to be a Python programmer with really solid SQL skill then you\u2019ll learning curve is going to be short. Why? Because what most of us do all day is work with SQL and Python.\n\nIf you know nothing about this space or data then you\u2019re looking at 5\u20136 years before you\u2019re doing entry level machine learning in the real-world.\n\nLook at this job posting below. That first requirement took me years to learn and become comfortable working with in the real-world. I know people on Quora find SQL easy, that is\u2026 until the interview\u2026 where the failure rate is around 90%.\n\nThat\u2019s just one bullet point!!\n\nReal-world machine learning is not modeling, it\u2019s data cleansing. If you can\u2019t source and clean your own data then you won\u2019t be working in the real-world.\n\nAround 80% of all applied machine learning is data cleansing. I didn\u2019t make this up, this comes from studies and surveys from those in the real-world.\n\nThere\u2019s a reason why the salaries and packages are big in this space, it\u2019s because the knowledge, skills and experience are big.\n\nThere is no easy path and most will need to start working in a lesser role, like a data analyst or business analyst in order to make the jump to machine learning.\n\nNow, this shouldn\u2019t discourage you. It should help you set some realistic expectations of what\u2019s really required to work in this space. When you\u2019re ready to dig in and learn this platform will get you there [ https://www.logikbot.com/ ]. It\u2019s not an easy path and most simply won\u2019t make it.\n\nhttps://youtu.be/xKYDdk4r88w\nTake it from those in the real-world.\n\n",
                "In the US probably not.\n\nMost companies use a bachelors in something as a filter.\n\nAll you need is a bachelors though.\n\nThe good news is, machine learning engineers are made in the real-world, not from academia.\n\nDon\u2019t listen to anyone who tells you about the math and advanced degree you need. That\u2019s utter horseshit.\n\nRead this.\n\nhttps://www.quora.com/Is-it-mathematics-that-makes-the-difference-between-the-greatest-AI-machine-learning-engineers-and-the-rest/answer/Mike-West-99\n",
                "In this age, where schools and books are being made obsolete by MOOCs and programming websites (at least in the field of Computer Science), yes you can, and you should. \n\n Let's take 'Machine Learning Engineer' as a running example. In their daily role, they build/test/deploy machine learning systems that can classify/analyse hundreds of data. They are not expected to take state-of-the-art methods further, they only need to be aware of the tools to use for their specific purpose.\n\nI think a good combination of programming skills, coupled with a good machine learning foundation (being aware of classification and clustering algorithms, being able to understand mathematical formulas and tricks stated in the papers) is enough for that role, as a beginner. \n\nYou will see an image below, an open position at Palantir [3], a machine learning and data analysis giant based in USA. The formal position is here: Software Engineer (Machine Learning) [ https://www.palantir.com/careers/positions/software-engineer-machine-learning/ ]\n\nFigure.1 A machine learning engineer job requirements and desirable skills at Palantir[3]\n\nWhat do they want? They require BS in CS, communication skills to understand customer needs, data cleaning and processing skills and strong programming  (C/C++) and scripting (Python, R) background. \n\nYou need to understand base algorithms on machine learning. You may check Andrew NG's excellent machine learning course [2] from Coursera for this. Weekly assignments held in Matlab will enable you to do hands-on experience with a scripting language while understanding the algorithms. I strongly suggest you to read excellent  \"A few useful things to know about machine learning\" from Pedro Domingos [1]. \n\nYou should be aware of available machine learning libraries which will fuel your work when it comes to implementation [6]\n\nThen, you can go further and set-up a distributed computing platform (Hadoop, Spark, etc) in your own desktop [4]. As you probably do not have so many clusters to distribute work across, you can work on a virtual machine version of Spark. Then, you may try to implement the same algorithms told in the machine learning course using Python, in a distributed or single machine setting. This will allow you to think in terms of 'scalability' of algorithms, which is one of the important skills as an engineer. You may use a scalable algorithm or find a way to distribute workload to clusters. \n\nYou can even go further and join Kaggle [5] competitions which is an excellent place to compete with people for different real-world problems. \n\nWhat you have done so far? You understood the basics of machine learning algorithms, you implemented them using scripting languages, you experience distributed computing platforms, and you even compete with real people on real-world data problems. OK, a masters will not hurt your chance for doing all of these, but who really need this in today's world? \n\nGood Luck.\n\n[1] Page on hacettepe.edu.tr [ http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2013/bil682/readings/week4/machine-learning-review-domingos.pdf ]\n[2] Coursera [ https://class.coursera.org/ml-005/lecture ]\n[3] Home [ https://www.palantir.com/ ]\n[4] Scalable Machine Learning [ https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x ]\n[5] The Home of Data Science [ https://www.kaggle.com/ ]\n[6] 17 Great Machine Learning Libraries [ http://daoudclarke.github.io/machine%20learning%20in%20practice/2013/10/08/machine-learning-libraries/ ]",
                "What skill do you need to learn?\n\nMachine learning.\n\nIf you want to skip the rant and go to my actual advice, scroll to the bottom!\n\nRant\n\nYes, it\u2019s an obvious advantage that you know some programming. Coding proficiency is a basic requirement to do machine learning, although it\u2019s not as important as in a typical software developer job.\n\nMathematical optimization and calculus is somewhat important.\n\nLinear algebra and statistics is really important.\n\nProbability and combinatorics is really important.\n\nBut these are just prerequisites. Most importantly, machine learning is an entire branch of computer science in itself. There is feature engineering, deep learning, ensembling and all kinds of stuff you need to learn about that are not part of the above-mentioned areas.\n\nIf you come from a regular programming background, machine learning will require a shift in perspective. You\u2019re no longer dealing with deterministic logic in the form of ifs and elses. You\u2019re now dealing with probabilities and uncertainties.\n\nIn most of software development, the business logic is very simple. In machine learning, business logic is everything.\n\nObject-oriented programming is no longer important. Efficient functions are. For loops and while loops are things to be avoided. Matrix multiplications and vectorized code is your new bread and butter. I\u2019m tempted to say that you will need to throw out everything you have learned about programming and start over. That would be an exagerration, but there is some truth in it.\n\nAdvice\n\nWith your background, you have a really good foundation for becoming a big data architect. Linux, Python, Java and systems administration is mostly what big data is all about. As a big data developer, you can start experimenting with big data anlytics and gradually progress into machine learning from there.",
                "It\u2019s a job.\n\nPlease don\u2019t put this career on a pedestal or you\u2019ll need a shit not of Prozac when you get hired. It\u2019s very little modeling and mostly data and meetings.\n\nWhat\u2019s great about being a machine learning engineer?\n\nI\u2019m not going to lie\u2026 the money. If you would have told me in college that someday I\u2019d make 300K a year I would have walked you to counselling services.\n\nIn addition to the money, job flexibility is insane. I pick jobs I want. If I don\u2019t like what I hear in the interview I end it and move on.\n\nI like working with smart people. It\u2019s really less draining than working with idiots.\n\nI love learning and in this role, you\u2019ll learn something new everyday I promise.\n\nNow, it\u2019s not all sunshine and lollipops.\n\nWorking with real-world data sucks. There\u2019s no two ways about. It\u2019s what I spend most of my time doing. If you have to work with unstructured data then the suckage level increases exponentially.\n\nIt\u2019s a great career but it\u2019s not like winning the lottery.\n\nIf you\u2019re interested in some of my insights about the job follow this space.\n\nhttps://logikbot.quora.com/\n",
                "Quora. We have an awesome team [ https://www.quora.com/about/team ] here and ML has had and is continuing to have a big impact on the company. Come join us!",
                "Today\u2019s AI is an easy-peasy thing to learn, study and apply; for it is reduced to ML and DL or Predictive Analytics:\n\nAI = ML %3E DL = DNNs %3C Statistical Learning %3C Computational Statistics %3C AGI %3C ASI\n\nArtificial intelligence (AI) is generally conceived as the application of mathematics, statistics and data analytics and programming to the modelling (primarily) of the functions of the human brain.\n\nThe advocates of the \u2018Techno-Singularity\u2019 expect that such an AI modelling might one day lead to an irreversible and uncontrollable explosion of ever more intelligent machines; for there seems obvious to all that there is an almost limitless potential for further, equally significant AI discoveries in the future. {Why Machine Will Never Rule the World: AI Without Fear, 2023}\n\nTo start, follow its simple general description:\n\n\u201cIn machine learning, a common task is the study and construction of algorithms that can learn from and make predictions on data. Such algorithms work by making data-driven predictions or decisions, through building a mathematical model from input data.\u201d\n\nLearn a triple of basic learning methods:\n\nSupervised learning infers a function from labeled training data.\n\nUnsupervised learning finds previously unknown patterns in data set without pre-existing labels.\n\nReinforcement learning is concerned with how software agents ought to take actions in an environment so as to maximize a cumulative reward.\n\nSo, you need to study a bit of applied mathematics, statistics and programming.\n\nReal and True AI is too complex, and demands a lot of multi- and trans-disciplinary knowledge of\n\nPhilosophy, Ontology and Epistemology\n\nScience and Technology\n\nCognitive Science and Linguistics\n\nMathematics and Statistics\n\nComputer Science\n\nData Science and Engineering\u2026\n\nAs a result, such a transdisciplinary AI (Trans-AI) conception and model architecture embraces the narrow and weak AI paradigm, as ML and DL, as well as its possible prospects of AGI and ASI.\n\nReal AI = Trans-AI = Causal Machine Intelligence and Learning =\n\nGlobal Data Framework {Data Understanding Master Algorithm}\n\nAI World Model Engine: Scientific Modelling, making a part or feature of the world to represent, map, understand, define, quantify, visualize, or simulate by referencing it to the world\u2019s knowledge/information/data\n\nCommon Sense Knowledge\n\nStatistical modelling, Data Analytics\n\nAI models, NLP/NLU, ML Algorithms, ANNs, DNNs\n\nAGI (Strong AI, Human-Level AI)\n\nASI (Artificial Superintelligence)\n\nRAI Internet Infrastructure (AI Software, AI Hardware, Cloud Computing, Edge Computing, Communication Networks + Intelligent Applications)\n\n= Global AI Internet Platform\n\nhttps://www.linkedin.com/pulse/which-big-technology-disrupt-world-next-5-10-years-azamat-abdoullaev/?published=t\nNote, there is no demand for the Trans-AI, its foundational general-purpose technology creating a new global technology system, but only for the weak and narrow AI/ML/DL, with some of the most common jobs as data scientists, machine learning engineers, and software engineers, getting a compensation from $60,000 up to a million.",
                "The other answers do a great job covering the technical aspects of this question, such as the fundamentals you need to learn, the courses you should take, the papers you should read etc. I'm going to cover this question from another angle and talk about the traits you need to have or be able to cultivate in order to be a good machine learning engineer. \n\nHere is a list of must-have traits, in no particular order:\n\n * You need to enjoy an iterative process of development. If you want to build a machine learning system, you need to be able to build a version 0.1 using a very simple model quickly. Then iterate on getting it better at every successive stage.\n * You also need to have a good intuition for when to stop. In any machine learning system, you can always improve the accuracy by iterating on it more. But at some point, the effort you put into it exceeds the value you derive from it. You need to be able to identify that point.\n * You should be comfortable with failure. A lot of your models and experiments will fail. And that\u2019s ok.\n * You should be driven by curiosity. The best people are the ones who are genuinely curious about the world around them and channel that curiosity when working on machine learning.\n * You need to have a good data intuition. You should be good at identifying patterns in the data. Being able to create quick data visualizations (using R, Python, Matlab or Excel etc.) helps.\n * You need to have a good sense of metrics and be metrics-driven. You should to be able to establish metrics that define success or failure of your system. You should feel comfortable with blind experiments [1]  and terms like precision, recall, accuracy, ROC, conversion rates, NDCG etc.\n * Metrics are great at giving a high level view of how your system is doing. But at the same time, you should never stop directly looking at individual examples. Manually looking at your biggest wins and your biggest losses (e.g. worst performing queries for a search engine), as well as random samples of your data, teaches you a lot about your machine learning system that raw metrics don\u2019t.\n * You should be able to develop a generalized approach to fixing the bugs/misclassifications in your models. Fixing individual bugs will only let you attain a local maxima. More often that not, individual fixes will also make your models more complex and harder to work with. Gathering all the issues together and identifying common themes will let you focus on the biggest impact issues that you can fix in your next round of iteration.\n * When developing your models, you should be able to put yourself in the minds of the users / customers of your system. It\u2019s easy to build something that\u2019s good enough for you. But your current and future users matter way more than you do. There are also a lot of biases [2] that affect individual decision making, and you should try to be aware of and account for as many of them as possible.\nThis list was adapted from a blog post I wrote a while ago: What makes a good data scientist/engineer? [ https://medium.com/@nikhilbd/what-makes-a-good-data-scientist-engineer-a8b4d7948a86#.ylky30l9h ] \n\n\n1. Blind experiment [ https://en.wikipedia.org/wiki/Blind_experiment ]\n2. List of cognitive biases [ https://en.wikipedia.org/wiki/List_of_cognitive_biases ]",
                "Machine Learning is the only kind of AI there is.\n\nAI is changing. We are now recognizing that most things called \"AI\" in the past are nothing more than advanced programming tricks. As long as the programmer is the one supplying all the intelligence to the system by programming it in as a World Model, the system is not really an Artificial Intelligence. It's \"just a program\".\n\nDon't model the World; Model the Mind.\n\nWhen you Model the Mind you can create systems capable of Learning everything about the world. It is a much smaller task, since the world is very large and changes behind your back, which means World Models will become obsolete the moment they are made. The only hope to create intelligent systems is to have the system itself create and maintain its own World Models. Continuously, in response to sensory input.\n\nFollowing this line of reasoning, Machine Learning is NOT a subset of AI. It really is the ONLY kind of AI there is.\n\nAnd this is now proving to be true, and in a big way. Since 2012, a specific Machine Learning technique called Deep Learning is taking the AI world by storm. Researchers are abandoning the classical \"Programming Tricks\" style of AI in droves and switching to Deep Learning... based mainly on the fact that it actually works. We've made more progress in three years since 2012 than we've done in the preceeding 25 years on several key AI problems, including Image Understanding (a really hard one), Signal Processing, Voice Understanding, and Text Understanding.\n\nAnother clue that we are now on the right track: Old style AI projects like CYC ran to millions of propositions or millions of lines of code. Systems that (successfully) Model the Mind can be as small as 600 lines of code; several recent Deep Learning projects clock in somewhere in that range. And these programs can move from one problem domain to another with very few changes to the core; this means these methods are GENERAL intelligences, not specific to any one problem domain. This is why it is called Artificial General Intelligence. And we've never had any AI programs that could do this in the past. As an example, the language understanding programs we are creating using DL will work equally well in any language, not just English. It just takes a re-training to switch to Japanese... another indication that Deep Learning is closer to true intelligence than traditional NLP systems.\n\nGoogle is currently using Machine Learning a lot - in my estimate, over a hundred places in their systems have been replaced by Deep Learning and other ML techniques in the past few years. even their patented \"PageRank\" algorithm which was the initial key to their success is being replaced, even as I write this, with a new algorithm called \"RankBrain\" which is based on Deep Learning. In the shareholder's call last week, the CEO of Google said that they were looking at using ML (probably Deep Learning) *everywhere* in all their products. More generally, I expect a deluge of apps and systems that understand languages and images in the next several years, all based on Deep Learning.\n\nI really shouldn't confuse things but strictly speaking, Deep Learning is not AI either. We are currently using Supervised Deep Learning, which is another (but less critical) programmer's cheat since the \"supervision\" is a kind of World Model. Real AI requires Unsupervised Deep Learning. Many people including myself are working on this; it is possibly thousands of times more difficult than Supervised Learning. But this is where we have to go.\n\nDeep Learning isn't AI but it's the only thing we have that's on the path to True AI.",
                "If you want to go for machine learning and AI. Computer science is the ideal field to be in.\n\nThe course structure helps you a lot to study AI and ML in future. But if you're a mechanical engineering student. Just learning data structures and algorithms and discrete maths would help you learn AI&ML",
                "I would expect someone in physics, math, or statistics to be very well-equipped to do machine learning, provided they are also good programmers, and can demonstrate their coding skills in an interview. For most people, the bottleneck is math, not CS \u2014 and most math majors are better at math than CS people.",
                "The answer is both \u2018YES\u2019 and \u2018NO\u2019.\n\n\u2018NO\u2019 if you don\u2019t understand the mathematics and the logic behind Machine Learning.\n\nSource:Google\n\nIn today\u2019s world, more than your degree, \u201cSKILLS\u201c matter. You may or may not have a Master or PhD in CS yet you can become a ML expert. The condition is you\u2019re good in Mathematics and Statistics. In this era of free MOOCs upgrading yourself is not difficult. You can have a Bachelors in Economics/Mathematics/Statistics or even Literature but you have to be quite good with basic mathematics to get a good grasp of ML models. There are several courses in Coursera, Analytics Vidhya, Edureka, DataQuest, KDNuggets, GeeksforGeeks which one can use to learn.\n\nParticipate in hackathons and competitions in Kaggle. Make mistakes and learn from it. That is the best way to teach yourself any skill in any domain.\n\nIt\u2019s better to do Masters in AI and DataScience than just CS if the career to be pursued is in Machine Learning.\n\nPrerequisites of a ML expert - Knowledge in Python/R, Good knowledge in SQL, Basic knowledge of Linear Algebra, Probability, Stochastic Calculus, Inferential Statistics. If it\u2019s more niche role, Graph theory or Discreet mathematics will also be required above them.\n\nMostly employers prefer candidates with bachelors who know how to work using built-in ML models without writing complex formulas or equations. But in certain companies where problems require algorithms to be tweaked or even built from scratch - a MSc/PhD will be of more importance.",
                "I am a machine learning enthusiast. I might not be very familiar with all branches of AI. \nBut a lot of working stuff now has used some part of machine learning. \nInitially you might feel lost or think that stuff are too difficult for you. But if you just keep trying to learn, you will eventually get comforatble with things. Its good to know what you dont know. Its good to know what to read and in what order. \nStarting with a MOOC is great, it gets you started and excited about this field. Being excited about something is important. But what to do after this?\n\nWhat I really think is very very important is knowing few things and as you get familiar with this the older stuffs you read makes more sense. It might look a lot but if you consume slowly bit by bit you will understand a lot in sometime.\n\nso 1st thing you should learn is Linear Algebra at an intuitive level and understand the computational issues. \nA large part of Multiple veiw geometry in computer vision is linear algebra and a bit of optimization. This will make you aprreciate it more. (sorry for ignoring tensors.. anyways it gives headaches)\n\n2nd thing is understanding probability and statistic. Useful in understanding Graphical models and Deep Learning. Note: Graphical model is a major topic in machine learning.\n\n3rd would be watching/rewatching a standard machine learning lectures. Like the one by mathmatical monk on youtube, learning from data, Andrew Ng's longer version of course. I would also suggest you to read all the lecture notes of Andrew Ng's Class.\n\n4th for getting a flavour of optimization start with linear programming. The one at coursera is a great intro into this(The one by UoColorado). There are others which might be a bit more serious and some more insights.\n\n5th would be deeper into optimization(Convex optimization). The one by BOYD is awesome.\n\nMaybe be now comes the time to read a standard ML text. Reading about probability, statistics, information theory, inference would also be useful. (This can be done in parallel with video lectures.)\n\nNOTE: this is not a race which you need to finish fast. Just keep learning take your time. If you really try to do it fast you will be overwhelmed by the content which will force you to ask more questions like these. :P\n\nOther things not in answer which is also a part of knowing ML well are scalablity, functional analysis, Advanced optimization, compressed sensing, Probablistic Graphical Models ,Approximate inference.\n\nBtw I am running a year machine learning course. Ofcourse there is a limitation in how much can be covered in a year. But it's a good well balanced course for beginners and intermediates.\n\nWebsite link: thecuriouscurator.in\n\nHope this helps.",
                "Disclaimer: I am not a machine learning/artificial intelligence expert or actively working in ML/AI. I took 1 course in ML in my undergrad, still however, I hope that I can share some insight.\n\nIf I were trying to do research in ML/AI there are several things I would do:\n\n * Take an online course in Machine Learning/AI (Udacity/Coursera are good sources from what I hear)\n * Network with engineers working in the field. Go to meetups/conferences to meet engineers working in the field.\n * Look at various graduate programs that might interest you, see what kind of problems the professors there working on. Read their research.\n * Dabble around with various ML/AI frameworks. Like with most things in SE, some frameworks can be better for solving certain kinds of problems.\n * Checkout out this popular ML roadmap repo on Github [ https://github.com/ZuzooVn/machine-learning-for-software-engineers ]\n * Listen to the experts: Andrew Ng, Andrej Karpathy, Geoffrey Hinton\nOne thing to note about working on Machine Learning Research it\u2019s very different to working in the industry.\n\nIn particular, working in ML/AI research you\u2019d be working on something novel, where as with industry, the goal is to solve problems using methods that have been tried before.\n\nHope this helps and best of luck!",
                "In this age, where schools and books are being made obsolete by MOOCs and programming websites (at least in the field of Computer Science), yes you can, and you should. \n\n Let's take 'Machine Learning Engineer' as a running example. In their daily role, they build/test/deploy machine learning systems that can classify/analyse hundreds of data. They are not expected to take state-of-the-art methods further, they only need to be aware of the tools to use for their specific purpose.\n\nI think a good combination of programming skills, coupled with a good machine learning foundation (being aware of classification and clustering algorithms, being able to understand mathematical formulas and tricks stated in the papers) is enough for that role, as a beginner. \n\nYou will see an image below, an open position at Palantir [3], a machine learning and data analysis giant based in USA. The formal position is here: Software Engineer (Machine Learning) [ https://www.palantir.com/careers/positions/software-engineer-machine-learning/ ]\n\nFigure.1 A machine learning engineer job requirements and desirable skills at Palantir[3]\n\nWhat do they want? They require BS in CS, communication skills to understand customer needs, data cleaning and processing skills and strong programming  (C/C++) and scripting (Python, R) background. \n\nYou need to understand base algorithms on machine learning. You may check Andrew NG's excellent machine learning course [2] from Coursera for this. Weekly assignments held in Matlab will enable you to do hands-on experience with a scripting language while understanding the algorithms. I strongly suggest you to read excellent  \"A few useful things to know about machine learning\" from Pedro Domingos [1]. \n\nYou should be aware of available machine learning libraries which will fuel your work when it comes to implementation [6]\n\nThen, you can go further and set-up a distributed computing platform (Hadoop, Spark, etc) in your own desktop [4]. As you probably do not have so many clusters to distribute work across, you can work on a virtual machine version of Spark. Then, you may try to implement the same algorithms told in the machine learning course using Python, in a distributed or single machine setting. This will allow you to think in terms of 'scalability' of algorithms, which is one of the important skills as an engineer. You may use a scalable algorithm or find a way to distribute workload to clusters. \n\nYou can even go further and join Kaggle [5] competitions which is an excellent place to compete with people for different real-world problems. \n\nWhat you have done so far? You understood the basics of machine learning algorithms, you implemented them using scripting languages, you experience distributed computing platforms, and you even compete with real people on real-world data problems. OK, a masters will not hurt your chance for doing all of these, but who really need this in today's world? \n\nGood Luck.\n\n[1] Page on hacettepe.edu.tr [ http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2013/bil682/readings/week4/machine-learning-review-domingos.pdf ]\n[2] Coursera [ https://class.coursera.org/ml-005/lecture ]\n[3] Home [ https://www.palantir.com/ ]\n[4] Scalable Machine Learning [ https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x ]\n[5] The Home of Data Science [ https://www.kaggle.com/ ]\n[6] 17 Great Machine Learning Libraries [ http://daoudclarke.github.io/machine%20learning%20in%20practice/2013/10/08/machine-learning-libraries/ ]",
                "You need Computer Science.\n\nSoftware Engineering would be focused on how to write software for large projects. You would miss some of the math (Linear Algerbra, Graph Theory, Set Theory, Probability Theory, and Automata Theory) and AI course. Instead you will take courses in large project economics, understanding requirements engineering, and testing that the SE degree requires. These courses won\u2019t help you in AI and machine learning.\n\nComputer Engineering should teach you how to design computers and program FPGA. You won\u2019t get AI background and Math you desperately need.",
                "My recommendation is a little different from others answering this question; I assume you want to become a star at both Machine Learning AND Engineering.\n\nWhy do I draw the distinction? Well, there are lots of folks in the market that are great engineers and there are also lots of folks who are great at machine learning, but there is a severe shortage of great Machine Learning Engineers.\n\nEngineers who are great in both fields are basically unicorns and are at least 10x as valuable as someone who is great in just one of the fields. These are the engineers who don\u2019t just work on algorithms or systems all day but instead launch personalization products in the market. These are the types of engineers who are behind the personalization teams at companies such as Amazon, Netflix, LinkedIn and many successful personalization startups\n \nSo, what do you do if you want to become one of these unicorns? (In no particular order)\n\n1. Learn how to be a great engineer.  Learn multiple languages and get really good at them. Don\u2019t just focus on a single language such as Python as many Machine Learning Engineers do. Instead, broaden your scope to include languages like Java, C++, Scala and JavaScript. This will allow you to join a team and and to hit the ground running with any company's systems you work with.\n \n2. Learn how to build highly-scaled distributed systems.  Build systems that have a 50ms SLA and take hundreds or thousands of transactions per second. Ideally, systems that are critical for a business to run. Real-time event ingestion and recommendations systems are ideal.\n \n3. Build your machine learning fundamentals by studying some material on the subject:\n * Andrew Ng\u2019s Machine Learning lectures are a great start: https://www.youtube.com/playlist?list=PLA89DCFA6ADACE599 [ https://www.youtube.com/playlist?list=PLA89DCFA6ADACE599 ]\n\n * Stanford\u2019s Data Mining and Applications Certificate: http://scpd.stanford.edu/public/category/courseCategoryCertificateProfile.do?method=load&certificateId=1209602 [ http://scpd.stanford.edu/public/category/courseCategoryCertificateProfile.do?method=load&certificateId=1209602 ]\n\n * Machine Learning Summer School: https://www.youtube.com/playlist?list=PLZSO_6-bSqHQCIYxE3ycGLXHMjK3XV7Iz [ https://www.youtube.com/playlist?list=PLZSO_6-bSqHQCIYxE3ycGLXHMjK3XV7Iz ]\n\n\n4. Play with some big datasets that are publicly available. Find a dataset that you find personally interesting or that you have theories about and see if you are correct!\n * US Government Data http://www.data.gov/\n\n * SF City Data http://datasf.org/  (I personally find local data easy to identify with)\n\n * Reddit/r/DataSets https://www.reddit.com/r/datasets\n\n\n5. Take a role with a product-focused machine learning or personalization team.  The team you search for should be filled with engineers whom you think you can both teach and learn from. This will make you a much better machine-learning engineer. Also, by working on a product team you will quickly learn how the science and theory of machine learning differ from the practice. In particular, how customer behavior will teach you something new every single day.",
                "Great question! How indeed does one prepare oneself for a (research or otherwise) career in machine learning, in particular in terms of familiarizing oneself with the underlying mathematics? I\u2019m going to resist the temptation of trotting out some standard books, and instead, focus on giving broad advice.\n\nThere\u2019s some bad news on this front, and it\u2019s best to get this out of the way as quickly as possible. Having spent 35+ years studying machine learning, let me put this in the most direct way possible: no matter how much time and effort you devote to it, you can never know enough math to read through all the ML literature. Different parts of ML use a variety of esoteric math. There\u2019s just no way one person can know all of this math, so it\u2019s good to be forewarned.\n\nOK, with that out of the way, how does one prepare oneself? Think of the process analogous to conditioning your mind and body to run a marathon. It\u2019s a gradual process, of improving your fitness, your ability to run for longer and longer distances, your breathing technique, your mental focus, and dozens of other dimensions. Working in ML is not like running a 100 meter sprint, where the race is pretty much over in a single breath. It\u2019s much more of an endurance sport, where you have to constantly work at it to remain in shape, and there\u2019s no point at which you can relax and say: OK, I know it all! Because no one does!\n\nAn example from my recent work will clarify the issues involved. One of the major challenges in machine learning is that there\u2019s never enough training data to tackle every ML problem that presents itself. Humans are especially adept in solving this challenge. I can get on a flight from San Francisco and within a few short hours find myself in a dizzying diversity of new environments, from the glitzy subways of Tokyo and the bleak winter in Scandinavia to an arid savannah in Africa, or a swampy rainforest in Brazil. There\u2019s no way I can ever hope to collect training samples from every possible environment that I can encounter in life. So, what do we do? We transfer our learned knowledge from places we\u2019ve been \u2014 so, having taken the BART subway in San Francisco, and subways in New York and London, I can try to handle the complexity of the subway in Tokyo by drawing upon my previous experience. Of course, it doesn\u2019t quite match \u2014 the language is completely different, the tone and texture of the visual experience is completely different (attendants in gloved hands show you the way in Tokyo \u2014 no such luxury is available in the US!). Yet, we somehow manage, and plod our way through new experiences. We even cherish the prospect of finding ourselves in some alien new culture, where we don\u2019t speak the language and can\u2019t ask for directions. It opens up our mind to new horizons, all part of the charm of travel.\n\nSo, what\u2019s the mathematics involved in implementing a transfer learning algorithm? It varies a lot depending on what type of approach you investigate. Let\u2019s review some approaches from computer vision over the past few years. One class of approaches are so-called subspace methods, where the training data from a collection of images in the \u201csource\u201d domain (which conveniently has labels given to us) is to be compared with a collection of unlabeled images from a \u201ctarget\u201d domain (e.g., \u201csource\u201d \u2192 NY subway, \u201ctarget\u201d \u2192 Tokyo subway).\n\nOne can take a collection of images of size NxN and using a variety of different methods find the smallest subspace that the source images lie in (treating each image as a vector in N^2 dimensions). Now, to understand this body of work, you obviously need to know some linear algebra. So, if you don\u2019t understand linear algebra, or you took a class way back when and forgot it all, it\u2019s time to refresh your memory or learn anew. Fortunately, there are excellent textbooks (Strang is usually a good place to start) and also something like MATLAB will let you explore linear algebraic ML methods without having to implement things like eigenvalue or singular value decomposition. As I usually told my students, keep the motto \u201ceigen do it if I try\u201d in mind. Persevere, and keep the focus on why you are learning this math. Because it is important and essential to understand much of modern ML.\n\nOK, great, you\u2019ve managed to learn some linear algebra. Are you done? Ummm, not quite. So, back to our transfer learning example. You construct a source subspace from the source images, and a target subspace from the target images. Umm, how does one do that. OK, you can use a garden variety dimensionality reduction method like principal components analysis (PCA), which just computes the dominant eigenvectors of the covariance matrices of the source and target images. This is one subroutine call in MATLAB. But, PCA is 100 years old. How about something new and cool, like a ooh la la subspace tracking method like GOUDA, which uses the fancier math of Lie groups. Oops, now you need to learn some group theory, the mathematics of symmetry. As it turns out, matrices of certain types, like all invertible matrices, or all positive definite matrices, are not just linear algebraic objects, they are also of interest in group theory, a particularly important subfield of which is Lie groups (Lie \u2192 \u201cLee\u201d).\n\nOK, great, you have a smattering of knowledge of group theory and Lie groups. Are you done? Hmmm\u2026actually not, because it turns out Lie groups are not just groups, but they are also continuous manifolds. What in the blazes is a \u201cmanifold\u201d? If you google this, you are likely to encounter web pages describing engine parts! No, a manifold is something entirely different in machine learning, where it means a non-Euclidean space that has curvature. It turns out the set of all probability distributions (e.g., 1 dimensional Gaussians with a scalar variance dimension and a scalar mean dimension) are not Euclidean, but rather, describe a curved space. So, the set of all positive definite matrices form a Lie group, with a certain curvature. What this implies is that obvious operations like taking the average have to done with considerable care. So, off you go, learning all there is to know about manifolds, Riemannian manifolds, tangent spaces, covariant derivatives, exp and log mappings, etc. Oh, what fun!\n\nGetting back to our transfer learning method, if you compute the source covariance matrix C_s and the target covariance matrix C_t, then there is a simple method called CORAL (for correlational alignment) that figures out how to transform C_s into C_t using some invertible mapping A. CORAL is popular as a transfer learning method in computer vision. But, CORAL does not actually use the knowledge that the space of positive definite matrices (or covariance matrices) forms a manifold. In fact, it forms something called a cone in convex analysis. If you subtract one covariance matrix from another, the result is not a covariance matrix. So, they do not form a vector space, but rather something else entirely. Oops, it turns out the study of cones is important in convex analysis, so there you go again, you need to learn about convex sets and functions, projections onto convex sets, etc. The dividing line between tractable and intractable optimization is not linear vs. nonlinear, but rather, convex vs. non-convex.\n\nI hope the pattern is becoming clear. Like one of those legendary Russian dolls, where each time you open one, you find it is not the end, but there\u2019s another one inside it, so it is with learning math in machine learning. Each time you learn a bit of math, you find it opens the door to an entirely new field of math, which you need to know something about as well. For my most recent paper, I had to digest a whole book devoted entirely to the topic of positive definite matrices (it\u2019s like the old joke, where the deeper you go, the more you know about a specialized topic, until you know everything about \u2014- nothing!).\n\nAny given problem in machine learning, like transfer learning, can be formulated as a convex optimization problem, as a manifold learning problem, as a multivariate statistical estimation problem, as a nonlinear gradient based deep learning problem, etc. etc. Each of these requires learning a bit about the underlying math involved.\n\nIf you feel discouraged, and feel like tearing your hair out at this point, I sympathize with you. But, on the other hand, you can look on the positive side, and realize that in terms of our analogy of running a marathon, you are steadily becoming better at running the long race, building your mathematical muscle as you go along, and gradually things start falling into place. Things start to make sense, and different subfields start connecting with each other. Something strange happens. You start liking it! Of course, there\u2019s a drawback. Someone who doesn't understand any of the math you get good at using asks you to explain your work, and you realize that it\u2019s impossible to do that without writing equations.\n\nMost researchers find their comfort zone and try to stay within it, since otherwise, it takes a great deal of time and effort to master the dozens of mathematical subfields that modern ML uses. But, this strategy eventually fails, and one is always forced to get outside one\u2019s comfort zone and learn some new math, since otherwise, a whole area of the field becomes alien to you.\n\nFortunately, the human brain is an amazing instrument, and provides decades and decades of trouble-free operation, allowing us to continually learn over 40,50, 60, years or more. How precisely it does that without zeroing out all prior learning is one of the greatest unsolved mysteries in science!",
                "Great answers here already:\n\nThe foundation of machine learning (ML) is maths and not data science. So start by polishing up your maths skills. ML currently is a very hot area with many more people trying to learn it but most don't understand that the underlying principles in machine learning are that of optimization theory in maths.\n\nThus to give you a boost revisit the prerequisites.\n\n1. Maths:\n2. \n1. Linear algebra: Make sure you are comfortable with matrices, vectors and singular value decomposition (SVD).\n2. Calculus: Especially differential calculus, become comfortable with evaluation of derivatives of any function and learn chain rule.\n3. Numerical optimization: Like I said above ML is currently more related to numerical optimization than anything else. Most optimization methods are variants of gradient descent such as stochastic gradient descent (SGD). So learn about first order and second order optimization methods.\n4. Statistics and probability: Bayes theorem, random variables, probability distribution functions such as the Binomial and Gaussian distributions. Sigmoid and softmax functions are motivated by probability.\n\n3. Programming: In any of the following:\n4. \n1. \n1. Python: Is an easy to learn scripted language that you can quickly pick up for the purpose of practicing what you are learning. Python has a lot of ML libraries supporting it thus it is ideal not only for beginners but also for experts.\n2. Java: is a fairly high-level language that is fairly easy to learn but not as easy as Python. It also has a lot of ML libraries supporting it.\n3. C/C++: Don't mess with this one at beginner level but start getting used to writing ML code from scratch as you advance so that you can learn more details about most ML algorithms. It is recommended to build your own mini-ML library at some point in your learning journey using such low-level high-performance languages.\n\n\nWith that said, there are several roadmaps to reaching your destination. I can split them up into three stages.\n\n1. Beginner stage.\n2. Intermediary stage.\n3. Advanced (expert) stage.\nBeginner stage:\n\nThe beginning part has been partially covered above, begin with maths. And if you are not good at maths you will need to make sure you are good. Thus make sure to sharpen the axe before starting to chop down the tree. Maths is very important for learning ML as most of the times, systems are expressed in mathematical terms. So before starting, please go through the basics. Don't worry about forgetting something along the way, you can always go back to revisit the stuff you have forgotten, this is not an exam.\n\nThis is where you also have to set yourself up for success in ML, so I suggest you skim through the basic concepts of ML and the book [1] by Ian Goodfellow and others is an excellent book to introduce you to ML and the current hot area in ML called deep learning (DL). Go through the book in any order you like but make sure to read the introduction first.\n\nAt beginner level is also the time to sharpen up another axe, programming. Practice coding on problems that are not even related to ML so that you can learn the syntax of that language. You can't learn coding by reading but through practice. Python being English-like is very easy to pick up and there are a lot of resources out there teaching Python coding. Just Google and you will find high quality tutorials on Python programming. In fact your best way to learn coding is to just jump directly to coding and just Google and Stackoverflow your way through learning the syntax of the language. Every modern programmer owes their project completions to Google and Stackoverflow, let no one lie to you, we are in an era of powerful \u201ccheating\u201d tools like Google search. Don't just copy and paste though, understand the solutions you find online and code your own versions afterwards. If you can't code a matrix operation in Python just Google and try to understand what others did. Sometimes very basic code can be just copied and pasted.\n\nAlso go through the TensorFlow (TF) tutorials as well. They will not only introduce you to TF but also to the concepts of ML such as linear and logistic regression.\n\nIntermediary stage:\n\nAt this stage you should have already read several online sources such as articles, books and watched several YouTube videos on ML and you should have coded up some models at high-level using libraries like TF. Now it is time to start moving forward, you need to make sure that that knowledge does not slip away, you need to consolidate that knowledge in your mind.\n\nThis is where things start to get interesting because you need to now ask yourself, can I implement backpropagation algorithm from scratch? You will notice that you probably wouldn't, that's okay. You need at this point to start pursuing proofs like deriving mathematical expressions you find while reading ML literature. Try to derive backprop for simple neural networks yourself from your own perspective and understanding and see if you can translate that maths to actual working code. Try to also implement convolutional neural networks (CNN) and recurrent neural networks (RNN) from scratch.\n\nThis is where you start to build your mini-ML library. You will be able to learn a great deal of detail about ML this way. Implement stochastic gradient descent and train your own ML models using your own mini-ML library and debug until the models work comparable to those from mature libraries like TF. Open source the mini-ML library afterwards to further show off your skills to potential employers or for the purpose of getting into the Google Brain residency program for example.\n\nYou also need to start reading journal after journal at this point. At first it will be hard but with more reading and rereading you will start to understand even complex journals from the likes of DeepMind, OpenAI, Microsoft, Facebook and Google. If you start to understand journals it means you are advancing well towards your goal.\n\nAdvanced stage:\n\nYes coding your own mini-ML library is sort of like reinventing the wheel but it is essential for learning the details of the most important underlying concepts in machine learning but not sufficient to make you an expert. This advanced stage comes after spending a few years in the intermediary stage.\n\nIn the advanced stage you need to start paying attention to your own intuitions and ideas. Build or start working on actual novel ML algorithms. You will need to empirically or theoretically validate your ideas by implementing them and doing lots of experiments. It is somewhat hard to design a novel ML algorithm as the ideas come, in form of a eureka moment, after years or months of thinking and lots of research work.\n\nThis is why being at an advanced level requires that you have built up a strong mental model of the field of ML from a variety of angles. You don't have to be an expert programmer though, as the coding skills only need to help you implement your models.\n\nAt this stage start asking and pursuing deep questions to which there are no answers yet. Focus on areas that are counterintuitive and try to come up with more novel intuitive solutions to those areas. It is like a PhD program.\n\n\nThat is a possible roadmap for someone trying to learn ML. It is also important to practice by explaining complex ML systems to someone else. Quora is a great place for answering ML related questions, that way you will be able to consolidate your knowledge when explaining the ML algorithms to others, it's a win-win situation, you help others while you gain and consolidate knowledge yourself.\n\nThe other thing worth noting is that you really need to be passionate about the ML field otherwise it won\u2019t be easy. You also need to have tenacity because some concepts take long to understand fully, you may think that you get a concept up until it's time to implement it yourself. Thus learning ML especially by yourself requires serious discipline and focus. You can only maintain that focus if you are passionate and determined to learn ML.\n\nWith that said the journey itself is quite fun, fulfilling and challenging, not hard but challenging just keep going and read anything that interests you concerning ML.\n\nBe passionately curious, you will get there.\n\nHope this helps.\n\n\n1. Deep Learning [ http://www.deeplearningbook.org ]",
                "As others mentioned before, it will mostly boil down to Statistics and Linear Algebra. However, I am very surprised how vague answers are.\n\nThere are lots of different techniques that you can specialise in so I\u2019ll try mentioning some places where you can start. Hopefully, it will make you realise, that there is a lot to cover and how you can start doing that.\n\nLinear Algebra\n\nTopics:\n\n * Vectors & Matrices\n * Matrix Multiplication & Transformations\n * Eigenvector analysis\n * Linear Equation Systems\nGood place to learn:\n\n * Precalculus | Khan Academy [ https://www.khanacademy.org/math/precalculus/ ]\n * Linear Algebra | Khan Academy [ https://www.khanacademy.org/math/linear-algebra ]\n * Linear Algebra [ https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/index.htm ]\n * http://cs229.stanford.edu/section/cs229-linalg.pdf\nCalculus\n\nBasically, you need to be able to do derivative and integration inside out.\n\nTopics:\n\n * Differentiation\n * Chain Rule\n * Partial Derivatives\n * Integrals\nGood places to learn:\n\n * AP Calculus AB | Khan Academy [ https://www.khanacademy.org/math/ap-calculus-ab ]\n * Multivariable Calculus | Khan Academy [ https://www.khanacademy.org/math/multivariable-calculus ]\nStatistics\n\nProbably, the most important thing is to have a pretty decent grasp on probability density and mass functions (PDFs and PMFs) for most of the common distributions (like Gaussian, Binomial, or Exponential family).\n\nIn the end, if you know statistics really well, you can make any machine learning problem into a statistics problem. I would even claim, that the current machine learning developments (apart from deep learning) are just rediscovery of statistics with the computation power of modern computers.\n\nAlso, I will skip things like expected value, variance, and etc. because that\u2019s basics.\n\nTopics:\n\n * Distributions and their CDFs and PDFs:\n * \n * Binomial\n * Poisson\n * Exponential family\n * \n * Gaussian distribution in particular\n\n\n * Calculating CDFs from PDFs\n * Bayes Rule\n * Naive Bayes\n * Markov Chains\n * Belief Networks (or Graphical Models in General)\nGood places to learn:\n\n * Statistics and probability [ https://www.khanacademy.org/math/statistics-probability ]\n * Probability | Statistics and probability | Math | Khan Academy [ https://www.khanacademy.org/math/statistics-probability/probability-library ]\n * Journey into information theory [ https://www.khanacademy.org/computing/computer-science/informationtheory ]\n * http://cs229.stanford.edu/section/cs229-prob.pdf\nOptimisation\n\nAnd finally optimisation. I would say it\u2019s just a part of Calculus (and it would look like that Khan agrees) but whatever. Here you want to be able to find the minimum or either the maximum of some function.\n\nTopics:\n\n * Gradient Descend\n * Stochastic (Online) Gradient Descend\n * Expectation Maximization\nGood places to learn:\n\n * Derivative applications [ https://www.khanacademy.org/math/calculus-home/derivative-applications-calc ]\n * Machine Learning - Stanford University | Coursera [ https://www.coursera.org/learn/machine-learning ] (note: it seems that Andrew Ng is in love with gradient descend so it is covered there pretty well)\nWhat\u2019s next?\n\nThere is a lot of material online for machine learning online but the problem with it is that most of it is very shallow and, honestly, it isn\u2019t worth your time.\n\nHowever, I would go with the following courses:\n\nMachine Learning - Stanford University | Coursera [ https://www.coursera.org/learn/machine-learning ] - A great introductory course to Machine Learning by Andrew Ng. It will cover some statistics and fair amount of linear algebra. Additionally, it exposes you to some decent optimisation basics.\n\nProbabilistic Graphical Models | Coursera [ https://www.coursera.org/specializations/probabilistic-graphical-models ] - Pretty tough course by Daphne Koller. You might also want to check out a book, that the course is based on Probabilistic Graphical Models [ http://pgm.stanford.edu/ ] . The course will give you some decent intro to Bayesian Probabilistic Models.\n\nNeural Networks for Machine Learning - University of Toronto | Coursera [ https://www.coursera.org/learn/neural-networks ] - Neural Nets by Jeff Hinton. If this course is anything as it was before, take it only if you are really interested in neural nets. It makes you do some proper maths, to realise how do those nets actually work. I wouldn\u2019t take it as a beginner.\n\nHowever, that course doesn\u2019t cover some newer development in DeepNets so you might want to see CS224d: Deep Learning for Natural Language Processing [ http://cs224d.stanford.edu/syllabus.html ] or CS231n Convolutional Neural Networks for Visual Recognition [ http://cs231n.github.io ] to cover things like RNNs and LSTMs.\n\nA quick peek\n\nWhat it would take you to solve these?\n\n1. Solve [math]Xw=b [/math] for w where X is data matrix and w is a weight vector and b is a target value (constant).\n2. Calculate Kullback-Leibler divergence between [math]p(x)=N(x|\\mu,\\sigma^2) [/math]and [math]q(x)=N(x|m,s^2)[/math].\n3. Prove [math]p(x,y|z) = p(x|z)p(y|x,z) [/math]\nGiven all of this, you should be good enough to know where to go next.\n\nGood Luck\n\nI am pretty sure I\u2019ve missed some important stuff but I am also sure that this will set you on the right track.\n\nGood luck!",
                "The usual way is to sign up for a bunch of online courses, breeze through some of the videos while skipping the coding assignments and then spending the next few months procrastinating and wondering which deep learning library is better out of TensorFlow, Theano, Torch, Keras and CNTK.\n\nThe best way is to start doing stuff right away. Install Python or R or some other programming language that does machine learning well and start coding. Don\u2019t worry too much about which language or tool is better. Get some data or even generate some random data yourself and implement basic stuff like linear regression and gradient descent. Google how to do stuff. Some of the simpler algorithms can be understood just by reading their Wikipedia pages. If you like what you\u2019re doing, you can sign up for Coursera\u2019s Machine Learning course by Andrew Ng to get an introduction to the concepts and math behind ML. If you know the basics and can successfully implement 3\u20135 algorithms you are already way ahead of most beginners.",
                "No. I wrote my first ML program waaay back in 1982, before there was Internet, Google, GPU computing, laptops, cellphones, digital cameras, desktop PCs, heck before there was almost anything remotely resembling what you see in the tech world around you today.\n\nHow did I even discover the existence of such a field? Back then, to educate oneself, you read books. Of course, you had to either go to a library, or in my case, a quaint event called a book fair. I attended a large book fair in New Delhi, India\u2019s capital, and picked up this 800 page tome, a fairly massive affair. Why, I don\u2019t know. After all, I was at the Indian Institute of Technology, Kanpur, studying to become an electrical engineer.\n\nHofstadter\u2019s first book, and in my opinion, still his best, was an utter revelation to me. It opened up a whole new world of imagination of what deep links there are between art, music and abstract math, realized by the three central characters \u2014 Johann Sebastian Bach, Maurice Escher, Kurt G\u00f6del \u2014 and computing, including of course AI and ML.\n\nThe book featured an intriguing set of visual puzzles from a Russian researcher named Bongard, where the task is to discover a rule that separates the six figures on the left from those on the right. This is an elementary problem in ML called classification. It\u2019s analogous to distinguishing email from spam or deciding if an image contains a face. As humans, we classify sensory stimuli billions of times through our lives, and our very survival depends on it. As you cross the road, is the object approaching you a person or a Fedex truck? Get the answer wrong and your life may indeed be over. Not surprisingly, we solve such problems amazingly well.\n\nI\u2019ll leave you to work this one out, but with absolutely no training in this field, I nonetheless decided to foolishly make this the core of my Masters thesis project. Somehow I plodded through and worked out a solution, however naively it seems in retrospect. That experience made me realize that AI and ML was my life\u2019s goal, and I decided to come to the United States in 1983, where I was incredibly fortunate to work with this brilliant Stanford educated researcher, Thomas Mitchell, now the Dean of the School of Computer Science at Carnegie Mellon University.\n\nFrom Tom, I learned the most important lesson of all, which no book can teach you. Research is *fun*. He simply embodied the spirit of a researcher who bubbled with enthusiasm for the field of ML. He worked harder than anyone I had met, yet seemed to be having a ball. That lesson made a huge imprint on me and stayed with me ever since.\n\nAfter getting my PhD, I joined IBM Watson Research in New York in late 1989, where they couldn\u2019t figure out what to do with an ML researcher. So they threw me into a newly formed robotics group, even though I had absolutely no background in this field. I had never ever programmed a robot. Amazingly enough, I seemed to thrive in this somewhat challenging situation, and ended up writing some of my most highly cited papers, exploring how robots can acquire new behaviors using the newly emerging field of reinforcement learning. I also published in 1993 perhaps the first book on robot learning, which featured research from all over the world in this new area of AI. Despite having no background in robotics, I still managed to make a name for myself.\n\nMany years later, I was elected a Fellow of AAAI, the leading international professional society for AI researchers. Each year, a small handful of researchers are selected and the competition is fierce. This year\u2019s AAAI Fellows include some of the founders of the deep learning revolution: Yoshua Bengio, and Yann Le Cun.\n\nThe list of AAAI Fellows include some of the most amazing researchers in AI and ML, and I\u2019m humbled to be listed in such distinguished company. None of this would have happened if back in 1982, I thought doing ML with no formal training in this field, with primitive computing, or doing robot learning at IBM in 1990 with no training in robotics, was \u201cdifficult\u201d.\n\nFor those aspiring young researchers reading this on Quora, the best advice I can give you is that nothing is \u201cdifficult\u201d if you set yourself the challenge of working on it. Above all, remember: research is fun! It\u2019s an exploration into the unknown.\n\nFor many years, from 2001\u20132018, I was privileged to co-direct the Autonomous Learning Laboratory at the University of Massachusetts, Amherst, with one of reinforcement learning\u2019s true pioneers, Andrew Barto. Andy and his former PhD student, Rich Sutton, helped establish the modern field of RL, the area that gave rise to Deep Mind and Alpha Go Zero. Andy and Rich embodied the true spirit of researchers having fun and working with them was the best professional experience of my career.\n\nPhD students at the lab hung up a sign on the main door that was a quote from one of the most distinguished scientists of all time, Albert Einstein.\n\nThat sums it all. Research doesn\u2019t need expertise. Einstein in fact hated textbook knowledge. Above all, he prized imagination, the ability to dream. If you want to make your children smart, he told parents, teach them fairy stories.\n\nAs we battle the latest pandemic, the Wuhan coronavirus, the biggest weapon at our disposal is our ability to sequence its genome. The biggest breakthrough in biology of the 20th century came from Watson and Crick, two brash biologists who upturned the world of biology by having fun! Watson went on to write a highly popular account of their discovery called the Double Helix. In it he tells the story of how they scandalized established researchers, like Oswald Avery of Columbia University, when he realized they didn\u2019t know elementary biochemistry. Yet, by playing with 3D models and in effect stealing from Rosalind Franklin\u2019s carefully gathered data sets, they cracked the secret of life. They were simply having fun!\n\nSo, again, my answer is, no, ML is not difficult. It is fun!",
                "Hi there,\n\nFirst of all welcome to the world of data science. I would share my experience when I was at your stage. First will talk about the mistakes and then, how to fix them.\n\na. If you are at a stage where your just completed few tutorials in R programming, or data science courses in in Udemy or Coursera, stop before you feel like let's participate in Kaggle or other such competitions. Mistake 1. Kaggle competition in most cases doesn't teach you data science. Yes it does tell \"it's home for data science, but it's not a good host\", just kidding.\n\nb. Do not tell yourself that if you know how to apply logistic regression, or PCA or K-means clustering et.al [ http://et.al ]. machine learning (ML) algorithms, you will become Data Scientist. No. Stop. Mistake 2.\n\nC. Not trying to ask questions(high level, theoretical and oriented towards business problem and how doing data science would bring answers to those questions. If you are not starting from this way, guess what, you are doing mistake 3. and we can go on with this list.\n\nNow, let's fix them.\n\na. Yes, you definitely need a tool to experiment, you can pick R which is fine. Others pick Python or Clojure or Weka etc, and some still use Java and Javascript. It depends. So, make sure you are comfortable with R functions such as lapply, tapply, mapply, sapply, as you are going to need them daily. You also practice how to read data from thousand of files sequentially, or from SQL database or from CSVs and try some NoSQL databases. This is totally \"not data science\". This is just an exercise to use R (any tool you pick) tool for data ingestion, processing and make them ready for your analysis. I initially started with Python, and after two years too R and just in a month I fell in love with R programming. I now prefer to use R.\n\nb. Start reading a book which teaches you Statistics using R (again pick any tool you like). Why, because even before you get to apply those fancy ML algos, you will need to understand what your data represents, how are they distributed, how do they relate to each other, do they have missing values, how do you find or evaluate whether those variables are of any use to you, should you remove them or if you remove them are you going to lose any information, etc. You will get even more questions. So statistics. Very important. Still \"no data science.\"\n\nC. Once you have basic understanding of R(or any tool) and Statistics, pick the most simple machine learning algorithm i.e. \u201cLinear Regression\u201d. Read it first from statistical point of view, and then how to apply in R and why and when would you use Linear Regression. This will give your first glimpse of how to apply a ML algo to a data set. Then add one more algorithm, may be logistic regression, and so on. Do not rush, as you would need to make sure you very well know what are the strong and weak points of your algorithm. This is still \"not data science\"\n\nD. Once you have reached this stage. Pick a case study. Now this could be scary, but its ok. Here you immediately notice mix of statistics, ML algos, programming and function been built to several things. Just stick to it, and cover one topic or section at a time.\n\nYou will find tons of case studies online. I would suggest pick a case study on Fraud detection or recommendation systems. This is an area, which every industry has a problem with in this digital world.\n\nNow comes, data science. Say you work for an eCommerce company and your colleague or managers asks you, can you detect fraudulent transactions from the entire transaction history and would you be able to predict it, given that historical transactions are already reviewed by \"domain experts\" and termed them either fraud transaction or non fraud. Well in real world, not all data would be clearly termed. Now, given such a scenario, you would need to do data processing first format the data in such a way that you ML algo understand, in the processing stage you do not want to lose any useful information so you have put extra measure, manual analysis and data analysis utilizing your statistics knowledge, work experience, domain experience etc. Hence you would spend 80% of your time cleaning the data and preparing them. Rest 20% would be to apply ML and deploying the solution.\n\nAfter all of these, if you can answer the question above based on the data, which could save your company/clients/customers millions of dollars from the fraudulent transactions or getting new clients based on the products you have recommended, that's when you have applied all your knowledge and expertise to do data science.\n\nI thought this might help you see what to expect from this path. Hope this answers your question.",
                "A lot of answers here are talking about Coursera online courses, I would probably say that it is cool to do online courses but I didn't learn machine learning (ML) that way, I learned by doing, practicing the stuff for years continuously even to this very day I still code up some ML algorithms and slowly growing my own library for ML and computer vision.\n\nI read a lot of journals i.e the recent interesting paper from Google DeepMind WaveNet  [ https://arxiv.org/pdf/1609.03499.pdf ]and over the past 6+ years I have read a lot of journals covering a lot of algorithms in ML from k-means clustering [ https://en.wikipedia.org/wiki/K-means_clustering ] and hierarchical clustering [ https://en.wikipedia.org/wiki/Hierarchical_clustering ] through support vector machine [ https://en.wikipedia.org/wiki/Support_vector_machine ] to deep learning [ https://en.wikipedia.org/wiki/Deep_learning ] algorithms. I normally read a single journal 8 or so times before understanding the underlying stuff. I also complement the journals by watching YouTube presentation videos such as\n\nhttps://www.youtube.com/watch?v=UzxYlbK2c7E\nor referencing to Wikipedia or other sources such as answers on Quora.\n\nI am mainly self-driven thus I can set goals and pursue them even without a mentor I get things done. For example at some point I had little understanding of the underlying backpropagation algorithm but when I set a goal to understand it I was able to do that just by using a Wikipedia article on backpropagation [ https://en.wikipedia.org/wiki/Backpropagation ] and in one day I was even able to code my own backprop algorithm to optimize my very own neural networks in my library. I recycle a lot of knowledge to avoid wasting time and I learn by first principal.\n\nMy self-studies are triggered by curiosity, I would first become curious when I encounter a particular technology then I would think about the possibilities by brainstorming then I would identify the stuff I need to understand in order to fully grasp the technology at hand. I would then take a few days or weeks depending on the complexity of the problem at hand researching the technology and to be able to code up the stuff, I normally use my own library to speed up the coding of the algorithms which I have been building upon for some years now.\n\nThus the methodology I use is an engineering approach to solving problems, the answers here are suggesting a more scientific approach which is good for most people I guess, on my part the engineering methodology helps me learn more practical stuff because I learn ML and computer vision for practical reasons because I am building a robot operating system. I didn't learn ML or computer vision to get a job but just to build complex systems like I am working on a very advanced vision system called IRIS-(integrated recognition and inference system) to process visual stimuli using novel learning algorithms. The IRIS system so far learns using very few training examples and it is designed to achieve one-shot learning via a complex form of transfer learning which I designed but I am still doing finishing touches on it plus run several experiments.\n\nOn your part since you are just beginning, you need to know some maths such as linear algebra, numerical optimization, differential calculus and statistical analysis. I personally know those stuff because I am an electronics engineer by profession, so if you are starting from scratch completely make sure to know your maths and learn coding as well such as in Python, Java and C/C++. Then start reading journal after journal and practice a lot by coding stuff from scratch, it will take you some time to start getting used to this routine but it will make you one of the best in the field.\n\nIt is okay you can also take online courses as suggested by most answers here such as Coursera or others that is up to you but you need to be passionate and curious in order to learn such complex stuff on your own. You also need to be open-minded and learn as you go.\n\nHere is an example of the many journals I have read: NOTE: I read based upon the task I am currently working on so all these journals will show you a snapshot of the complexity of the stuff I am currently working on.\n\n1. Reciprocal nearest neighbor clustering [ https://pdfs.semanticscholar.org/adac/55688e733c3c91a7e12a15e13db677789b62.pdf ]\n2. Deep face [ https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf ]\n3. Sequential minimal optimization for SVMs [ http://web.iitd.ac.in/~sumeet/tr-98-14.pdf ]\n4. R-CNN [ https://people.eecs.berkeley.edu/~rbg/papers/pami/rcnn_pami.pdf ]\n5. SIFT [ http://www.inf.fu-berlin.de/lehre/SS09/CV/uebungen/uebung09/SIFT.pdf ]\n6. Computer Vision: Algorithms and Applications [ http://szeliski.org/Book/ ] check the pdf draft here [ http://szeliski.org/Book/drafts/SzeliskiBook_20100903_draft.pdf ]\nHope this helps.",
                "Machine Learning is an application of Artificial Intelligence and is revolutionizing the way companies do business. At it\u2019s core, it\u2019s an algorithm or model that learns patterns in big data and then predicts similar patterns in new data. In layman\u2019s terms, it\u2019s the theory that machines should be able to learn and adapt through experience to produce reliable, repeatable decisions and results.\n\nWhile ML isn\u2019t new, theres now more data than ever which has attributed to it\u2019s recent popularity.\n\nSo, where have you seen Machine Learning in your everyday life?\n\n-You know those show/movie recommendations you get on Netflix? Machine learning\n\n-Ever get a call or text from your bank regarding what they believe to be a fraudulent charge? Machine learning\n\n-The self-driving Google car? Machine learning\n\nI\u2019m in the Project Management software [ http://taskreports.com ] space, so this is a very exciting time as many companies, such as ClickUp, are currently testing Machine Learning [ https://clickup.com/blog/machine-learning-ml-vs-artificial-intelligence-ai-whats-the-difference/ ]. By incorporating ML into PM tools, the software will be able to predict which actions an individual is likely to take.\n\nAutomated actions, resource planning, and smarter decisions are just a few of the benefits we are looking forward to in my industry.\n\nThis will totally shift the way projects are run and have a positive impact on the performance of all human kind!\n\n(I love their graphics)",
                "The classical definition is that it is the study of algorithms that infer the function they compute from example data. An example of this is the \"Theory of the Learnable\" paper which is arguably the seminal machine learning publication (http://portal.acm.org/citation.cfm?id=1972). In this sense it is meant to be computer science's answer to induction.\n\nThis definition is probably too narrow now, as the idea of inferring a function from examples makes the most sense with classification or regression problems and makes less sense with clustering or other problems.\n\nHistorically, machine learning was something of a reaction within artificial intelligence research. AI focused heavily on logic rather than probability or statistics, and preferred heuristic and search to formal optimization. It was also a fairly open ended research program in which it is relatively hard to judge progress. The popularity of machine learning in academia was probably due to the lack of success and growing skepticism people had of AI research in the late 80s. Machine learning is, in comparison, an extremely well defined area focusing on concrete algorithmic and mathematical problems. Focusing on better defined problems with concrete measures of progress probably helped researchers insulate themselves from the discouragement with AI generally (and lack of funding).\n\nIn actual practice, I think machine learning is just a combination of mathematical optimization and statistics, both academic disciplines in their own right. Machine learning is currently riding something of a popularity wave. My belief is that this is because machine learning is a part of computer science, and hence its practitioners are highly trained computer programmers. \nThis has allowed applications of machine learning in data intensive areas like voice recognition, search, social network analysis, recommendation algorithms, computer vision, etc. Basically the machine learning people got the cool applications and hence the funding. They also had the freedom to ignore some of the culture baggage of traditional statistics, in no small part due to ignorance of it.\n\nThe current state of the field is that algorithms exist that can be used to solve a single well-defined problem, often better than humans can (for example creating a classifier to recognize spam). However significant work generally needs to be done to get data into a form that is algorithmically usable. Furthermore, these algorithms do not match people's expectation of human intelligence in that they learn a single thing very precisely, but can't learn more than one thing (the same instance of the classifier that predicts spam, won't also predict the stock market). Hence it is not possible for a program to learn like a human learns, continually acquiring new concepts and ideas and understanding the relationship between them.\n\nSo we are safe from the machines for a little longer.",
                "Machine learning works by finding a function, or a relationship, from input X to output Y.\n\nHuh?\n\nAlright, let\u2019s start from the basics.\n\n\nWhat is machine learning?\n\nThe high level and most commonly accepted definition is: machine learning is the ability for computers to learn and act without being explicitly programmed.\n\nFor example, let\u2019s say you want to program a program to play a game- chess (or Go). There are two fundamentally different ways to go about programming such a bot.\n\n1. The first approach is to explicitly program the instructions your bot will take. If the board is in a certain configuration (state), move your knight to B4. Or if the board is in another state, move your pawn to C4. This hard-coded method isn\u2019t particularly practical or appealing since it is near-impossible for the programmer to consider all of the possible scenarios beforehand.\n2. The second approach is to simply program your bot with the rules of chess. Then, let the bot play thousands of games and let it decide which moves to make given a certain configuration. You can reward it when it wins or punish it when it loses, so it \u201clearns\u201d which moves are \u201cgood\u201d by rewarding them.\nFor the game of chess, the input would be a board configuration (X) and the output would be your bot\u2019s next move (Y).\n\nHopefully, after training, your bot will have found the function that outputs the best move (Y), given a certain game configuration (X). Remember that a function is just a relationship between an input and output.\n\nHowever, the key takeaway is that the bot is not hard-coded to output anything. All the moves it makes is \u201clearned\u201d through training.\n\n\nThe chess example above falls under Reinforcement Learning, which is a branch of Machine Learning. The other branches include Supervised and Unsupervised Learning.\n\nSupervised learning is when you have knowledge of the input (X) and the output (Y), then you \u201csupervise\u201d the program in predicting the right outcome via trial and error. An example would be predicting house prices:\n\n * Input X: 10,000 houses- you know the area, city, and age of each house\n * Output Y (known): 10,000 house prices\nYou want to find the function that can most accurately predict the house price (Y) given the various inputs (X).\n\n\nUnsupervised learning is when you have zero knowledge of the output and you want to try to find patterns or groupings within the data. Think clustering:\n\n * Input X: a city\n * Output Y(unknown): most densely populated areas\nYou want to find the mapping from a city (X) to the most densely populated areas (Y).\n\nSome more examples can be found in the picture below.\n\n\nConclusion\n\nAt the core of every machine learning algorithm, all the algorithm is trying to do is find a relationship from the input X to the output Y. How exactly it finds this function varies by the algorithm.\n\nBut almost all algorithms require a lot of computations. And a lot of revisions. And usually a good amount of data.\n\nFor a basic example, I point you here [ https://www.quora.com/How-does-machine-learning-work/answer/Lyken-Syu-1 ].\n\nHappy learning.\n\nP.S. As a side note, one of the main reasons Deep Learning has been doing extremely well is that Neural Networks have been proven to be an Universal Function Approximater, meaning that it can approximate any function between X or Y- given enough data of course.",
                "We can answer this question as follows:\n1. Come up with some algorithms that involve machine learning, and some that do not.\n2. Train a classifier using this initial set of data.\n3. For any new algorithm, the classifier will tell us whether it uses machine learning.\n\nThis classifier may not be 100% accurate, but such are the limitations of machine learning.",
                "Hey,\n\nMachine Learning is the foundation for most AI solutions. AI helps us meet the needs of today, so we\u2019re prepared for tomorrow. it helps us to reduce waste in an agricultural area. Sustainable farming techniques are essential to maximizing food production while protecting a fragile environment.\n\nData scientists can use all of that data to train machine learning models that can make predictions and inferences based on the relationships they find in the data. There are three machine learning types: supervised, unsupervised, and reinforcement learning.\n\nMachine learning is used in internet search engines, email filters to sort out spam, websites to make personalized recommendations, banking software to detect unusual transactions, and lots of apps on our phones such as voice recognition.\n\nI\u2019m inviting you to follow the WnCD Internship group. As a follower, you\u2019ll receive personalized updates on content from the Space related to webinars, internships, and course discussions.\n\nhttps://wncdgroupinternshipgroup.quora.com/\nThank You!",
                "A Little Intro:\n\nOne of the thriving areas of artificial intelligence is machine learning. Machine learning had existed long before I was born, so why has it gained such traction after nearly 50 years? We now have access to an enormous amount of data, whether as images, text, audio, or video\u2014this data power machine learning.\n\nThere are two schools of thought when it comes to Learning,\n\n * learning by example, humans learn everything by example.\n * Humans learn due to innate capabilities that yet need figuring.\nThe first thought was adopted to train machines using past data /examples/ samples, and that's the basis of the current state of the art for machine learning.\n\nMachine learning basics:\n\nMachine learning uses the same idea as human Learning to teach machines to learn from past data and perform tasks at a much faster pace than humans. Let's use an example to illustrate how it happens. So relax and read; I'll use a famous example. Assume you want to discover what kind of song a person likes or dislikes based on several parameters such as voice gender, genre, tempo, mood, and personal preference. Let's say this person likes rapid, soaring music and dislikes light, calm music. Feeding the machine various samples of his/her likes and dislikes, the machine can determine whether or not the next song I choose is one of his/her preferences.\n\nAs I have trained the machine with examples for his likes and dislikes based on tempo and intensity, now if I ask if he likes \u201cx\u201d song? The machine here choosing an algorithm (K-nearest neighbor) decides that \u201cx\u201d is a song that this person \"likes\"( as its nearest neighbors lie more in the like group).\n\nThat was something that everyone could see while listening to their favorite music on Spotify, Gaana, or any other music streaming service. It uses machine learning to recommend a song based on the player\u2019s song selection.\n\nWays you can make machines learn:\n\nSupervised Learning: In my previous example, I could train my machine with samples in which I've already stated that the person \"likes\" and \"dislikes\" certain songs, i.e., I've noted a decision with each instance. Supervised Learning is when the decision/conclusion associated with the set of parameters is already known. In supervised Learning, these decisions are usually called labels. We fed the machine with labeled data.\n\nUnsupervised Learning: You train your machine with different parameters here, but there is no decision associated with it. It does grouping using various techniques(algorithms) to assist humans in making decisions regarding multiple groups. In unsupervised learning, we fed the machine with unlabeled data.\n\nReinforcement learning: This is reward-based Learning that works on feedback. We give positive feedback for each correct action performed by the machine, while negative feedback for each erroneous action. Here again, we make use of Unlabeled data.\n\nIs ML currently overhyped?\n\n * A \"No\" ML is not overhyped if you can comprehend its practical limitations. Minds that feel appealed towards ML should study and explore what benefits it can accomplish, for ML to succeed.\n * And a \"Yes\" ML is overhyped when your expectations are projected to boundaries beyond practicality. Thanks to marketing geniuses and movies for misleading people about what ML actually offers.\nThere is a distinction to be made between what a few have promised and what ML can realistically deliver. After all, it's not a genie but a well-understood field with enormous potential for any industry that seeks to benefit from its practical applications.",
                "Think of a day when the sky is full of dark clouds and thunderstorms. The 1st thing that come to your mind is Its going to rain today.\n\nHow did you know that it's going to rain?\n\nYou know it because, in your life, whenever you have seen the sky behaving the same then it has rained. That's what Machine Learning is.\n\n\n%3E Machine Learning is the domain of computer science(and the sub-part of Artificial Intelligence) that uses statistical analysis/techniques to give computers the ability to think according to the previous data provided to it.\nTalking technically, there is a huge database containing data of a particular domain of few years(5\u201310 years). A particular algorithm is made to run on this data that does data analysis, to recognize the hidden pattern in the data. When further such a same situation arises then the computer gives the solution that fall in the specific range of the pattern recognised.\n\nAny technology user today has benefitted from machine learning. Facial recognition technology allows social media platforms to help users tag and share photos of friends. Optical character recognition (OCR) technology converts images of text into movable type. Recommendation engines, powered by machine learning, suggest what movies or television shows to watch next based on user preferences. Self-driving cars that rely on machine learning to navigate may soon be available to consumers.\n\nThank you.",
                "Machine learning works by finding a function, or a relationship, from input X to output Y.\n\nHuh?\n\nAlright, let\u2019s start from the basics.\n\n\nWhat is machine learning?\n\nThe high level and most commonly accepted definition is: machine learning is the ability for computers to learn and act without being explicitly programmed.\n\nFor example, let\u2019s say you want to program a program to play a game- chess (or Go). There are two fundamentally different ways to go about programming such a bot.\n\n1. The first approach is to explicitly program the instructions your bot will take. If the board is in a certain configuration (state), move your knight to B4. Or if the board is in another state, move your pawn to C4. This hard-coded method isn\u2019t particularly practical or appealing since it is near-impossible for the programmer to consider all of the possible scenarios beforehand.\n2. The second approach is to simply program your bot with the rules of chess. Then, let the bot play thousands of games and let it decide which moves to make given a certain configuration. You can reward it when it wins or punish it when it loses, so it \u201clearns\u201d which moves are \u201cgood\u201d by rewarding them.\nFor the game of chess, the input would be a board configuration (X) and the output would be your bot\u2019s next move (Y).\n\nHopefully, after training, your bot will have found the function that outputs the best move (Y), given a certain game configuration (X). Remember that a function is just a relationship between an input and output.\n\nHowever, the key takeaway is that the bot is not hard-coded to output anything. All the moves it makes is \u201clearned\u201d through training.\n\n\nThe chess example above falls under Reinforcement Learning, which is a branch of Machine Learning. The other branches include Supervised and Unsupervised Learning.\n\nSupervised learning is when you have knowledge of the input (X) and the output (Y), then you \u201csupervise\u201d the program in predicting the right outcome via trial and error. An example would be predicting house prices:\n\n * Input X: 10,000 houses- you know the area, city, and age of each house\n * Output Y (known): 10,000 house prices\nYou want to find the function that can most accurately predict the house price (Y) given the various inputs (X).\n\n\nUnsupervised learning is when you have zero knowledge of the output and you want to try to find patterns or groupings within the data. Think clustering:\n\n * Input X: a city\n * Output Y(unknown): most densely populated areas\nYou want to find the mapping from a city (X) to the most densely populated areas (Y).\n\nSome more examples can be found in the picture below.\n\n\nConclusion\n\nAt the core of every machine learning algorithm, all the algorithm is trying to do is find a relationship from the input X to the output Y. How exactly it finds this function varies by the algorithm.\n\nBut almost all algorithms require a lot of computations. And a lot of revisions. And usually a good amount of data.\n\nFor a basic example, I point you here [ https://www.quora.com/How-does-machine-learning-work/answer/Lyken-Syu-1 ].\n\nHappy learning.\n\nP.S. As a side note, one of the main reasons Deep Learning has been doing extremely well is that Neural Networks have been proven to be an Universal Function Approximater, meaning that it can approximate any function between X or Y- given enough data of course.",
                "First I should point out that it's impossible to generalize because there are many different types of machine learning and they all work a little differently. Machine learning is a very broad term and simply means methods of giving a computer known input-output relations and getting it to predict the output given new, unknown inputs. If I'm honest, when I first learned how a neural network worked, I was a little disappointed because there's no magic: it's just a generalization of linear regression fitting. Most machine learning techniques are essentially statistical in nature.\n\nHere I will explain how a neural network works because it is one of the most common and basic methods. First, you have a set of known, input-output relations. To express this mathematically, you could say that [math]\\vec x_i \\rightarrow y_i[/math] where i goes from 1 to n. Note that the inputs are vectors whereas the output is scalar. To deal with vector outputs, you can train multiple models. The above mapping is called the training data.\n\nNext, you have a function that takes an input in the same form and returns an output, also in the same form. This function should have two properties: it is nonlinear and it has a series of coefficients which can be varied so as to emulate a variety of other functions. Again, expressing this mathematically:\n\n[math]y=f(\\vec x,~\\vec k)[/math]\n\nwhere k is a vector of coefficients. We need to vary these coefficients so that the function f, or model, returns y values as close to those in the above mapping, given the corresponding input x values, as possible. Thus we need to train it by varying the coefficients and feeding both the results from f and the training data to a cost function which measures the goodness of fit.\n\nLets express the cost function as follows:\n\n[math]C [ f(\\vec x_i,~\\vec k),~y_i ][/math]\n\nSo we are feeding both the results of the model, given the inputs, and the known outputs to C and getting back only a single result which measures the goodness of fit. Note that we need to feed all of the training data to the cost function, thus it has 2*n parameters. We need to minimize C with respect to the coefficients, k. This is typically expressed as follows:\n\n[math]\\underset{\\vec k}{\\min} C [ f(\\vec x_i,~\\vec k),~y_i ][/math]\n\nThe most common form of C is a sum of squares:\n\n[math]C=\\sum_{i=1}^n \\left [f(\\vec x_i) - y_i \\right ]^2[/math]\n\nMaking this a least squares problem. For brevity, I've omitted the coefficients from f.\n\nBecause f is non-linear, there is usually no closed-form solution to the minimization problem, and we must solve it by an iterative, numerical algorithm. I won't go into much detail about minimization algorithms because this is a huge and diverse field, however to get a flavour of how these things work, you might want to look up Newton's method, which is easy to generalize to multiple dimensions. Another important method is gradient descent.\n\nThere are no good, general methods for multidimensional minimization and it is impossible to guarantee a global minimum except for a narrow subset of well-behaved problems. Stable methods are typically slow while faster methods tend to be unstable, so many of the best methods will combine a slow, stable method with a fast, unstable one, e.g. when things are going well, take a Newton step, while if the solution veers off, switch to gradient descent.",
                "As others mentioned before, it will mostly boil down to Statistics and Linear Algebra. However, I am very surprised how vague answers are.\n\nThere are lots of different techniques that you can specialise in so I\u2019ll try mentioning some places where you can start. Hopefully, it will make you realise, that there is a lot to cover and how you can start doing that.\n\nLinear Algebra\n\nTopics:\n\n * Vectors & Matrices\n * Matrix Multiplication & Transformations\n * Eigenvector analysis\n * Linear Equation Systems\nGood place to learn:\n\n * Precalculus | Khan Academy [ https://www.khanacademy.org/math/precalculus/ ]\n * Linear Algebra | Khan Academy [ https://www.khanacademy.org/math/linear-algebra ]\n * Linear Algebra [ https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/index.htm ]\n * http://cs229.stanford.edu/section/cs229-linalg.pdf\nCalculus\n\nBasically, you need to be able to do derivative and integration inside out.\n\nTopics:\n\n * Differentiation\n * Chain Rule\n * Partial Derivatives\n * Integrals\nGood places to learn:\n\n * AP Calculus AB | Khan Academy [ https://www.khanacademy.org/math/ap-calculus-ab ]\n * Multivariable Calculus | Khan Academy [ https://www.khanacademy.org/math/multivariable-calculus ]\nStatistics\n\nProbably, the most important thing is to have a pretty decent grasp on probability density and mass functions (PDFs and PMFs) for most of the common distributions (like Gaussian, Binomial, or Exponential family).\n\nIn the end, if you know statistics really well, you can make any machine learning problem into a statistics problem. I would even claim, that the current machine learning developments (apart from deep learning) are just rediscovery of statistics with the computation power of modern computers.\n\nAlso, I will skip things like expected value, variance, and etc. because that\u2019s basics.\n\nTopics:\n\n * Distributions and their CDFs and PDFs:\n * \n * Binomial\n * Poisson\n * Exponential family\n * \n * Gaussian distribution in particular\n\n\n * Calculating CDFs from PDFs\n * Bayes Rule\n * Naive Bayes\n * Markov Chains\n * Belief Networks (or Graphical Models in General)\nGood places to learn:\n\n * Statistics and probability [ https://www.khanacademy.org/math/statistics-probability ]\n * Probability | Statistics and probability | Math | Khan Academy [ https://www.khanacademy.org/math/statistics-probability/probability-library ]\n * Journey into information theory [ https://www.khanacademy.org/computing/computer-science/informationtheory ]\n * http://cs229.stanford.edu/section/cs229-prob.pdf\nOptimisation\n\nAnd finally optimisation. I would say it\u2019s just a part of Calculus (and it would look like that Khan agrees) but whatever. Here you want to be able to find the minimum or either the maximum of some function.\n\nTopics:\n\n * Gradient Descend\n * Stochastic (Online) Gradient Descend\n * Expectation Maximization\nGood places to learn:\n\n * Derivative applications [ https://www.khanacademy.org/math/calculus-home/derivative-applications-calc ]\n * Machine Learning - Stanford University | Coursera [ https://www.coursera.org/learn/machine-learning ] (note: it seems that Andrew Ng is in love with gradient descend so it is covered there pretty well)\nWhat\u2019s next?\n\nThere is a lot of material online for machine learning online but the problem with it is that most of it is very shallow and, honestly, it isn\u2019t worth your time.\n\nHowever, I would go with the following courses:\n\nMachine Learning - Stanford University | Coursera [ https://www.coursera.org/learn/machine-learning ] - A great introductory course to Machine Learning by Andrew Ng. It will cover some statistics and fair amount of linear algebra. Additionally, it exposes you to some decent optimisation basics.\n\nProbabilistic Graphical Models | Coursera [ https://www.coursera.org/specializations/probabilistic-graphical-models ] - Pretty tough course by Daphne Koller. You might also want to check out a book, that the course is based on Probabilistic Graphical Models [ http://pgm.stanford.edu/ ] . The course will give you some decent intro to Bayesian Probabilistic Models.\n\nNeural Networks for Machine Learning - University of Toronto | Coursera [ https://www.coursera.org/learn/neural-networks ] - Neural Nets by Jeff Hinton. If this course is anything as it was before, take it only if you are really interested in neural nets. It makes you do some proper maths, to realise how do those nets actually work. I wouldn\u2019t take it as a beginner.\n\nHowever, that course doesn\u2019t cover some newer development in DeepNets so you might want to see CS224d: Deep Learning for Natural Language Processing [ http://cs224d.stanford.edu/syllabus.html ] or CS231n Convolutional Neural Networks for Visual Recognition [ http://cs231n.github.io ] to cover things like RNNs and LSTMs.\n\nA quick peek\n\nWhat it would take you to solve these?\n\n1. Solve [math]Xw=b [/math] for w where X is data matrix and w is a weight vector and b is a target value (constant).\n2. Calculate Kullback-Leibler divergence between [math]p(x)=N(x|\\mu,\\sigma^2) [/math]and [math]q(x)=N(x|m,s^2)[/math].\n3. Prove [math]p(x,y|z) = p(x|z)p(y|x,z) [/math]\nGiven all of this, you should be good enough to know where to go next.\n\nGood Luck\n\nI am pretty sure I\u2019ve missed some important stuff but I am also sure that this will set you on the right track.\n\nGood luck!",
                "That it is basically just glorified curve fitting at its core ;)\n\n",
                "Great question! How indeed does one prepare oneself for a (research or otherwise) career in machine learning, in particular in terms of familiarizing oneself with the underlying mathematics? I\u2019m going to resist the temptation of trotting out some standard books, and instead, focus on giving broad advice.\n\nThere\u2019s some bad news on this front, and it\u2019s best to get this out of the way as quickly as possible. Having spent 35+ years studying machine learning, let me put this in the most direct way possible: no matter how much time and effort you devote to it, you can never know enough math to read through all the ML literature. Different parts of ML use a variety of esoteric math. There\u2019s just no way one person can know all of this math, so it\u2019s good to be forewarned.\n\nOK, with that out of the way, how does one prepare oneself? Think of the process analogous to conditioning your mind and body to run a marathon. It\u2019s a gradual process, of improving your fitness, your ability to run for longer and longer distances, your breathing technique, your mental focus, and dozens of other dimensions. Working in ML is not like running a 100 meter sprint, where the race is pretty much over in a single breath. It\u2019s much more of an endurance sport, where you have to constantly work at it to remain in shape, and there\u2019s no point at which you can relax and say: OK, I know it all! Because no one does!\n\nAn example from my recent work will clarify the issues involved. One of the major challenges in machine learning is that there\u2019s never enough training data to tackle every ML problem that presents itself. Humans are especially adept in solving this challenge. I can get on a flight from San Francisco and within a few short hours find myself in a dizzying diversity of new environments, from the glitzy subways of Tokyo and the bleak winter in Scandinavia to an arid savannah in Africa, or a swampy rainforest in Brazil. There\u2019s no way I can ever hope to collect training samples from every possible environment that I can encounter in life. So, what do we do? We transfer our learned knowledge from places we\u2019ve been \u2014 so, having taken the BART subway in San Francisco, and subways in New York and London, I can try to handle the complexity of the subway in Tokyo by drawing upon my previous experience. Of course, it doesn\u2019t quite match \u2014 the language is completely different, the tone and texture of the visual experience is completely different (attendants in gloved hands show you the way in Tokyo \u2014 no such luxury is available in the US!). Yet, we somehow manage, and plod our way through new experiences. We even cherish the prospect of finding ourselves in some alien new culture, where we don\u2019t speak the language and can\u2019t ask for directions. It opens up our mind to new horizons, all part of the charm of travel.\n\nSo, what\u2019s the mathematics involved in implementing a transfer learning algorithm? It varies a lot depending on what type of approach you investigate. Let\u2019s review some approaches from computer vision over the past few years. One class of approaches are so-called subspace methods, where the training data from a collection of images in the \u201csource\u201d domain (which conveniently has labels given to us) is to be compared with a collection of unlabeled images from a \u201ctarget\u201d domain (e.g., \u201csource\u201d \u2192 NY subway, \u201ctarget\u201d \u2192 Tokyo subway).\n\nOne can take a collection of images of size NxN and using a variety of different methods find the smallest subspace that the source images lie in (treating each image as a vector in N^2 dimensions). Now, to understand this body of work, you obviously need to know some linear algebra. So, if you don\u2019t understand linear algebra, or you took a class way back when and forgot it all, it\u2019s time to refresh your memory or learn anew. Fortunately, there are excellent textbooks (Strang is usually a good place to start) and also something like MATLAB will let you explore linear algebraic ML methods without having to implement things like eigenvalue or singular value decomposition. As I usually told my students, keep the motto \u201ceigen do it if I try\u201d in mind. Persevere, and keep the focus on why you are learning this math. Because it is important and essential to understand much of modern ML.\n\nOK, great, you\u2019ve managed to learn some linear algebra. Are you done? Ummm, not quite. So, back to our transfer learning example. You construct a source subspace from the source images, and a target subspace from the target images. Umm, how does one do that. OK, you can use a garden variety dimensionality reduction method like principal components analysis (PCA), which just computes the dominant eigenvectors of the covariance matrices of the source and target images. This is one subroutine call in MATLAB. But, PCA is 100 years old. How about something new and cool, like a ooh la la subspace tracking method like GOUDA, which uses the fancier math of Lie groups. Oops, now you need to learn some group theory, the mathematics of symmetry. As it turns out, matrices of certain types, like all invertible matrices, or all positive definite matrices, are not just linear algebraic objects, they are also of interest in group theory, a particularly important subfield of which is Lie groups (Lie \u2192 \u201cLee\u201d).\n\nOK, great, you have a smattering of knowledge of group theory and Lie groups. Are you done? Hmmm\u2026actually not, because it turns out Lie groups are not just groups, but they are also continuous manifolds. What in the blazes is a \u201cmanifold\u201d? If you google this, you are likely to encounter web pages describing engine parts! No, a manifold is something entirely different in machine learning, where it means a non-Euclidean space that has curvature. It turns out the set of all probability distributions (e.g., 1 dimensional Gaussians with a scalar variance dimension and a scalar mean dimension) are not Euclidean, but rather, describe a curved space. So, the set of all positive definite matrices form a Lie group, with a certain curvature. What this implies is that obvious operations like taking the average have to done with considerable care. So, off you go, learning all there is to know about manifolds, Riemannian manifolds, tangent spaces, covariant derivatives, exp and log mappings, etc. Oh, what fun!\n\nGetting back to our transfer learning method, if you compute the source covariance matrix C_s and the target covariance matrix C_t, then there is a simple method called CORAL (for correlational alignment) that figures out how to transform C_s into C_t using some invertible mapping A. CORAL is popular as a transfer learning method in computer vision. But, CORAL does not actually use the knowledge that the space of positive definite matrices (or covariance matrices) forms a manifold. In fact, it forms something called a cone in convex analysis. If you subtract one covariance matrix from another, the result is not a covariance matrix. So, they do not form a vector space, but rather something else entirely. Oops, it turns out the study of cones is important in convex analysis, so there you go again, you need to learn about convex sets and functions, projections onto convex sets, etc. The dividing line between tractable and intractable optimization is not linear vs. nonlinear, but rather, convex vs. non-convex.\n\nI hope the pattern is becoming clear. Like one of those legendary Russian dolls, where each time you open one, you find it is not the end, but there\u2019s another one inside it, so it is with learning math in machine learning. Each time you learn a bit of math, you find it opens the door to an entirely new field of math, which you need to know something about as well. For my most recent paper, I had to digest a whole book devoted entirely to the topic of positive definite matrices (it\u2019s like the old joke, where the deeper you go, the more you know about a specialized topic, until you know everything about \u2014- nothing!).\n\nAny given problem in machine learning, like transfer learning, can be formulated as a convex optimization problem, as a manifold learning problem, as a multivariate statistical estimation problem, as a nonlinear gradient based deep learning problem, etc. etc. Each of these requires learning a bit about the underlying math involved.\n\nIf you feel discouraged, and feel like tearing your hair out at this point, I sympathize with you. But, on the other hand, you can look on the positive side, and realize that in terms of our analogy of running a marathon, you are steadily becoming better at running the long race, building your mathematical muscle as you go along, and gradually things start falling into place. Things start to make sense, and different subfields start connecting with each other. Something strange happens. You start liking it! Of course, there\u2019s a drawback. Someone who doesn't understand any of the math you get good at using asks you to explain your work, and you realize that it\u2019s impossible to do that without writing equations.\n\nMost researchers find their comfort zone and try to stay within it, since otherwise, it takes a great deal of time and effort to master the dozens of mathematical subfields that modern ML uses. But, this strategy eventually fails, and one is always forced to get outside one\u2019s comfort zone and learn some new math, since otherwise, a whole area of the field becomes alien to you.\n\nFortunately, the human brain is an amazing instrument, and provides decades and decades of trouble-free operation, allowing us to continually learn over 40,50, 60, years or more. How precisely it does that without zeroing out all prior learning is one of the greatest unsolved mysteries in science!",
                "Not really brutal but more of anti-climactic realities:\n\n * Most business datasets are either too dirty or too sparse to make machine learning useful.\n * The actual use-cases for machine learning are few vs. more general business analytics. Many business problems do not require prediction. Inferential statistics and variance analysis will take care of 80\u201390% of insighting.\n * Businesses value interpretability over accuracy. This is why less efficient but more interpretable methods (e.g. logistic/linear regression) are still in use over the black box methods that can hit 95%+ AR.\n * Even with machine learning in place, business leaders remain skeptical of error rates. This is a cognitive bias; people value successes of human decisions and magnify the admittedly fewer failures of machine learning. Look at the media firestorm on the few self-driving car mishaps vs. the thousands to millions of people dying due to other causes.\n * The use-cases where machine learning does shine (e.g. object detection, text analysis, sound analysis, image analysis) require relatively more mature and advanced business culture and process. This rules out 90+% of typical businesses. Over time the purported AI-driven companies will multiply in number and more and more research is making ML more relevant to every day tasks.\n * The hype cycle for ML is also conflating the problem: many tech companies and startups are branding conventional solutions as AI or ML, which increases confusion and skepticism amongst clients. This will take its time to subside and the real successful use-cases will emerge. For practitioners like us, this time can\u2019t happen quick enough.\n",
                "The data science role has declined for three straight years now.\n\nWhen it declines in 2020, it will be the fourth year the role declines in the real-world.\n\nData science is dead. Read this for a wake up call. [ https://www.quora.com/Is-the-data-scientist-job-still-trending-on-the-number-1-job-in-the-world/answer/Mike-West-99 ]\n\nSo, the real question is\u2026 how do I become a machine learning engineer?\n\nFirstly, ignore just about every one of the comments here.\n\nMath is the single most OVERRATED SKILL for those in the applied space.\n\nMachine learning is PROGRAMMING.\n\nHere are a few real-world interview questions.  [ https://www.quora.com/What-types-of-technical-questions-are-most-commonly-asked-in-Senior-Machine-Learning-Engineer-interviews/answer/Mike-West-99 ]None of them are math related.\n\nHere\u2019s a pic of the end to end ML pipeline.\n\nIf you can\u2019t walk into an interview and work through the end to end process most companies won\u2019t be paying you the big coin associated with this field.\n\nIt\u2019s only 4 boxes, looks easy but not so much. Here\u2019s what you\u2019ll need to be able to do.\n\n1. Sit with the business types to define the problem.\n2. Ensure you have the data for their problem.\n3. Source the data. Have the SQL skills to create the code to be able to pull it from relational databases. If you can\u2019t craft multi-table joins then we pass on you. We\u2026 and most companies\u2026 now require 3\u20135 years real-world SQL experience.\n4. Wrangle and cleanse data. Again, heavy data experience required. Additionally, you\u2019ll need to know the stats and the \u201cwhy\u201d to cleansing your data. For example, why is a normal distribution important? If the data is not normally distributed, what can you do? New to data wrangling?\n5. We build our models in GCP and AWS so basic cloud skills are required.\n6. Be able to choose a model type then build and tune that model. If most real world models are supervised (and they are), what modeling approach might you choose and why? Again, if you don\u2019t know that almost all the structured dataset competitions are dominated by gradient boosters then that\u2019s a problem. Note: Stay away from deep learning for now. Most real world modeling is done on structured data, another class of models called gradient boosters works much better than ANNs. Almost all my work is on structured datasets. We only use gradient boosters.\n7. Work with front end types or author the front end code to call your model. Your model is useless if it\u2019s not being consumed.\n8. Monitor the performance of your models.\nYou get the idea.\n\nSo, how do you get started?\n\n1. You need to gain real-world experience working with data. Get a job working with SQL now.\n2. Learn Python and the core ML libraries.\n3. Be able to work the end to end ML pipeline.\n4. Learn applied stats.\n5. Set realistic expectations. If you are new with no IT background you are looking at 8 years before you are in a real-world job.\n6. Because a voracious learner of all things applied machine learning.\n7. Enter a Kaggle and score high. Be able to explain how you scored this well in great technical detail.\nGood luck.\n\nWhen you\u2019re ready to learn real-world machine learning\u2026 [ https://www.logikbot.com/ ]",
                "I am working as a Data Scientist myself therefore it makes me qualified enough to answer your question.\n\nAlso I will make sure to include the tricks in my answer that worked for me.\n\nSo Let's begin, Shall we?\n\nI will be answering this question, keeping in mind that a bunch of readers could be complete newbies into programming.\n\nSo addressing non-computer science students. Firstly, you need to work a lot on your problem-solving skills which is going to help you code effortlessly. You can achieve this by learning Data structures & Algorithms and coding in it. Also, DS & Algo are the building block of computer science so it will definitely help you on your Journey towards excellence in coding.\n\nAfter you are comfortable with problem-solving, you should stick to the below mentioned points:\n\n1. Opt for a good course on Machine learning and study it thoroughly to become well versed with all it\u2019s concepts.\n2. Practice machine learning problems on Kaggle: Your Machine Learning and Data Science Community [ https://www.kaggle.com/ ] which will help you gain confidence and give you enough hands-on skills.\n3. Post your projects on GitHub, LinkedIn and also you can use youtube to showcase your skills\n4. Now it\u2019s Time to market yourself. Make a clean and creative online portfolio and a strong resume based on ML. Start applying to your desired companies and surely circumstances will bend in your favour and soon you will become something you have worked so hard for and that is \u201cData scientist\u201d\n5. you can connect with me on LinkedIn\nPs: I am attaching my photo, in which you can see me working from home, just in case you are interested to know how a data scientist looks?! \ud83d\ude1b\n\n",
                "Truth be told first - even if it disappoints many people -the industry does not have an agreed upon definition of a data scientist! \n\nJokes like 'a data scientist is a data analyst living in the Silicon Valley' are abundant. Below is one such cartoon, just for fun.\n\n\n\nFinding an 'effective' data scientist is hard and finding people who understand who a data scientist is equally hard. Note the use of 'effective' here, I use it to highlight that there could be people who might possess some of these skills yet may not be the best fit in a data science role. Irony is that even the people looking for hiring data scientists do not know who a data scientist is. Hiring managers post job descriptions for traditional data analyst and business analyst roles while the title calling it a 'Data Scientist' role.  \n\nEverything that I say below is my experience working in a data scientist role with a major search engine and advertising platform. Instead of giving a bullet list of skills, I would first highlight the difference between some data related roles. \n\nConsider the following scenario. Shop-Mart and Bulk-Mart are two competitors in selling retail. Some higher up in the management chain asks this question: \"How many Shop-Mart customers also go to Bulk-Mart?\". \n\n[Please note that the question might be of interest to Bulk-Mart management or even a third party, possibly a market research or consumer behavior company, interested in shopping behavior of the population.]\n\n\n\nHere is how different data-related roles will approach the problem. This \n\nTraditional BI/Reporting Professional: Generate reports from structured data using SQL and some kind of reporting services (SSRS for instance) and send the data back to management. The management asks more questions based on the data that was sent and cycle continues. Insights about data are most likely not included in the reports. A person in this role will be experienced mostly in database related skills.\n\nData Analyst: In addition to doing what the BI guy did, a data analyst will also keep other factors like seasonality, segmentation and visualization in mind. What about if certain trends in shopping behavior are tied to seasonality? What if the trends are different across gender, demographics, geography, product category? A data analyst will slice and dice the data to understand and annotate the report with it. Besides database skills, a data analyst will have a understanding of some of the common visualization tools.\n\nBusiness Analyst: A business analyst possess the skills that the BI and data analysts have plus domain knowledge and understanding of the business. A business analyst may also have some basic skills in forecasting etc.\n\nData Mining or Big Data Engineer: Do what the data analyst did, possibly from unstructured data if needed. MapReduce and other big data skills may be needed. Understands the common issues in running jobs on large scale data and is able to debug the jobs. \n\nStatistician (A traditional one): Pull the data from a DB or obtain it from any of the roles mentioned above and run appropriate statistical tests. Ensure the quality of data and correctness of the conclusions by using standard practices like choosing the right sample size, confidence level, level of significance, type of test etc. \nThe situation has changed a bit recently. Statistics departments at most schools have evolved in way that statisticians graduate with strong programming and decent foundation skills in CS enabling them to do the tasks that statisticians traditionally were not trained in.\n\nProgram/Project Manager: Look at the data provided by the professionals mentioned so far, align business with the findings and influence the leadership to take appropriate action. Possesses communication skills, presentation skills and can influence without authority.\nIronically the person a PM is influencing business decisions using the data and insights provided by others. If the person does not have a knack for understanding data, chances are that the person will not be able to influence others to take the correct decisions. \n\n\nNow putting it altogether.\n\nThe rise of online services has brought a paradigm shift in software development life cycle and how business iterate over successive features and products. Having a different data puller, analyst, statistician and project manager is just now possible any more. The mantra now is ship, experiment and learn, adapt, ship, experiment and learn .... This situation has resulted in the birth of a new role - ' A Data Scientist'\n\nA data scientist should have the skills of all the individuals I have mentioned so far. In addition to the skills mentioned above, a data scientist should have rapid prototyping and programming, machine learning, visualization and hacking skills. \n\nDomain Knowledge and Soft Skills Are As Important as Technical Skills:\nThe importance of domain knowledge and soft skills like communication and influencing without authority are severely under-estimated both by hiring managers and aspiring data scientists. Insights without domain knowledge can potentially mislead the consumers of these insights. Correct insights without the ability to influence the decision making is as bad as having no insights.\n\nAll of what I have said above is based on my own tenure as a data scientist at a major search engine and later the advertising platform within the same company. I learnt that sometimes people asking the question may not know what they want to know - sounds preposterous - yet happens way too often. Very often a bozo will start rat holing into something that is not related to the issue at hand - just to prove that he/she is relevant. A data scientist encounters such HIPPOS (Highly Paid Person's Opinion) that are somewhat unrelated to the problem if not complete nonsense very often. A data scientist should posses the right soft skills to manage situations where people ask irrelevant, distracting or outside of scope questions. This is hard, especially in situations where the person asking the question is several levels up in the corporate ladder and is known to have an ego. It is a data scientist's responsibility to manage up and around while presenting and communicating insights. \n\nBelow is a summary of necessary skills a data scientist should possess in my opinion.\n\nCuriosity About Data and Passion for Domain: If you are not passionate about the domain/business and curious about  data then it is unlikely that you will succeed in a data scientist role. If you are working as a data scientist with an online retailer, you should be hungry all the time to crunch and munch from the Sm\u00f6rg\u00e5sbord (of data of course) to know more. If your curiosity does not keep you awake, no skill in the world can help you succeed. \n\nSoft Skills: Communication and influencing without authority. Understanding of what is the minimum that has the maximum impact. Too many findings are as bad as no findings at all.   Ability to scoop information out of partners and customers, even from the unwilling ones is extremely important. The data you are looking for may not be sitting in one single place. You may have to beg, borrow, steal and do whatever it takes to get the data.\n\nBeing a good story teller is also something that helps. Sometimes the insights obtained from data are counter-intuitive, if you are not a good story teller, it will be difficult to convince your audience.\n\nMath/Theory: Machine Learning. Stats and Probability 101. Optimization would be icing on the cake.\n\nCS/Programming: At least one scripting language (I prefer python). Decent algorithms and DS skills, to be able to write code that can analyze a lot of data efficiently. You may not be a production code developer but should be able write code that does not suck. Database management and SQL skills. Knowledge of a statistical computing package, most people including myself prefer R. A spread sheet software like excel.\n\nBig Data and Distributed Systems: Understanding of basic MapReduce concepts, Hadoop and Hadoop file system and least one language like Hive/Pig. Some companies have their own proprietary implementations of these languages. Knowledge of tools like Mahout and any of the xaaS like Azure and AWS would be helpful. Once again big companies have their own xaaS so you may be working on variants of any of these.\n\nVisualization: Ability to create simple yet elegant and meaningful visualization. In my case, R packages like ggplot, lattice and others have helped me in most cases but there are other packages that you can use. In some cases, you might want to use D3. \n\n\nBelow is a visualization of high level description of skills needed to become a data scientist.\n\n\nWhere is a data scientist in the big data pipeline?\nHere is a nice visualization of the big data pipeline, the associated technologies and the regions of operation. In general the depiction of where the data scientist belongs in this pipeline is largely correct, there is one caveat however. A data scientist should be comfortable to dive into the 'Collect' and 'Store' territories if needed. Usually  data scientists would be working on transformed data and beyond but in scenarios where the business does cannot afford to wait for the transformation process to complete, a data scientist has to turn to raw data to gather insights.\n\n\n\nTo be continued .....\n----------------------------------------------------------------------------------------------------------\nNote: I am publishing this without any edits/reviews. I will update with more thoughts as I get a chance. I am writing myself a note to finish this answer in the next one week. Pardon the typos and scattered thoughts at least for now.\n------------------------------------------------------------------------------------------------------------",
                "Truth be told first - even if it disappoints many people -the industry does not have an agreed upon definition of a data scientist! \n\nJokes like 'a data scientist is a data analyst living in the Silicon Valley' are abundant. Below is one such cartoon, just for fun.\n\n\n\nFinding an 'effective' data scientist is hard and finding people who understand who a data scientist is equally hard. Note the use of 'effective' here, I use it to highlight that there could be people who might possess some of these skills yet may not be the best fit in a data science role. Irony is that even the people looking for hiring data scientists do not know who a data scientist is. Hiring managers post job descriptions for traditional data analyst and business analyst roles while the title calling it a 'Data Scientist' role.  \n\nEverything that I say below is my experience working in a data scientist role with a major search engine and advertising platform. Instead of giving a bullet list of skills, I would first highlight the difference between some data related roles. \n\nConsider the following scenario. Shop-Mart and Bulk-Mart are two competitors in selling retail. Some higher up in the management chain asks this question: \"How many Shop-Mart customers also go to Bulk-Mart?\". \n\n[Please note that the question might be of interest to Bulk-Mart management or even a third party, possibly a market research or consumer behavior company, interested in shopping behavior of the population.]\n\n\n\nHere is how different data-related roles will approach the problem. This \n\nTraditional BI/Reporting Professional: Generate reports from structured data using SQL and some kind of reporting services (SSRS for instance) and send the data back to management. The management asks more questions based on the data that was sent and cycle continues. Insights about data are most likely not included in the reports. A person in this role will be experienced mostly in database related skills.\n\nData Analyst: In addition to doing what the BI guy did, a data analyst will also keep other factors like seasonality, segmentation and visualization in mind. What about if certain trends in shopping behavior are tied to seasonality? What if the trends are different across gender, demographics, geography, product category? A data analyst will slice and dice the data to understand and annotate the report with it. Besides database skills, a data analyst will have a understanding of some of the common visualization tools.\n\nBusiness Analyst: A business analyst possess the skills that the BI and data analysts have plus domain knowledge and understanding of the business. A business analyst may also have some basic skills in forecasting etc.\n\nData Mining or Big Data Engineer: Do what the data analyst did, possibly from unstructured data if needed. MapReduce and other big data skills may be needed. Understands the common issues in running jobs on large scale data and is able to debug the jobs. \n\nStatistician (A traditional one): Pull the data from a DB or obtain it from any of the roles mentioned above and run appropriate statistical tests. Ensure the quality of data and correctness of the conclusions by using standard practices like choosing the right sample size, confidence level, level of significance, type of test etc. \nThe situation has changed a bit recently. Statistics departments at most schools have evolved in way that statisticians graduate with strong programming and decent foundation skills in CS enabling them to do the tasks that statisticians traditionally were not trained in.\n\nProgram/Project Manager: Look at the data provided by the professionals mentioned so far, align business with the findings and influence the leadership to take appropriate action. Possesses communication skills, presentation skills and can influence without authority.\nIronically the person a PM is influencing business decisions using the data and insights provided by others. If the person does not have a knack for understanding data, chances are that the person will not be able to influence others to take the correct decisions. \n\n\nNow putting it altogether.\n\nThe rise of online services has brought a paradigm shift in software development life cycle and how business iterate over successive features and products. Having a different data puller, analyst, statistician and project manager is just now possible any more. The mantra now is ship, experiment and learn, adapt, ship, experiment and learn .... This situation has resulted in the birth of a new role - ' A Data Scientist'\n\nA data scientist should have the skills of all the individuals I have mentioned so far. In addition to the skills mentioned above, a data scientist should have rapid prototyping and programming, machine learning, visualization and hacking skills. \n\nDomain Knowledge and Soft Skills Are As Important as Technical Skills:\nThe importance of domain knowledge and soft skills like communication and influencing without authority are severely under-estimated both by hiring managers and aspiring data scientists. Insights without domain knowledge can potentially mislead the consumers of these insights. Correct insights without the ability to influence the decision making is as bad as having no insights.\n\nAll of what I have said above is based on my own tenure as a data scientist at a major search engine and later the advertising platform within the same company. I learnt that sometimes people asking the question may not know what they want to know - sounds preposterous - yet happens way too often. Very often a bozo will start rat holing into something that is not related to the issue at hand - just to prove that he/she is relevant. A data scientist encounters such HIPPOS (Highly Paid Person's Opinion) that are somewhat unrelated to the problem if not complete nonsense very often. A data scientist should posses the right soft skills to manage situations where people ask irrelevant, distracting or outside of scope questions. This is hard, especially in situations where the person asking the question is several levels up in the corporate ladder and is known to have an ego. It is a data scientist's responsibility to manage up and around while presenting and communicating insights. \n\nBelow is a summary of necessary skills a data scientist should possess in my opinion.\n\nCuriosity About Data and Passion for Domain: If you are not passionate about the domain/business and curious about  data then it is unlikely that you will succeed in a data scientist role. If you are working as a data scientist with an online retailer, you should be hungry all the time to crunch and munch from the Sm\u00f6rg\u00e5sbord (of data of course) to know more. If your curiosity does not keep you awake, no skill in the world can help you succeed. \n\nSoft Skills: Communication and influencing without authority. Understanding of what is the minimum that has the maximum impact. Too many findings are as bad as no findings at all.   Ability to scoop information out of partners and customers, even from the unwilling ones is extremely important. The data you are looking for may not be sitting in one single place. You may have to beg, borrow, steal and do whatever it takes to get the data.\n\nBeing a good story teller is also something that helps. Sometimes the insights obtained from data are counter-intuitive, if you are not a good story teller, it will be difficult to convince your audience.\n\nMath/Theory: Machine Learning. Stats and Probability 101. Optimization would be icing on the cake.\n\nCS/Programming: At least one scripting language (I prefer python). Decent algorithms and DS skills, to be able to write code that can analyze a lot of data efficiently. You may not be a production code developer but should be able write code that does not suck. Database management and SQL skills. Knowledge of a statistical computing package, most people including myself prefer R. A spread sheet software like excel.\n\nBig Data and Distributed Systems: Understanding of basic MapReduce concepts, Hadoop and Hadoop file system and least one language like Hive/Pig. Some companies have their own proprietary implementations of these languages. Knowledge of tools like Mahout and any of the xaaS like Azure and AWS would be helpful. Once again big companies have their own xaaS so you may be working on variants of any of these.\n\nVisualization: Ability to create simple yet elegant and meaningful visualization. In my case, R packages like ggplot, lattice and others have helped me in most cases but there are other packages that you can use. In some cases, you might want to use D3. \n\n\nBelow is a visualization of high level description of skills needed to become a data scientist.\n\n\nWhere is a data scientist in the big data pipeline?\nHere is a nice visualization of the big data pipeline, the associated technologies and the regions of operation. In general the depiction of where the data scientist belongs in this pipeline is largely correct, there is one caveat however. A data scientist should be comfortable to dive into the 'Collect' and 'Store' territories if needed. Usually  data scientists would be working on transformed data and beyond but in scenarios where the business does cannot afford to wait for the transformation process to complete, a data scientist has to turn to raw data to gather insights.\n\n\n\nTo be continued .....\n----------------------------------------------------------------------------------------------------------\nNote: I am publishing this without any edits/reviews. I will update with more thoughts as I get a chance. I am writing myself a note to finish this answer in the next one week. Pardon the typos and scattered thoughts at least for now.\n------------------------------------------------------------------------------------------------------------",
                "I am working as a Data Scientist myself therefore it makes me qualified enough to answer your question.\n\nAlso I will make sure to include the tricks in my answer that worked for me.\n\nSo Let's begin, Shall we?\n\nI will be answering this question, keeping in mind that a bunch of readers could be complete newbies into programming.\n\nSo addressing non-computer science students. Firstly, you need to work a lot on your problem-solving skills which is going to help you code effortlessly. You can achieve this by learning Data structures & Algorithms and coding in it. Also, DS & Algo are the building block of computer science so it will definitely help you on your Journey towards excellence in coding.\n\nAfter you are comfortable with problem-solving, you should stick to the below mentioned points:\n\n1. Opt for a good course on Machine learning and study it thoroughly to become well versed with all it\u2019s concepts.\n2. Practice machine learning problems on Kaggle: Your Machine Learning and Data Science Community [ https://www.kaggle.com/ ] which will help you gain confidence and give you enough hands-on skills.\n3. Post your projects on GitHub, LinkedIn and also you can use youtube to showcase your skills\n4. Now it\u2019s Time to market yourself. Make a clean and creative online portfolio and a strong resume based on ML. Start applying to your desired companies and surely circumstances will bend in your favour and soon you will become something you have worked so hard for and that is \u201cData scientist\u201d\n5. you can connect with me on LinkedIn\nPs: I am attaching my photo, in which you can see me working from home, just in case you are interested to know how a data scientist looks?! \ud83d\ude1b\n\n",
                "You have Python and linear Algebra and a basics programming skills that a good things, so you should to add some other stuffs for Data science, Below some of the best online platforms with related courses, where you can learn and \u201cpractice\u201d on those topics to become a data scientist\n\nCoursera [ http://coursera.org ] (from a big university ):\n\n * Data science Specializations [ https://www.coursera.org/browse/data-science ]\n * Life sciences [ https://www.coursera.org/browse/life-sciences ]\n * Computer science [ https://www.coursera.org/browse/computer-science ]\nUdacity [ http://udacity.com ] (with certification):\n\n * Inferential Statistics\n * Intro to Hadoop and MapReduce\n * Data Analysis with R\n * Intro to Descriptive Statistics\n * Intro to Data Science\n * Data Visualization and D3.js\nLink: https://www.udacity.com/courses/... [ https://www.udacity.com/courses/data-science ]\n\nEdx  [ http://edx.org ](with certification ):\n\n * The Analytics Edge\n * Statistics for the Life Sciences Using R\n * Querying with Transact-SQL\n * Explore Statistics with R\n * Matrix Algebra and Linear Models\n * Statistics for Business\n * Introduction to Computational Thinking and Data Science\nBigDataUniversity [ http://bigdatauniversity.com ]:\n\n * Introduction to Data Analysis using R\n * Hadoop Fundamentals\n * SQL Access for Hadoop\nMachine Learning\n\n * Machine Learning - Stanford University | Coursera [ https://www.coursera.org/learn/machine-learning ] (online course)\n * Practical Machine Learning - Johns Hopkins University | Coursera [ https://www.coursera.org/learn/practical-machine-learning ]\n * Machine Learning | Coursera [ https://www.coursera.org/specializations/machine-learning ] ( Machine Learning Specialization : University Of Washington)\n * Machine Learning [ http://www.amazon.com/dp/0070428077?tag=inspiredalgor-20 ] (Book)\n * Machine Learning for Hackers [ http://www.amazon.com/dp/1449303714?tag=inspiredalgor-20 ] (Book)\n * Machine Learning: An Algorithmic Perspective [ http://www.amazon.com/dp/1420067184?tag=inspiredalgor-20 ] (Book)\n * Programming Collective Intelligence: Building Smart Web 2.0 Applications [ http://www.amazon.com/dp/0596529325?tag=inspiredalgor-20 ](Book)\nFor practice :\n\n * Kaggle platform gives you a possibily to do that Your Home for Data Science [ http://kaggle.com ]\nAlso check this Abdelbarre Chafik's answer to What tools do data scientists use? [ https://www.quora.com/What-tools-do-data-scientists-use/answer/Abdelbarre-Chafik ]\n\nHope That Helps :)",
                "Truth be told first - even if it disappoints many people -the industry does not have an agreed upon definition of a data scientist! \n\nJokes like 'a data scientist is a data analyst living in the Silicon Valley' are abundant. Below is one such cartoon, just for fun.\n\n\n\nFinding an 'effective' data scientist is hard and finding people who understand who a data scientist is equally hard. Note the use of 'effective' here, I use it to highlight that there could be people who might possess some of these skills yet may not be the best fit in a data science role. Irony is that even the people looking for hiring data scientists do not know who a data scientist is. Hiring managers post job descriptions for traditional data analyst and business analyst roles while the title calling it a 'Data Scientist' role.  \n\nEverything that I say below is my experience working in a data scientist role with a major search engine and advertising platform. Instead of giving a bullet list of skills, I would first highlight the difference between some data related roles. \n\nConsider the following scenario. Shop-Mart and Bulk-Mart are two competitors in selling retail. Some higher up in the management chain asks this question: \"How many Shop-Mart customers also go to Bulk-Mart?\". \n\n[Please note that the question might be of interest to Bulk-Mart management or even a third party, possibly a market research or consumer behavior company, interested in shopping behavior of the population.]\n\n\n\nHere is how different data-related roles will approach the problem. This \n\nTraditional BI/Reporting Professional: Generate reports from structured data using SQL and some kind of reporting services (SSRS for instance) and send the data back to management. The management asks more questions based on the data that was sent and cycle continues. Insights about data are most likely not included in the reports. A person in this role will be experienced mostly in database related skills.\n\nData Analyst: In addition to doing what the BI guy did, a data analyst will also keep other factors like seasonality, segmentation and visualization in mind. What about if certain trends in shopping behavior are tied to seasonality? What if the trends are different across gender, demographics, geography, product category? A data analyst will slice and dice the data to understand and annotate the report with it. Besides database skills, a data analyst will have a understanding of some of the common visualization tools.\n\nBusiness Analyst: A business analyst possess the skills that the BI and data analysts have plus domain knowledge and understanding of the business. A business analyst may also have some basic skills in forecasting etc.\n\nData Mining or Big Data Engineer: Do what the data analyst did, possibly from unstructured data if needed. MapReduce and other big data skills may be needed. Understands the common issues in running jobs on large scale data and is able to debug the jobs. \n\nStatistician (A traditional one): Pull the data from a DB or obtain it from any of the roles mentioned above and run appropriate statistical tests. Ensure the quality of data and correctness of the conclusions by using standard practices like choosing the right sample size, confidence level, level of significance, type of test etc. \nThe situation has changed a bit recently. Statistics departments at most schools have evolved in way that statisticians graduate with strong programming and decent foundation skills in CS enabling them to do the tasks that statisticians traditionally were not trained in.\n\nProgram/Project Manager: Look at the data provided by the professionals mentioned so far, align business with the findings and influence the leadership to take appropriate action. Possesses communication skills, presentation skills and can influence without authority.\nIronically the person a PM is influencing business decisions using the data and insights provided by others. If the person does not have a knack for understanding data, chances are that the person will not be able to influence others to take the correct decisions. \n\n\nNow putting it altogether.\n\nThe rise of online services has brought a paradigm shift in software development life cycle and how business iterate over successive features and products. Having a different data puller, analyst, statistician and project manager is just now possible any more. The mantra now is ship, experiment and learn, adapt, ship, experiment and learn .... This situation has resulted in the birth of a new role - ' A Data Scientist'\n\nA data scientist should have the skills of all the individuals I have mentioned so far. In addition to the skills mentioned above, a data scientist should have rapid prototyping and programming, machine learning, visualization and hacking skills. \n\nDomain Knowledge and Soft Skills Are As Important as Technical Skills:\nThe importance of domain knowledge and soft skills like communication and influencing without authority are severely under-estimated both by hiring managers and aspiring data scientists. Insights without domain knowledge can potentially mislead the consumers of these insights. Correct insights without the ability to influence the decision making is as bad as having no insights.\n\nAll of what I have said above is based on my own tenure as a data scientist at a major search engine and later the advertising platform within the same company. I learnt that sometimes people asking the question may not know what they want to know - sounds preposterous - yet happens way too often. Very often a bozo will start rat holing into something that is not related to the issue at hand - just to prove that he/she is relevant. A data scientist encounters such HIPPOS (Highly Paid Person's Opinion) that are somewhat unrelated to the problem if not complete nonsense very often. A data scientist should posses the right soft skills to manage situations where people ask irrelevant, distracting or outside of scope questions. This is hard, especially in situations where the person asking the question is several levels up in the corporate ladder and is known to have an ego. It is a data scientist's responsibility to manage up and around while presenting and communicating insights. \n\nBelow is a summary of necessary skills a data scientist should possess in my opinion.\n\nCuriosity About Data and Passion for Domain: If you are not passionate about the domain/business and curious about  data then it is unlikely that you will succeed in a data scientist role. If you are working as a data scientist with an online retailer, you should be hungry all the time to crunch and munch from the Sm\u00f6rg\u00e5sbord (of data of course) to know more. If your curiosity does not keep you awake, no skill in the world can help you succeed. \n\nSoft Skills: Communication and influencing without authority. Understanding of what is the minimum that has the maximum impact. Too many findings are as bad as no findings at all.   Ability to scoop information out of partners and customers, even from the unwilling ones is extremely important. The data you are looking for may not be sitting in one single place. You may have to beg, borrow, steal and do whatever it takes to get the data.\n\nBeing a good story teller is also something that helps. Sometimes the insights obtained from data are counter-intuitive, if you are not a good story teller, it will be difficult to convince your audience.\n\nMath/Theory: Machine Learning. Stats and Probability 101. Optimization would be icing on the cake.\n\nCS/Programming: At least one scripting language (I prefer python). Decent algorithms and DS skills, to be able to write code that can analyze a lot of data efficiently. You may not be a production code developer but should be able write code that does not suck. Database management and SQL skills. Knowledge of a statistical computing package, most people including myself prefer R. A spread sheet software like excel.\n\nBig Data and Distributed Systems: Understanding of basic MapReduce concepts, Hadoop and Hadoop file system and least one language like Hive/Pig. Some companies have their own proprietary implementations of these languages. Knowledge of tools like Mahout and any of the xaaS like Azure and AWS would be helpful. Once again big companies have their own xaaS so you may be working on variants of any of these.\n\nVisualization: Ability to create simple yet elegant and meaningful visualization. In my case, R packages like ggplot, lattice and others have helped me in most cases but there are other packages that you can use. In some cases, you might want to use D3. \n\n\nBelow is a visualization of high level description of skills needed to become a data scientist.\n\n\nWhere is a data scientist in the big data pipeline?\nHere is a nice visualization of the big data pipeline, the associated technologies and the regions of operation. In general the depiction of where the data scientist belongs in this pipeline is largely correct, there is one caveat however. A data scientist should be comfortable to dive into the 'Collect' and 'Store' territories if needed. Usually  data scientists would be working on transformed data and beyond but in scenarios where the business does cannot afford to wait for the transformation process to complete, a data scientist has to turn to raw data to gather insights.\n\n\n\nTo be continued .....\n----------------------------------------------------------------------------------------------------------\nNote: I am publishing this without any edits/reviews. I will update with more thoughts as I get a chance. I am writing myself a note to finish this answer in the next one week. Pardon the typos and scattered thoughts at least for now.\n------------------------------------------------------------------------------------------------------------",
                "I am working as a Data Scientist myself therefore it makes me qualified enough to answer your question.\n\nAlso I will make sure to include the tricks in my answer that worked for me.\n\nSo Let's begin, Shall we?\n\nI will be answering this question, keeping in mind that a bunch of readers could be complete newbies into programming.\n\nSo addressing non-computer science students. Firstly, you need to work a lot on your problem-solving skills which is going to help you code effortlessly. You can achieve this by learning Data structures & Algorithms and coding in it. Also, DS & Algo are the building block of computer science so it will definitely help you on your Journey towards excellence in coding.\n\nAfter you are comfortable with problem-solving, you should stick to the below mentioned points:\n\n1. Opt for a good course on Machine learning and study it thoroughly to become well versed with all it\u2019s concepts.\n2. Practice machine learning problems on Kaggle: Your Machine Learning and Data Science Community [ https://www.kaggle.com/ ] which will help you gain confidence and give you enough hands-on skills.\n3. Post your projects on GitHub, LinkedIn and also you can use youtube to showcase your skills\n4. Now it\u2019s Time to market yourself. Make a clean and creative online portfolio and a strong resume based on ML. Start applying to your desired companies and surely circumstances will bend in your favour and soon you will become something you have worked so hard for and that is \u201cData scientist\u201d\n5. you can connect with me on LinkedIn\nPs: I am attaching my photo, in which you can see me working from home, just in case you are interested to know how a data scientist looks?! \ud83d\ude1b\n\n",
                "Right now I am in the third week of prof Andrew Ng's machine learning course. In the past one week I learned following things\n\n1. Classification using logistics regression. This involved hypothesis represention for logistics regression problem and understanding of decision boundary\n2. Formulation of cost function and use of gradient descent to minimize the function in turn finding of the fitting parameters for hypothesis function.\n3. Use of advanced optimization techniques like conjugate gradient, BFGS, L-BFGS. These algorithms are faster than gradient descent and do not involve selection of alpha (step size) parameter. In general, use readymade libraries of these algorithms and not try to implement them by yourself unless you are numerical computing expert.\n4. Multiclass classification. What if we have more than two categories for classification?. Here strategy is to use binary classifications by choosing one class and lumping all other classes in to one class. Repeat this for every other class and then return the highest value.\n5. What's underfitting and overfitting and how to avoid it. Underfitting happens when function maps poorly to the data. So we won't do so well on training data. Overfitting happens when we are doing well in training set and not so well on testing set. Overfitting can be avoided using reducing number of features, model selection algorithm and reducing magnitude of fitting parameters.\n6. Implementation of logistics regression using Octave. This I have not completed but will do it tomorrow.\nEnjoying machine learning.",
                "Truth be told first - even if it disappoints many people -the industry does not have an agreed upon definition of a data scientist! \n\nJokes like 'a data scientist is a data analyst living in the Silicon Valley' are abundant. Below is one such cartoon, just for fun.\n\n\n\nFinding an 'effective' data scientist is hard and finding people who understand who a data scientist is equally hard. Note the use of 'effective' here, I use it to highlight that there could be people who might possess some of these skills yet may not be the best fit in a data science role. Irony is that even the people looking for hiring data scientists do not know who a data scientist is. Hiring managers post job descriptions for traditional data analyst and business analyst roles while the title calling it a 'Data Scientist' role.  \n\nEverything that I say below is my experience working in a data scientist role with a major search engine and advertising platform. Instead of giving a bullet list of skills, I would first highlight the difference between some data related roles. \n\nConsider the following scenario. Shop-Mart and Bulk-Mart are two competitors in selling retail. Some higher up in the management chain asks this question: \"How many Shop-Mart customers also go to Bulk-Mart?\". \n\n[Please note that the question might be of interest to Bulk-Mart management or even a third party, possibly a market research or consumer behavior company, interested in shopping behavior of the population.]\n\n\n\nHere is how different data-related roles will approach the problem. This \n\nTraditional BI/Reporting Professional: Generate reports from structured data using SQL and some kind of reporting services (SSRS for instance) and send the data back to management. The management asks more questions based on the data that was sent and cycle continues. Insights about data are most likely not included in the reports. A person in this role will be experienced mostly in database related skills.\n\nData Analyst: In addition to doing what the BI guy did, a data analyst will also keep other factors like seasonality, segmentation and visualization in mind. What about if certain trends in shopping behavior are tied to seasonality? What if the trends are different across gender, demographics, geography, product category? A data analyst will slice and dice the data to understand and annotate the report with it. Besides database skills, a data analyst will have a understanding of some of the common visualization tools.\n\nBusiness Analyst: A business analyst possess the skills that the BI and data analysts have plus domain knowledge and understanding of the business. A business analyst may also have some basic skills in forecasting etc.\n\nData Mining or Big Data Engineer: Do what the data analyst did, possibly from unstructured data if needed. MapReduce and other big data skills may be needed. Understands the common issues in running jobs on large scale data and is able to debug the jobs. \n\nStatistician (A traditional one): Pull the data from a DB or obtain it from any of the roles mentioned above and run appropriate statistical tests. Ensure the quality of data and correctness of the conclusions by using standard practices like choosing the right sample size, confidence level, level of significance, type of test etc. \nThe situation has changed a bit recently. Statistics departments at most schools have evolved in way that statisticians graduate with strong programming and decent foundation skills in CS enabling them to do the tasks that statisticians traditionally were not trained in.\n\nProgram/Project Manager: Look at the data provided by the professionals mentioned so far, align business with the findings and influence the leadership to take appropriate action. Possesses communication skills, presentation skills and can influence without authority.\nIronically the person a PM is influencing business decisions using the data and insights provided by others. If the person does not have a knack for understanding data, chances are that the person will not be able to influence others to take the correct decisions. \n\n\nNow putting it altogether.\n\nThe rise of online services has brought a paradigm shift in software development life cycle and how business iterate over successive features and products. Having a different data puller, analyst, statistician and project manager is just now possible any more. The mantra now is ship, experiment and learn, adapt, ship, experiment and learn .... This situation has resulted in the birth of a new role - ' A Data Scientist'\n\nA data scientist should have the skills of all the individuals I have mentioned so far. In addition to the skills mentioned above, a data scientist should have rapid prototyping and programming, machine learning, visualization and hacking skills. \n\nDomain Knowledge and Soft Skills Are As Important as Technical Skills:\nThe importance of domain knowledge and soft skills like communication and influencing without authority are severely under-estimated both by hiring managers and aspiring data scientists. Insights without domain knowledge can potentially mislead the consumers of these insights. Correct insights without the ability to influence the decision making is as bad as having no insights.\n\nAll of what I have said above is based on my own tenure as a data scientist at a major search engine and later the advertising platform within the same company. I learnt that sometimes people asking the question may not know what they want to know - sounds preposterous - yet happens way too often. Very often a bozo will start rat holing into something that is not related to the issue at hand - just to prove that he/she is relevant. A data scientist encounters such HIPPOS (Highly Paid Person's Opinion) that are somewhat unrelated to the problem if not complete nonsense very often. A data scientist should posses the right soft skills to manage situations where people ask irrelevant, distracting or outside of scope questions. This is hard, especially in situations where the person asking the question is several levels up in the corporate ladder and is known to have an ego. It is a data scientist's responsibility to manage up and around while presenting and communicating insights. \n\nBelow is a summary of necessary skills a data scientist should possess in my opinion.\n\nCuriosity About Data and Passion for Domain: If you are not passionate about the domain/business and curious about  data then it is unlikely that you will succeed in a data scientist role. If you are working as a data scientist with an online retailer, you should be hungry all the time to crunch and munch from the Sm\u00f6rg\u00e5sbord (of data of course) to know more. If your curiosity does not keep you awake, no skill in the world can help you succeed. \n\nSoft Skills: Communication and influencing without authority. Understanding of what is the minimum that has the maximum impact. Too many findings are as bad as no findings at all.   Ability to scoop information out of partners and customers, even from the unwilling ones is extremely important. The data you are looking for may not be sitting in one single place. You may have to beg, borrow, steal and do whatever it takes to get the data.\n\nBeing a good story teller is also something that helps. Sometimes the insights obtained from data are counter-intuitive, if you are not a good story teller, it will be difficult to convince your audience.\n\nMath/Theory: Machine Learning. Stats and Probability 101. Optimization would be icing on the cake.\n\nCS/Programming: At least one scripting language (I prefer python). Decent algorithms and DS skills, to be able to write code that can analyze a lot of data efficiently. You may not be a production code developer but should be able write code that does not suck. Database management and SQL skills. Knowledge of a statistical computing package, most people including myself prefer R. A spread sheet software like excel.\n\nBig Data and Distributed Systems: Understanding of basic MapReduce concepts, Hadoop and Hadoop file system and least one language like Hive/Pig. Some companies have their own proprietary implementations of these languages. Knowledge of tools like Mahout and any of the xaaS like Azure and AWS would be helpful. Once again big companies have their own xaaS so you may be working on variants of any of these.\n\nVisualization: Ability to create simple yet elegant and meaningful visualization. In my case, R packages like ggplot, lattice and others have helped me in most cases but there are other packages that you can use. In some cases, you might want to use D3. \n\n\nBelow is a visualization of high level description of skills needed to become a data scientist.\n\n\nWhere is a data scientist in the big data pipeline?\nHere is a nice visualization of the big data pipeline, the associated technologies and the regions of operation. In general the depiction of where the data scientist belongs in this pipeline is largely correct, there is one caveat however. A data scientist should be comfortable to dive into the 'Collect' and 'Store' territories if needed. Usually  data scientists would be working on transformed data and beyond but in scenarios where the business does cannot afford to wait for the transformation process to complete, a data scientist has to turn to raw data to gather insights.\n\n\n\nTo be continued .....\n----------------------------------------------------------------------------------------------------------\nNote: I am publishing this without any edits/reviews. I will update with more thoughts as I get a chance. I am writing myself a note to finish this answer in the next one week. Pardon the typos and scattered thoughts at least for now.\n------------------------------------------------------------------------------------------------------------",
                "I am working as a Data Scientist myself therefore it makes me qualified enough to answer your question.\n\nAlso I will make sure to include the tricks in my answer that worked for me.\n\nSo Let's begin, Shall we?\n\nI will be answering this question, keeping in mind that a bunch of readers could be complete newbies into programming.\n\nSo addressing non-computer science students. Firstly, you need to work a lot on your problem-solving skills which is going to help you code effortlessly. You can achieve this by learning Data structures & Algorithms and coding in it. Also, DS & Algo are the building block of computer science so it will definitely help you on your Journey towards excellence in coding.\n\nAfter you are comfortable with problem-solving, you should stick to the below mentioned points:\n\n1. Opt for a good course on Machine learning and study it thoroughly to become well versed with all it\u2019s concepts.\n2. Practice machine learning problems on Kaggle: Your Machine Learning and Data Science Community [ https://www.kaggle.com/ ] which will help you gain confidence and give you enough hands-on skills.\n3. Post your projects on GitHub, LinkedIn and also you can use youtube to showcase your skills\n4. Now it\u2019s Time to market yourself. Make a clean and creative online portfolio and a strong resume based on ML. Start applying to your desired companies and surely circumstances will bend in your favour and soon you will become something you have worked so hard for and that is \u201cData scientist\u201d\n5. you can connect with me on LinkedIn\nPs: I am attaching my photo, in which you can see me working from home, just in case you are interested to know how a data scientist looks?! \ud83d\ude1b\n\n",
                "In my decade of experience in data clean-up, data analysis, core modeling, data sciences (mostly machine learning or M/L), mobile analytics etc. I think 2 fundamental skills are to \"think logically\" and \"instrumenting right data\".  Most problems get solved when you know what and how to instrument and interpret the results logically.  Everything else can be learnt assuming you have a sharp curve and decent appetite.\n\nBriefly the necessary steps could be - Think how you frame the business problem, how you get the data, what needs to be done to that data, how to determine what to do, doing the algo/technique, interpreting algorithm results, implement the model/results to the problem, visualization, feedback, optimize- every step has a combination of tools/techniques.. \n\nIn terms of giving names of tools, techniques etc here is the list:\n\n1. Tools like  Hadoop, PIG, SAS, R, Python, Weka, Knime etc  are useful\n2. Techniques that involve  decent statistical knowledge starting from parametric/non parametric tests to regressions to classifications\n3. Fundamental mathetical knowledge \n4. For M/L, understanding  SVM, neural networks or any other technique to a decent depth so that you know how its being used to get results\n5. Image processing (face recognition etc) , Information retrieval and  AI techniques\n\nIam assuming basic stuff like excel, SQL (basic) is already known.\n\nAlso please note that data scientists require different skill sets than business analysts who require another set of skills from optimization skill sets.\nIt's almost not possible to understand everything in depth but may be if you spend enough time in analytics and data sciences you will be able to know what you still do not know and what you do not understand.\n\nHope it helps some of you!",
                "Because data science helped elect Donald Trump!\n\nIf there\u2019s one person who deserves much more respect than he gets and (I think) is much smarter than people think, it\u2019s Jared Kushner (Trump\u2019s son-in-law), who oversaw the data operation for the Trump campaign.\n\nAccording to this [ https://www.forbes.com/sites/stevenbertoni/2016/11/22/exclusive-interview-how-jared-kushner-won-trump-the-white-house/#71e3add43af6 ] Forbes article, he built a 100-person data hub in Texas, which dictated every campaign decision including travel, fundraising, advertising, rally locations and even the topics of the speeches. He seems to have understood the true power of data-driven decision making and machine learning for sentiment manipulation and message tailoring.\n\nAs a simple example, the data operation extensively used social media micro-targeting to move from selling $8000 worth of merchandise per day to $80,000 per day, thus massively increasing the number of human billboards (aka \u201cMake America Great Again\u201d hats) in key swing states.\n\nYes, Obama did something similar in 2007. But a lot has changed in 9 years, especially the relevance of social media. From the same article:\n\n\n%3E \"Jared Kushner is the biggest surprise of the 2016 election,\" adds Eric Schmidt, the former CEO of Google, who helped design the Clinton campaign's technology system. \"Best I can tell, he actually ran the campaign and did it with essentially no resources.\"\nAnd that is why data science is important. The amount of data the world generates is increasing exponentially and with that, the number of \u201cunconventional\u201d fields (politics, sports, journalism, etc.) that can now benefit from data is also ever increasing.\n\nIn short, data science is going to be important because it is not just a \u201ctech\u201d topic anymore.",
                "I am working as a Data Scientist myself therefore it makes me qualified enough to answer your question.\n\nAlso I will make sure to include the tricks in my answer that worked for me.\n\nSo Let's begin, Shall we?\n\nI will be answering this question, keeping in mind that a bunch of readers could be complete newbies into programming.\n\nSo addressing non-computer science students. Firstly, you need to work a lot on your problem-solving skills which is going to help you code effortlessly. You can achieve this by learning Data structures & Algorithms and coding in it. Also, DS & Algo are the building block of computer science so it will definitely help you on your Journey towards excellence in coding.\n\nAfter you are comfortable with problem-solving, you should stick to the below mentioned points:\n\n1. Opt for a good course on Machine learning and study it thoroughly to become well versed with all it\u2019s concepts.\n2. Practice machine learning problems on Kaggle: Your Machine Learning and Data Science Community [ https://www.kaggle.com/ ] which will help you gain confidence and give you enough hands-on skills.\n3. Post your projects on GitHub, LinkedIn and also you can use youtube to showcase your skills\n4. Now it\u2019s Time to market yourself. Make a clean and creative online portfolio and a strong resume based on ML. Start applying to your desired companies and surely circumstances will bend in your favour and soon you will become something you have worked so hard for and that is \u201cData scientist\u201d\n5. you can connect with me on LinkedIn\nPs: I am attaching my photo, in which you can see me working from home, just in case you are interested to know how a data scientist looks?! \ud83d\ude1b\n\n",
                "Data is a magical stick that is used to extract valuable insights to guide businesses, organizations, and industries to make informed decisions. \ud83d\udcca To do that, an aspiring data scientist needs specific skill sets to make better decisions using their analytical and problem-solving skills. There are some skills like programming, ML, statistics, and domain expertise are required to master a versatile field and work as a data scientist.\n\nSo, without wasting any time, let\u2019s look at the typical skills needed for a data scientist job role: \ud83e\uddd0\n\n1. Python and R is essential for data analysis, manipulation, and modeling.\n2. Knowledge of statistical concepts and techniques like regression, hypothesis testing, etc.\n3. Machine learning skills in Scikit-Learn and Deep Learning.\n4. Knowledge of Big Data Technologies.\n5. SQL and Database management\n6. Knowledge of concepts like data warehousing and data wrangling.\n7. Knowledge of Tools and techniques used in this field like Pandas, NumPy, Tableau, Matplotlib, etc.\n8. Domain Knowledge to make meaningful interpretations from data.\n9. Lastly, soft skills are equally important such as communication skills, problem-solving skills, data representation skills, etc.\nYou can develop those skills in 2 ways:\n\nBy learning at your own pace with the help of books \ud83d\udcdaand free videos available on the internet. Or, opt for an online course \ud83d\udcbbthat will help you to understand those DS concepts with practical training and live classes.\n\nHowever, the skills needed for a data scientist are of an advanced level and need hands-on experience to tackle real-world problems. So, it is advisable to go for degree programs where you can learn advanced skills and gain hands-on experience on project work.\n\nPursuing a master's degree in data science is much needed if you\u2019re applying for job roles like data scientist. Moreover, employers are also interested in those candidates who have higher qualifications in DS. Because it shows your in-depth knowledge of foundational concepts and ability to handle real-world challenges with problem-solving skills.\n\nHowever, earning a degree certificate is now an easy process through an online institute. These institutes provided degree programs accredited by top universities.\n\nSo, let\u2019s explore some programs:\n\n * Udacity - Programming in Data Science with Python (Nanodegree Program) \u2611\ufe0f\n * upGrad - Advanced Certificate Programme in Data Science \u2611\ufe0f\n * Learnbay - Master\u2019s Degree Program in Data Science and AI \u2611\ufe0f\nWhich program will be suitable for those who are looking for specific domain skills?\n\nYou can go for,\n\nThis institute offers domain-specific skills in its \u201cMaster's Degree Program in Data Science and AI by Learnbay.\u201d It is suitable for working professional who wants to earn a master\u2019s degree in domain-specific. The course is designed as per the academic curriculum.\n\n * \n * Furthermore, participants can access domain-elective \ud83e\udd29 in the fields of BFSI, Retail, Healthcare, Manufacturing, Managers and leaders, Ecom, and Supply Chain. Having this advantage will unlock new opportunities and help you to land a job in your desired company.\n\nThe highlighted features is that they offer in an online environment (Hybrid Learning). So, you don\u2019t need to relocate or pay high course fees. You will learn through live classes and gain exposure to practical training at Experience Centers. \ud83c\udfe2\n\n * \n * Experience centers are situated in Delhi, Pune, Hyderabad, and Bangalore.\n\nNOTE: Practical Training is provided in both online and offline mode. So, you can choose at your convenience.\n\n * LIMITATION - Udacity and Upgrad degree programs don\u2019t offer domain specialization courses. For practical training, they offer online training (No offline practical centers or labs are available for training.)\nMoreover, you will receive a certificate that is recognized by 60+ Western countries along with the Asia Pacific and it\u2019s also equivalent to a degree in Europe, the US, and Canada. \ud83c\udf10The certificate is named as a \u201cMaster\u2019s Degree Certificate accredited by Woolf.\u201d\n\n * \n * This certificate will allow students to connect with global peers and boost their career opportunities. \ud83e\udd79\n\nAdditionally, you will also earn an industry-accredited certificate for the project from IBM that showcases your skills and knowledge.\n\nNot only this, an IBM Project certificate will be given for simulated real-time and capstone projects namely Learning and developing classification techniques for the digital transformation of banking, Building a content recommendation model on the basis of regional viewer categorization, etc.\n\n * LIMITATION - If you\u2019re going for upGrad or Udacity degree program you will receive a certificate upon completion of the course. In addition, their degree programs focus on real-time projects more than theoretical knowledge. But, they don\u2019t offer project certificates that validate your practical knowledge.\nHowever, to get hired by your desired company, this institute offers a \u2018Career Service Pro\u201d feature, which includes 100% placement support and career guidance. \ud83d\udcab Participants can opt for online or offline services in the cities of Pune, Hyderabad, Bangalore, and Delhi.\n\n * This is the premium feature offered in this \u201cMaster Degree Program in Data Science and AI of Learnbay Institute.\u201d Other services include:\n * \n * 1:1 resume session\n * Unlimited interview calls\n * 5+ mock interviews\n * 3-year job support\n * Career guidance from industry experts.\n\nLIMITATION - upGrad and Udacity platforms offer career guidance, but 100% placement assistance is not available. Moreover, it is provided in an online mode. So, you will not receive any offline career guidance.\n\nAfter completing this program, you will get opportunities in different job roles like data analyst, product analytics, NLP Engineers, and many more. \ud83e\uddd1\u200d\ud83d\udcbb\n\nFinally, To land a job as a data scientist, it is crucial to upgrade your skills from a degree program in data science by mastering various concepts, tools, and techniques. \ud83c\udfafAdditionally, acquiring domain expertise can help you excel in a particular industry and provide long-term growth with a plethora of opportunities.\n\nThank You for reading this answer!",
                "Yes, you can become a self-taught data scientist. I'm assuming that you\u2019re in a position of having a full-time job and want to self-teach yourself to become a data scientist. There are many other great responses here that've given you a ton of material to go through. This is great but the problem with this approach, is that when do you stop? There are years upon years of content and theoretically, you can learn data science forever. Here\u2019s what it takes to become a self-taught data scientist:\n\nMindsets You Must Internalize:\n\n * Always Be Learning: The reality of this field, is that there are always new packages, libraries, algorithms created. This means, you must always be willing to learn new tools, new methodologies. Many things you do today, may be outdated in a few years.\n * Figuring things out on your own: Many times, you will encounter bugs or problems where you have nobody there to answer your questions. You must get good at figuring out things on your own. This means, reading stack overflow, blog posts, videos to teach yourself new concepts.\n * Handle Frustration: You must be able to withstand frustration when you are doing a lot of work and there is seemingly no progress. You must be comfortable with running lots of failed experiments. You must be comfortable spending hours debugging code.\nOnce you understand those mindsets, here's the progression I'd recommend:\n\n1. Pick an interesting data problem that you're excited about: The purpose of data science is to solve problems. Learning data science is a difficult, difficult process . The way to maintain enough motivation to push through these obstacles, is to be working on a problem you\u2019re genuinely interested in. Perhaps, it's composing music using deep learning, predicting the price of bitcoin, visualizing basketball shot charts etc. Start with an interesting problem and find interesting projects that people have done.\n\n2. Find someone\u2019s github who has built a project you\u2019re excited about: Finding someone else\u2019s open-sourced code will give you direct feedback on where you are in terms of skill level. By finding another project, this also gives you a solid \u201cgoal\u201d to aim for with your project. Don\u2019t worry about understanding the code, we just need a goal.\n\n3. Break down the project into bite-sized chunks and then find resources that fill these chunks of knowledge: I like using the CRISP-DM methodology [ https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining ] in building data science project. The principle here, is that we learn just enough to be able to move forward in the CRISP-DM methodology. Pick one resource and use the rest as supplemental resources. Don\u2019t drown in information. Pick the resources that resonate best with your learning style.\n\na. Programming: You'll need to build your project in some language, so you'll need programming. Either R or Python will do:\n\n- Zed Shaw's Learn Python the Hard Way\n\n- Google's Python Course: Google's Python Class | Python Education\n | Google Developers [ https://developers.google.com/edu/python/ ]\n\nb. Data Acquisition: To get your data you could find them using the ready-made sites or scrape your data:\n\n- Kaggle [ http://kaggle.com ]\n\n- data.world [ http://data.world ]\n\n- 100+ Interesting Data Sets for Statistics - rs.io [ http://rs.io/100-interesting-data-sets-for-statistics/ ]\n\nor....\n\n- Building your own web scraper: https://www.dataquest.io/course/apis-and- scraping\n\nc. SQL: Building your own projects won\u2019t require you to necessarily need SQL. However, SQL is EXTREMELY IMPORTANT if you want to work as a data scientist at any company. I guarantee you will also be tested on this in interviews. Good resources:\n\n * hackerrank [ https://www.hackerrank.com/domains/sql ]\n * Mode Analytics\u2019 SQL Course [ https://community.modeanalytics.com/sql/tutorial/introduction-to-sql/ ]\nd. Data Cleaning/Transformation: So you know how to code and you have data. How do you actually start manipulating the dataset? If you chose Python, you'll need to learn Pandas or Numpy. If you're using R, these libraries are built-into the language:\n\n- Numpy & Pandas: 10 Minutes to pandas [ http://pandas.pydata.org/pandas-docs/stable/10min.html ]\n\ne. Data Visualization: Viz + cleaning/transformation iteratively go together. This means, that you transform to get a certain visualization and then transform again to get another visualization. Great viz resources:\n\n- R: Hadley Wickham's R for Data Science: http://r4ds.had.co.nz/data- visualisation.html#the-layered-grammar-of-graphics\n\n- Matplotlib: Data Visualization With Matplotlib Course [ https://www.dataquest.io/course/exploratory-data-visualization ]\n\nf. Statistics: Once you create histograms, boxplots etc, it'll be important to be able ot understand these diagrams. To do this, you'll need statistics. Khan Academy [ http://khanacademy.org ] is great for these concepts.\n\ng. Linear Regression/Logistic Regression:\n\n- Read the Linear & Logistic Regression sections of ISLR: http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf\n\nh. Machine Learning: Use arxiv [ https://arxiv.org/list/stat.ML/recent ] to find research papers on a variety of algorithms. Because you found someone who\u2019s built your project, you already know which algorithms they used.\n\nGrab a pen & notepad and really dig into the research papers. It\u2019s likely that you won\u2019t understand anything the first time you read a research paper. Don\u2019t give up. I probably re-read a paper 10+ times to make sure I understand how the algorithm works.\n\n4. Make your project public on github, a blog and write a good README: You want this project to be part of your portfolio, essentially proof that you can build data science projects. Write a good README explaining your thought process on why you chose certain algorithms. Articulating this also prepares you well for interviews, as companies will ask you about this.\n\n5. Repeat: Do this multiple times so you build out your portfolio.\n\n6. Networking/Job-Hunting: Do this concurrently with Step 5. This means, going out to meetup events, using LinkedIn to connect, asking for intros. This step is just as important for becoming a data scientist, however, this would require another post.\n\nData Science interviews are actually a separate beast to tackle, with whiteboarding, coding challenges, take-homes. This also, will require another post.\n\nAll in all, I\u2019d say, becoming a self-taught data scientist, will require at least 500\u2013700 hours of learning upfront. Whether you want to do this in 3 months, a year, two years, depends on your situation. After you finish these 500 hours, you should know just enough to get an entry-level data scientist position. Once you\u2019ve got a solid portfolio setup & your skillset honed, you should split your time 50/50 studying for interviews + job applications.\n\nIf you love my content, visit my website at www.jefflichronicles.com [ http://www.jefflichronicles.com/ ].",
                "Data scientists require a diverse and dynamic skill set to thrive in their role as the architects of data-driven decision-making. First and foremost, proficiency in programming languages like Python and R is essential. These languages are the backbone of data manipulation, statistical analysis, and machine learning model development. Data scientists use them to collect, clean, and preprocess data, making it ready for analysis.\n\n * Strong statistical knowledge is another cornerstone. Data scientists must understand a wide range of statistical methods and apply them effectively to draw meaningful insights from data. This includes hypothesis testing, regression analysis, and Bayesian statistics, among others.\n * Machine learning expertise is crucial, as it enables data scientists to build predictive models. This involves selecting the right algorithms, and training models, and evaluating their performance. Machine learning is integral for tasks like recommendation systems, fraud detection, and natural language processing.\n * Data visualization skills are vital for conveying insights to stakeholders. Tools like Matplotlib, Tableau, and Power BI are used to create compelling visualizations that make complex data accessible and actionable.\n * Domain knowledge is valuable, especially when working in specific industries like healthcare, finance, or retail. Understanding the intricacies of the industry helps data scientists identify relevant problems and develop tailored solutions.\nFrom where can you learn these skills?\n\nCompleting a Master's in Data Science online can help you gain experience with remote work and demonstrate your ability to manage time and tasks independently, which is becoming increasingly valuable in the job market.\n\nHere are the suggested programs\n\n1\ufe0f\u20e3Learnbay\n\nMany find that the most effective path toward a career in Data Science and AI is by pursuing a Master's degree. In recent job market trends, there has been a decline in the demand for certain standard certification courses, with recruiters increasingly preferring candidates with master's degrees in data science and AI.\n\n * Masters Degree in CS: Industry Partners In Collaboration with Data Science & AI\nEligibility: BE/B.tech/M.tech, MBA, BSC, BCA, BBA, Bachelor/Master in commerce. Minimum 60% marks in aggregate in bachelor\u2019s degree. This makes it accessible to both tech and non-tech professionals, and the program has a duration of 18 months.\n\nHighlights:\n\n * \n * Practical exposure is facilitated through experience centers located in cities such as Bangalore, Delhi, Hyderabad, and Pune. It employs a hybrid learning model that combines online and offline methods to cater to the needs of both working professionals and full-time students.\n * Regarding specialization, students have the option to choose from domains, including BFSI, Retail, Healthcare, Manufacturing, Managers and Leaders, Ecom, and Supply Chain allowing them to align their education with their specific career objectives.\n\nLearnbay\u2019s program holds accreditation by Woolf, and upon completion, students receive project experience certificates from IBM and a Master\u2019s degree in Data Science and AI Course of Learnbay, which is recognized in over 60 countries, offering potential immigration opportunities.\n\nIBM Project Certificate will be provided for the simulated real-time and capstone projects like:\n\n * \n * Learning and developing classification techniques for the digital transformation of banking, Building a content recommendation model on the basis of regional viewer categorization, etc.\n\nIf you are interested in other opportunities too, here is another platform that provides a data science course:\n\n2\ufe0f\u20e3LinkedIn Learning provides a \"Data Science and AI Master's Certification\" course that offers a comprehensive exploration of essential data science and artificial intelligence concepts and tools. This fully remote program is designed to impart practical knowledge through case studies and assignments, ensuring students gain valuable hands-on experience.\n\nBut Learnbay provides career assistance that LinkedIn Learning does not provide.\n\nIn terms of career support, the program offers robust assistance to graduates in overcoming job placement challenges through career pro services. This includes 100% placement assistance and a three-year resource package comprising six mock interviews, personalized mentorship, and numerous offline opportunities in cities like Bangalore, Delhi, Hyderabad, and Pune.\n\nWhat are the career opportunities?\n\n * \n * Data Scientist or Data Engineer\n * AI Research Scientist\n * Business Intelligence Analyst\n * MLOPs Engineer\n * Product analytics\n * Data Analyst\n * Natural Language Processing (NLP) Engineer\n\nImportant Highlights:\n\n * Globally recognized in Europe, the US, Canada, and 60+ countries\n * Industry-recognized Master's Degree\n * Immigration Opportunity\n * Equivalent to a Master's Degree in the US\n * Master's degree accredited by the EU and recognized globally\nSimilarly, there is another platform that might interest you.\n\n3\ufe0f\u20e3Coursera offers an engaging data Science program called 'Data Insights Mastery.' This course equips learners with valuable skills in data manipulation, statistical analysis, and data visualization, fostering a deep understanding of data-driven decision-making.\n\nConclusion:\n\nThe role of a data scientist is multifaceted, demanding a wide array of skills and competencies. Proficiency in programming, statistical knowledge, and machine learning expertise form the technical foundation, allowing data scientists to collect, analyze, and model data effectively. Data visualization skills aid in communicating insights, while domain knowledge enhances problem-solving in specific industries. Soft skills such as critical thinking and effective communication are vital for translating data into actionable recommendations and collaborating across teams.\n\nData scientists are not just data analysts; they are problem solvers, storytellers, and decision-makers, driving innovation and informed choices in a data-driven world. The combination of these skills positions data scientists as essential assets in organizations seeking to harness the power of data for competitive advantage and strategic growth.\n\nHappy Reading!",
                "I\u2019d say, in general, they\u2019ll be the same \u201cdream\u201d companies that any other software engineers want to work for. Google, Netflix, Facebook, etc.\n\nMy dream company for data engineering/science would have to be Spotify, though! I don\u2019t know much about the engineering culture because I haven\u2019t worked there.\n\nHowever, I have always had this thought that it would be SO cool to build a GOOD recommendation system for playlists. I think Spotify actually does this pretty well already!\n\nBut how sweet would it be to build a music streaming service with top-notch playlist recommendations? Think something like: being able to listen to a few of your favorite songs and then have the service build you an entire playlist curated just for your tastes! Sounds like a prime use-case for some machine learning goodness.\n\nObviously easier said than done. I\u2019m always on the hunt for that perfect playlist, though.",
                "Entering a new company as the first data scientist is a daunting challenge filled with many obstacles. You have to build trust, and you have to start delivering value as soon as possible, but the organization isn\u2019t set up for you to succeed.\n\nThe good news is that if you can go through the 8 steps I outline below, you can put yourself in a good position after all.\n\n1. Introduce yourself to your new colleagues and get to know them. This one should be fairly self-explanatory.\n2. Get access to the data. Basically, you want access to all the data you can get your hands on. Databases, file servers, ad-hoc Excel sheets, you name it. This often takes more time than it should, and you might have to remind the right people two or three times before you finally get the access you need.\n3. Explore the data. Depending on how well you know the domain, it can take quite some time before you have a basic understanding of the main data sources. This will typically involve talking to domain experts and DBAs (if there are any).\n4. Assess the data infrastructure. Do you have the right data and is it properly structured and stored in a sensible way so that you can start data science initiatives? Inevitably, the answer to these questions is going to be a big fat no. Before a company has a data scientist, they have no idea of what kind of data a data scientist needs. You will be horrified and disgusted at what you see, and you will want to change everything.\n5. Perform feasibility studies. Now that you know that all your data is useless, it\u2019s time to identify problems that you can solve and how/if you can get the data to solve them. In this phase, you have to talk to managers high up in the company hierarchy. If not, you\u2019ll just end up spending months trying to solve problems that have no business value.\n6. Initiate data engineering initiatives. You thought you were a data scientist? Well, think again. For the next few months, you\u2019re going to be a data engineer. If you don\u2019t have the skillset for that, you\u2019re either going to have to convince the company to hire a data engineer, learn how to become one yourself or quit the job. I\u2019m serious. If you don\u2019t get the data infrastructure properly set up at this point, you will fail miserably.\n7. Experiment and develop prototype. You have a proper data infrastructure now? Good. You\u2019re past the most difficult part. What you need to do now is attack a small, well-defined problem with clear business value. Hopefully, you have identified such a problem. If not, go back to step 5.\n8. Present your findings to managers. Once you find something interesting, you have to show it to the right people. Remember to drop the machine learning lingo and talk plainly and clearly. Good visualizations are important.\nIf you have successfully achieved all of this, you are in the right position to succeed in your job as the first data scientist. From there, the sky is the limit.",
                "I am working as a Data Scientist myself therefore it makes me qualified enough to answer your question.\n\nAlso I will make sure to include the tricks in my answer that worked for me.\n\nSo Let's begin, Shall we?\n\nI will be answering this question, keeping in mind that a bunch of readers could be complete newbies into programming.\n\nSo addressing non-computer science students. Firstly, you need to work a lot on your problem-solving skills which is going to help you code effortlessly. You can achieve this by learning Data structures & Algorithms and coding in it. Also, DS & Algo are the building block of computer science so it will definitely help you on your Journey towards excellence in coding.\n\nAfter you are comfortable with problem-solving, you should stick to the below mentioned points:\n\n1. Opt for a good course on Machine learning and study it thoroughly to become well versed with all it\u2019s concepts.\n2. Practice machine learning problems on Kaggle: Your Machine Learning and Data Science Community [ https://www.kaggle.com/ ] which will help you gain confidence and give you enough hands-on skills.\n3. Post your projects on GitHub, LinkedIn and also you can use youtube to showcase your skills\n4. Now it\u2019s Time to market yourself. Make a clean and creative online portfolio and a strong resume based on ML. Start applying to your desired companies and surely circumstances will bend in your favour and soon you will become something you have worked so hard for and that is \u201cData scientist\u201d\n5. you can connect with me on LinkedIn\nPs: I am attaching my photo, in which you can see me working from home, just in case you are interested to know how a data scientist looks?! \ud83d\ude1b\n\n",
                "This is a long\u2026really\u2026long answer. That will allow me to introduce newcomers, recruiters, seasoned professionals, C-level executives to the Data Science world. You need to persist, always. Read until the end, please.\n\nFirst of all, being a Data Scientist in a small company is TOTALLY DIFFERENT from being a Data Scientist on a multinational/large company. May I repeat? TOTALLY DIFFERENT\u2026We\u2019ll discover why together.\n\nBased on that, check out below my favorite posts about this different perspectives on \u201chow do I feel at work\u201d. It\u2019s clear that the authors described totally different scenarios that can exist between somewhere around a small company and a large corporation.\n\nSamson Hu, Data Scientist working in 500px - Small company - Total of 1 Data Scientist: Full Post [ https://medium.com/@samson_hu/building-analytics-at-500px-92e9a7005c83 ]\n\n * One sole warrior. Sometimes on \u201crogue\u201dor \u201cstand-alone\u201d mode;\n * Need to convince people about importance of data;\n * Do what needs to be done. ETL, (back)front-end, prototype, presentations etc.\nNishikant Dhanuka, Data Scientist working in Booking [ http://Booking.com ] - multinational - Total of 120+ Data Scientists: Full Post [ https://towardsdatascience.com/diary-of-a-data-scientist-at-booking-com-924734c71417 ]\n\n * Work in some narrow and closed scope;\n * Large team with each one specialized in some sort of data niche ;\n * \u201cEvangelized\u201d and aligned company;\n * Ps.: there is no right or wrong, if the environment and role are aligned with your profile.\nOn the other hand, we can still perceive some similarities between both worlds. I remember reading a post on Linkedin along these lines:\n\n\n%3E Ask the interview candidate about how much time he thinks that will be spent on data modeling.\n\nIf he answers more than 20%. Don\u2019t hire.\nThe fun part is that the post is telling us pretty much the truth.\n\nIt\u2019s true that some outliers are working on highly-specialized AI positions at IBM/Facebook/Google research departments, but in the end, for the mortals, its an accurate interview scenario.\n\nData Scientist main characteristics\n\nWhen I say main characteristics I need to be clear that we have an assumption that a Data Scientist is already good with math, statistics and computing plus a cherry on the cake, at least one year of experience in some market niche (domain knowledge).\n\nData Scientists ARE NOT plug and play. You can just unplug a Data Scientist from a cancer research program and plug into a stock market consulting role.\n\nSo with all said. That's the basics, that you see everywhere. No need do discuss or reinvent the wheel.\n\nData Scientists need to convince people of the importance, usefulness and accuracy of their work. Most of posts don\u2019t mention that. Let's think in hypothetical scenarios to illustrate the persuasion need:\n\n1. You show the output of a black-box neural network with maximum precision and recall. Apparently blindly ignoring your numbers, investors don't believe in the results and start an endless Q&A session. They say that they will invest millions of dollars on the idea only after a clear understanding of assumptions and a successful experiment\u2026and so on.\n2. Try to explain what is a moving average to a salesperson? They are supposed to know the difference between median, mean an mode, right? Or some basic thing like quartiles and a tricky question about bagplots. Probably by now, you noticed that there is a inherent evangelism role for every data science position. Tools are only good if people are using them. You can provide a set of outputs, predictions and reports, but they are useless if people don't understand or use. Adoption, adoptio\u2026adoption!\n3. You schedule a Friday meeting to transfer knowledge from Data Science to other departments like sales, marketing, maintenance and accounting. You have a naive goal to explain and teach what is a Fast Fourier transform (FFT), Taylor series, Laplace transforms. Near the end of the meeting, when you realize, people will be dead or in a coma. They were killed by boringness. Data is a bunch of fun only for data guys, remember it. A more successful approach would be to teach integrals by referring to them as the area below the curve. Geometry instead of algebric, Socrates! Visual communication instead of just words or equations.\nSo let\u2019s face it. We (analytical people) speak a different language and it needs to be translated into the world\u2019s lingua franca. Otherwise, no one will listen, or even worse, they will pretend to listen and understand.\n\nBy now you are supposed to be convinced that one of the most important characteristics of a Data Scientist is \u201cevangelism\u201d, \u201csell your fish\u201d. Not everyone can abandon \u201cgut feelings\u201d and be \u201cdata-driven\u201d by themselves.\n\nThere is another characteristics related do evangelism present in most of the well-know and seasoned Data Scientists. They can communicate very complex concepts in a way that people will understand (or maybe get a grasp of the actual meaning).\n\nOld versus new\n\nYou can be lucky to start working in a new company with all that fresh mentality of Business Intelligence, KPIs, predictive and prescriptive analytics, real-time dashboards, data access to everyone and the AI term being used every where (when actually is just machine learning. I forgive you, marketing department!).\n\nOr\u2026\nYou can start to work for a +100 years law company where your main job will be to convince others that your work is valuable and it\u2019s not just another \u201chype/trend\u201d being followed by a managerial decision.\n\nThe dream of every company, the central data repository of truth and a data-driven mindset can only be achieved with everyone embracing this long-term mission. A Data Scientist, can be the facilitator, a virus (in a good sense), but the virus needs to proliferate (create allies) in order to facilitate \u201cfever\u201d to shake down things.\n\nDomain Expertise/Knowledge\n\nThere was a competition for improving the Paypal fraud detection. In the middle of all the Kaggle obsession with algorithms, the company responsible for developing the winning solution was very clear. They didn\u2019t use any fancy algorithm or a deep neural network with thousands of hidden layers. They only used their past acquired experience of 10 years working with fraudulent transactions.\n\nIt\u2019s really important to understand this concept. People think in Data Scientists as the \u201cjoker\u201dcard, a wildcard character. It's true that some abilities that rely on programming, logic thinking and math/stats are portable to different industries. However, the domain expertise is highly related to the industry that the Data Scientist has more familiarity and years of work.\n\n\n%3E This is not supposed to be a new concept, right? It\u2019s been that way for all the history of humankind, for all professionals.\nThe problem is that some people took the \u201cgeneralization\u201d of certain intelligent models too seriously, as granted. Then, they applied to everything and everyone without restrictions.\n\nA Data Scientist with 5 years experience with banking will be totally different from one that spent one decade researching cancer. Different technology stacks (R or Python) and/or different approaches for similar problems. They can be interchangeable, but the learning curve is significant and painful. This needs to be take into account (special attention for HR recruiters reading this paragraph).\n\nData Science is all about controversy\n\nI don\u2019t personally like these two answers below, but I think that it\u2019s good to read them to hear different opinions about Data Scientists.\n\n * Michael Hochster [ https://www.quora.com/profile/Michael-Hochster ]'s anwser about Data Scientists, basically defines two types. Type A (Analysis) and Type B (Build) Data Scientists.\nI disagree with that statement. When I say disagree, I mean REALLY disagree. The main reason it\u2019s that the definition is too simplistic. The data market will not evolve with terms like type B or type A.\n\n\n%3E \u201cHey, I\u2019ve just landed a job position as Data Scientist Type B at the TypeC Company\u201d.\nThis is well aligned to the trend of IT industry of calling people by uninspired terms liks \u201carchitect\u201d or \u201cengineer\u201d. You can be a \u201csoftware engineer or a database architect\u201d, but if you have a computer science bachelors you are a \u201ccomputer scientist\u201d, not the single-word term \u201cengineer\u201d. I'm a mechanical engineer, thus an engineer. Same thing for architects. They are\u2026architects, simple as stated, architects.\n\nYes, it\u2019s good to discuss about these picky things for us to not lose control.\n\nAnother idea that I disagree but its good to understand is the conception from Ingo Mierswa [ https://rapidminer.com/author/ingo/ ]: Founder & President of RapidMiner.\n\n\n%3E Data scientists are people who apply all those analytical techniques and the necessary data preparation in the context of a business application.\nLet\u2019s go back to the case of an independent Data Scientist researcher fighting big corporations, mining data and trying to extract patterns that will help to cure cancer. I certainly think that it's not on the context of \u201cbusiness applications\u201d.\n\nDoes a Data Scientist develop products or product features?\n\nAnother great missconception. I really liked my summer internship at Cognitive Class  [ https://cognitiveclass.ai/ ](IBM Canada), because we were not developing products. We were developing courses and exploring algorithms and data.\n\nYes, you can improve the gamified leaderboard of an app. No problem with that, but we can\u2019t confine Data Scientists to work only with products.\n\nAs of today, I founded a consultancy company that have Data Scientist working with consulting and products.\n\nThe result of a Data Scientist analysis can affect a business decision, can motivate someone to change his/her career (like some of my posts and based on the private feedback that I'm receiving), endorse a hiring budget and many other possibilities. It doesn\u2019t need to be a product, it can be a service to external and/or internal clients!\n\nDoes A Data Scientist works near business, money flow, \u201cplata\u201d?\n\n\n%3E \u201cIt's All About the Money, Honey! \u201d\nOkay, now you want to talk about business.\n\nData Scientists working on Bussiness Intelligence, Strategy Consulting, banking and finance can have high-paying careers. The downside is that we can\u2019t take only these profiles into account to define a whole group of professionals.\n\nLet\u2019s get some data if we're talking about money:\n\nRef.: Oreilly 2016 data science salary survey [ http://www.oreilly.com/data/free/files/2016-data-science-salary-survey.pdf ]\n\nSometimes I can get astonishined by seing how the Central limit theorem  [ https://en.wikipedia.org/wiki/Central_limit_theorem ]really works for when you have a large  [ https://stats.stackexchange.com/questions/269/what-is-the-difference-between-a-population-and-a-sample ]sample of your population.\nThe survey looks approximately as a \u201cBell Curve\u201d (aka Normal distribution [ https://en.wikipedia.org/wiki/Normal_distribution ]) with the mean around 100k ~120k US Dollars.\n\nA lot of money!\n\nWe are getting closer on this post to find out why some companies are willing to pay that much for Data Scientists (if you are not convinced yet, remember, this is just the AVERAGE)\n\nDefinitions\n\nit\u2019s not only one, you are dealing with Data Science [ https://www.quora.com/What-is-data-science/answer/Luis-Martins-200 ]. Your eyes need to bleed for things start making sense.\n\nAfter this long post, I\u2019ll share my proposal that is more aligned with what I\u2019ve seen around my feed and what, at the same time, makes more sense (at least for me):\n\nData Engineer\n\n * Extract, transform and loads data from all types of sources\n * Maintain pipelines and databases (Some overlap with a DBA [ https://en.wikipedia.org/wiki/Database_administrator ])\n * Interface between back-end/database team and Data Scientists team\nMachine Learning Practitioner/Specialist/Engineer\n\n * Data Modeling. If the company is large enough, works only developing models;\n * Create containers to make the models \u201cproduction ready\u201d;\n * Test, Learn and deploy new algorithms (\n * \n * There is a lot of algorithms to learn, believe me, just Artificial Neural Networks would take all the 71.5 years of average life expectancy for a human being to be properly learned;\n\n * Optimization\nData Analyst/Business Analyst\n\n * Can be related to business, but not necessarily;\n * Can consume data from the models generated by the Machine Learning Practitioner or straight from data pipelines developed by Data Engineers;\n * Share insights with plain english.\nMathematicians and statisticians\n\n * Do everything that a Machine Learning Practitioner does but without all the glamour and hype;\n * Propose experiments to prove or reprove hypothesis (e.g. A/B Testing).\nData Scientist (The unicorn, The Asymptote [ https://en.wikipedia.org/wiki/Asymptote ], The GOAL!)\n\n * + 10 years of experience\n * Needs to have experience and proven knowledge in all the roles above and be capable of temporarily acting as one of the described positions, if in need. Doesn't mean that should;\n * The glue that put\u2019s everything together;\n * Direct report to upper management level (if in a business context);\n * Domain expertise focus\nOkay, One last chance for those of you that missed the Integral and differential Calculus [ https://en.wikipedia.org/wiki/Calculus ] class.\n\nI like to think in the following way to don\u2019t get too depressed while facing what a Data Scientist could know. To stay with sanity, I focus on what I should know and work to get better on that.\n\nRef.: my Wacom\n\nAccording to my own theory (US Patent pending), the Data Scientist Goal is an Red Dashed Asymptote, something that these blue and green lines will never achieve or get to know what is like.\n\n\u2026 Blue curve is a data professional working and studying 14hrs/day\n\u2026 Green curve is a data professional working and studying 8hrs/day\n\nThe only difference is the rate that they learn and evolve. The sad part is that both will not reach the Data Scientist Goal.\n\nThis approach is similar to:\n\n\n%3E \u201cShoot for the moon. Even if you miss, you'll land among the stars.\u201d - Norman Vincent Peale, author of The Power of Positive Thinking.\nAfter 1x Human life we start to see the prediction of the blue line. To give the blue one a chance, let\u2019s suppose that this poor data professional is actually immortal!\n\nThen, surprisingly, we'll have the dashed blue lines (projection) that still won\u2019t touch the Data Scientist Goal due to the mathematical nature of asymptotes. Theoretically, in the end of its immortality, this blue data professional would be a complete and up to date Data Scientist unicorn .\n\nI hope that this post familiarize you with lots of technical terms that you will face along the way if you are pursuing a data science career, planning to hire someone or invest in Data Science.\n\nPs.: asymptotes are not supposed to be new for you if you are planning to pursue an analytical career.\n\nConclusion\n\nAfter all, that\u2019s why I have the Jr. Data Scientist title on my Linkedin [ https://www.linkedin.com/in/luisotsm/ ] profile. It\u2019s something that reminds constantly my ego that we don\u2019t know anything, we don\u2019t have a clue of what is about to happen next and the only certainty is that we need to learn more. [Footnote: Update: 2018\u201303\u201326]\n\n\u201cEssentially, all models are wrong, but some are useful\u201d - George Box\n\n\nYou can know more about me on my personal site. Follow me if you would like to receive notifications about answers related to Data Science, Machine Learning, Artificial Intelligence and Engineering.\n\nAlways upvote answers that you find useful. Everyone can be wrong so be respectful and polite.\n\n\n[Update: 2018\u201303\u201326, I had to remove \u201cJr.\u201d from my profiles due to the increasing number of \u201cJr.\u201d job offers. People just don\u2019t get it. I conceptually still think in \u201cData Scientist\u201d as described on this answer]",
                "I am working as a Data Scientist myself therefore it makes me qualified enough to answer your question.\n\nAlso I will make sure to include the tricks in my answer that worked for me.\n\nSo Let's begin, Shall we?\n\nI will be answering this question, keeping in mind that a bunch of readers could be complete newbies into programming.\n\nSo addressing non-computer science students. Firstly, you need to work a lot on your problem-solving skills which is going to help you code effortlessly. You can achieve this by learning Data structures & Algorithms and coding in it. Also, DS & Algo are the building block of computer science so it will definitely help you on your Journey towards excellence in coding.\n\nAfter you are comfortable with problem-solving, you should stick to the below mentioned points:\n\n1. Opt for a good course on Machine learning and study it thoroughly to become well versed with all it\u2019s concepts.\n2. Practice machine learning problems on Kaggle: Your Machine Learning and Data Science Community [ https://www.kaggle.com/ ] which will help you gain confidence and give you enough hands-on skills.\n3. Post your projects on GitHub, LinkedIn and also you can use youtube to showcase your skills\n4. Now it\u2019s Time to market yourself. Make a clean and creative online portfolio and a strong resume based on ML. Start applying to your desired companies and surely circumstances will bend in your favour and soon you will become something you have worked so hard for and that is \u201cData scientist\u201d\n5. you can connect with me on LinkedIn\nPs: I am attaching my photo, in which you can see me working from home, just in case you are interested to know how a data scientist looks?! \ud83d\ude1b\n\n",
                "Yes, I think I qualify to answer this question. Prior to my current role at Kabbage, I had a (sucessful) 8 year long career in designing graphics, mobile and server processors. The entire process took nearly a year with my interview period lasting roughly 2\u20133 months. I ended up attending 5 interviews on site and had offers from 3. Here is my story.\n\nThe Motivation Phase (1 month)\n\nAs a computer engineering graduate, I had shockingly minimal exposure to statistics and no exposure to machine learning. Initially, the buzz around data science in the tech media got me interested in the area. I pored over blogs, news media and articles online trying to define to myself the idea of a \u2018data scientist\u2019 and what makes one successful in the role. This included exploring stories about data science making a difference in diverse fields from healthcare to recruiting to marketing to education to everything in between. Gradually, the hype turned to genuine potential in my mind. This phase of setting the motivation was hugely important and set up the intrinsic drive to succeed. Otherwise, it would have been highly likely that I would have given up the endeavor before seeing it through to the end.\n\nNext, I set out to figure out the skills required to become a data scientist. The most popular idea on the net is that a data scientist is a super-human who sits at the intersection of programming, statistical (and ML), math, business domain and communication skills. Additionally, I would also like to throw in familiarity with big data tools like Hadoop/Spark/AWS to the mix based on my experience. I could only check off the software engineering skills (hacking skills) in this Venn diagram! One of my primary complaints with my prior job as a computer architect was that I felt that my learning curve was saturating. With such a vast skills requirement, I knew that there would always something new for me to learn for a very long time!\n\nThe Knowledge Phase (9 months)\n\nThe knowledge phase involved signing up for and completing a number of courses online through Udacity [ https://www.udacity.com/ ] (U), Coursera [ https://www.coursera.org/ ] (C) and edX [ https://www.edx.org/ ] (E). While Coursera was somewhat dry and theoretical and focused on the traditional lecture format, both Udacity and EdX focussed on a more interactive learning experience with short videos and tons of built in quizzes and programming questions to help with learning by doing. While this phase lasted a long time, it was because there were huge knowledge gaps to fill. I was also mostly taking these classes in the evenings or weekends after my regular work hours aka spare time. My focus was on statistics, machine learning, exploratory data analysis and some advanced topics (since I already had the programming and software engineering skills from my prior job).\n\nList of classes I took but not necessarily in that order (not just skim, but actually complete all the lectures, assignments and mini-projects for the most part):\n\n1. Descriptive and Inferential Statistics (U) - Critical\n2. Machine Learning by Sebastian Thrun (U) - Critical/Practical\n3. Exploratory Data Analysis (U) - Critical\n4. A/B Testing (U) - Nice to Know\n5. Recommender Systems (C) - Nice to Know\n6. Text Mining and Analytics (C) - Skimmed/Nice to Know\n7. Machine Learning by Andrew Ng (C) - Critical/Theoretical\n8. Introduction to Hadoop (U) - Critical/Nice to know\n9. Introduction to Big Data (E) - Critical/Nice to know\n10. Big Data Analysis with Apache Spark (E) - Nice to know\n11. Algorithms I & II (C) - Mostly for interview purposes\nThe Skill Building Phase (6 months)\n\nThis was a super interesting phase.\n\nAfter arming myself with the necessary fundamentals and practical tools (Python, R, mathematical and statistical base), it was time to put them to work. Enter Kaggle [ https://www.kaggle.com/ ]. For those not in the know, Kaggle is an online competitive platform for budding and experienced data scientists. Its a chance to get your hands dirty with real datasets from real companies to solve real problems using insights gleaned from data. Most of all, it is a vibrant community of like minded people having fun and learning from each other. Kaggle was single handedly successful in teaching me the nuances of experiment design, data pre-processing, feature engineering, model validation, and ensemble building. The gamification of the task at hand (leaderboard, rankings, forums etc) made the experience hugely rewarding and fun at the same time. Not to mention that you can use your achievements on Kaggle to get recruiter eyeballs. For more details on this, see: Vijay Sathish's answer to Do recruiters really care about Kaggle achievements or successfully completed courses in Coursera? [ https://www.quora.com/Do-recruiters-really-care-about-Kaggle-achievements-or-successfully-completed-courses-in-Coursera/answer/Vijay-Sathish ]\n\nThe second phase of the skills building involved trying to get my hands dirty with real world data science projects. One of the drawbacks with Kaggle is that the problem is already defined, the data is provided to you (and mostly in clean format), and success is defined. In the real world, translating a business problem into a data science task, identifying data sources, extracting the data from multiple sources, data cleaning, defining metrics for success, find ground truth or labels are equally critical tasks. The model building and validation part is probably the easiest task. I identified several problem areas in my field at Oracle related to processor workload analysis, performance coverage analysis, and workload sampling - defined the problem, identified the datasets and metrics and got to work using supervised. unsupervised learning techniques and visualizations to tackle the issues at hand. Most of my team being fellow computer architects had minimal or no experience in machine learning, so this was unchartered territory. The insights I brought to the table from a different perspective was hugely rewarding and my colleagues looked at me with new respect. I was essentially disrupting computer architecture within the team! This also further strengthened my resolve to pursue data science as a full time job.\n\nThe Interview Phase (3 months)\n\nThe interview phase is the signaling phase. It is about signaling to the recruiter that you have the skills required on paper and convincing your future manager and team mates that you can execute on those skills on the job. The Kaggle achievements (my profile: VijaySathish | Kaggle [ https://www.kaggle.com/vijaysathish/competitions ] and see: Vijay Sathish's answer to How can a beginner train for machine learning contests? What are the fundamental ideas, tools, and information resources I need to start building expertise in machine learning? [ https://www.quora.com/How-can-a-beginner-train-for-machine-learning-contests-What-are-the-fundamental-ideas-tools-and-information-resources-I-need-to-start-building-expertise-in-machine-learning/answer/Vijay-Sathish ]) and my data science projects at work helped garner the attention of recruiters and get my foot in the door. My theoretical grounding from the coursework and all the experience picked up from actually executing the various data science projects helped convince my interviewers.\n\nI applied for interviews in discrete waves. This means that I would apply to 10\u201315 companies per week and wait for responses. If I got 2\u20133 responses, I would stop applying for sometime, otherwise I would apply for another handful next week. I also focused on the skills required of the job, and the specific industry while applying because the job \u2018Data Scientist\u2019 can take on a surprisingly wide range of possibilities depending on the company. (For example, a job description that included prior experience of NLP, deep learning or computer vision would be outside my expertise /skills, while a job that primarily involved querying databases, A/B testing or product analytics felt more like a traditional analyst role which was not what I was looking for.) I mostly used AngelList [ https://angel.co/ ] and LinkedIn to apply for data science jobs and focussed on medium stage startups. Early stage startups typically have little or no software infrastructure setup, so you would spend most of your time on software engineering rather than data science tasks. This is fine for some people, but this was not my primary focus for my first data science job. Big companies like Google, Facebook, Microsoft etc. have a higher bar and it would be almost impossible to even get past the recruiter stage given their vast candidate pool.\n\nData Science is an upcoming field and attracts professionals from diverse fields. The single most important skill in the interview (and perhaps also at work) is storytelling. You are the best person to market yourself. Highlight your competencies, your motivation for the job and what you can bring to the table. Wrap an interesting narrative around your favorite project. On my phone screen with my (to be) manager at Kabbage, I started with the story of solving the problem of computational resources and time for the company. I explained how I used workload clustering to pick and choose representative workloads from a huge workload space to monitor for weekly performance regressions and brought down computation requirements by 10x. I explained how I used our in-house performance simulator and our experiment logs to extract the data for this study. I could tell that my manager started very skeptical (because she was new to the field of computer architecture), but by the end, she came away very impressed with my effort. I can say that I had won more than 50% of the battle by that point.\n\n\nIn summary, transitioning to data science from a lateral field required immense patience, but it was also an extremely rewarding journey for me in the end. (On transitioning, see: Vijay Sathish's answer to Should I start as a Data Analyst or Software Engineer to become a Data Scientist? [ https://www.quora.com/Should-I-start-as-a-Data-Analyst-or-Software-Engineer-to-become-a-Data-Scientist/answer/Vijay-Sathish ])\n\nFinally, if you are a self taught data scientist and think you have what it takes, check us out and apply for these awesome roles: DataScientist@Kabbage [ https://www.kabbage.com/company/careers/?gh_jid=145222 ] and DataEngineer@Kabbage [ https://www.kabbage.com/company/careers/?gh_jid=145224 ].",
                "Truth be told first - even if it disappoints many people -the industry does not have an agreed upon definition of a data scientist! \n\nJokes like 'a data scientist is a data analyst living in the Silicon Valley' are abundant. Below is one such cartoon, just for fun.\n\n\n\nFinding an 'effective' data scientist is hard and finding people who understand who a data scientist is equally hard. Note the use of 'effective' here, I use it to highlight that there could be people who might possess some of these skills yet may not be the best fit in a data science role. Irony is that even the people looking for hiring data scientists do not know who a data scientist is. Hiring managers post job descriptions for traditional data analyst and business analyst roles while the title calling it a 'Data Scientist' role.  \n\nEverything that I say below is my experience working in a data scientist role with a major search engine and advertising platform. Instead of giving a bullet list of skills, I would first highlight the difference between some data related roles. \n\nConsider the following scenario. Shop-Mart and Bulk-Mart are two competitors in selling retail. Some higher up in the management chain asks this question: \"How many Shop-Mart customers also go to Bulk-Mart?\". \n\n[Please note that the question might be of interest to Bulk-Mart management or even a third party, possibly a market research or consumer behavior company, interested in shopping behavior of the population.]\n\n\n\nHere is how different data-related roles will approach the problem. This \n\nTraditional BI/Reporting Professional: Generate reports from structured data using SQL and some kind of reporting services (SSRS for instance) and send the data back to management. The management asks more questions based on the data that was sent and cycle continues. Insights about data are most likely not included in the reports. A person in this role will be experienced mostly in database related skills.\n\nData Analyst: In addition to doing what the BI guy did, a data analyst will also keep other factors like seasonality, segmentation and visualization in mind. What about if certain trends in shopping behavior are tied to seasonality? What if the trends are different across gender, demographics, geography, product category? A data analyst will slice and dice the data to understand and annotate the report with it. Besides database skills, a data analyst will have a understanding of some of the common visualization tools.\n\nBusiness Analyst: A business analyst possess the skills that the BI and data analysts have plus domain knowledge and understanding of the business. A business analyst may also have some basic skills in forecasting etc.\n\nData Mining or Big Data Engineer: Do what the data analyst did, possibly from unstructured data if needed. MapReduce and other big data skills may be needed. Understands the common issues in running jobs on large scale data and is able to debug the jobs. \n\nStatistician (A traditional one): Pull the data from a DB or obtain it from any of the roles mentioned above and run appropriate statistical tests. Ensure the quality of data and correctness of the conclusions by using standard practices like choosing the right sample size, confidence level, level of significance, type of test etc. \nThe situation has changed a bit recently. Statistics departments at most schools have evolved in way that statisticians graduate with strong programming and decent foundation skills in CS enabling them to do the tasks that statisticians traditionally were not trained in.\n\nProgram/Project Manager: Look at the data provided by the professionals mentioned so far, align business with the findings and influence the leadership to take appropriate action. Possesses communication skills, presentation skills and can influence without authority.\nIronically the person a PM is influencing business decisions using the data and insights provided by others. If the person does not have a knack for understanding data, chances are that the person will not be able to influence others to take the correct decisions. \n\n\nNow putting it altogether.\n\nThe rise of online services has brought a paradigm shift in software development life cycle and how business iterate over successive features and products. Having a different data puller, analyst, statistician and project manager is just now possible any more. The mantra now is ship, experiment and learn, adapt, ship, experiment and learn .... This situation has resulted in the birth of a new role - ' A Data Scientist'\n\nA data scientist should have the skills of all the individuals I have mentioned so far. In addition to the skills mentioned above, a data scientist should have rapid prototyping and programming, machine learning, visualization and hacking skills. \n\nDomain Knowledge and Soft Skills Are As Important as Technical Skills:\nThe importance of domain knowledge and soft skills like communication and influencing without authority are severely under-estimated both by hiring managers and aspiring data scientists. Insights without domain knowledge can potentially mislead the consumers of these insights. Correct insights without the ability to influence the decision making is as bad as having no insights.\n\nAll of what I have said above is based on my own tenure as a data scientist at a major search engine and later the advertising platform within the same company. I learnt that sometimes people asking the question may not know what they want to know - sounds preposterous - yet happens way too often. Very often a bozo will start rat holing into something that is not related to the issue at hand - just to prove that he/she is relevant. A data scientist encounters such HIPPOS (Highly Paid Person's Opinion) that are somewhat unrelated to the problem if not complete nonsense very often. A data scientist should posses the right soft skills to manage situations where people ask irrelevant, distracting or outside of scope questions. This is hard, especially in situations where the person asking the question is several levels up in the corporate ladder and is known to have an ego. It is a data scientist's responsibility to manage up and around while presenting and communicating insights. \n\nBelow is a summary of necessary skills a data scientist should possess in my opinion.\n\nCuriosity About Data and Passion for Domain: If you are not passionate about the domain/business and curious about  data then it is unlikely that you will succeed in a data scientist role. If you are working as a data scientist with an online retailer, you should be hungry all the time to crunch and munch from the Sm\u00f6rg\u00e5sbord (of data of course) to know more. If your curiosity does not keep you awake, no skill in the world can help you succeed. \n\nSoft Skills: Communication and influencing without authority. Understanding of what is the minimum that has the maximum impact. Too many findings are as bad as no findings at all.   Ability to scoop information out of partners and customers, even from the unwilling ones is extremely important. The data you are looking for may not be sitting in one single place. You may have to beg, borrow, steal and do whatever it takes to get the data.\n\nBeing a good story teller is also something that helps. Sometimes the insights obtained from data are counter-intuitive, if you are not a good story teller, it will be difficult to convince your audience.\n\nMath/Theory: Machine Learning. Stats and Probability 101. Optimization would be icing on the cake.\n\nCS/Programming: At least one scripting language (I prefer python). Decent algorithms and DS skills, to be able to write code that can analyze a lot of data efficiently. You may not be a production code developer but should be able write code that does not suck. Database management and SQL skills. Knowledge of a statistical computing package, most people including myself prefer R. A spread sheet software like excel.\n\nBig Data and Distributed Systems: Understanding of basic MapReduce concepts, Hadoop and Hadoop file system and least one language like Hive/Pig. Some companies have their own proprietary implementations of these languages. Knowledge of tools like Mahout and any of the xaaS like Azure and AWS would be helpful. Once again big companies have their own xaaS so you may be working on variants of any of these.\n\nVisualization: Ability to create simple yet elegant and meaningful visualization. In my case, R packages like ggplot, lattice and others have helped me in most cases but there are other packages that you can use. In some cases, you might want to use D3. \n\n\nBelow is a visualization of high level description of skills needed to become a data scientist.\n\n\nWhere is a data scientist in the big data pipeline?\nHere is a nice visualization of the big data pipeline, the associated technologies and the regions of operation. In general the depiction of where the data scientist belongs in this pipeline is largely correct, there is one caveat however. A data scientist should be comfortable to dive into the 'Collect' and 'Store' territories if needed. Usually  data scientists would be working on transformed data and beyond but in scenarios where the business does cannot afford to wait for the transformation process to complete, a data scientist has to turn to raw data to gather insights.\n\n\n\nTo be continued .....\n----------------------------------------------------------------------------------------------------------\nNote: I am publishing this without any edits/reviews. I will update with more thoughts as I get a chance. I am writing myself a note to finish this answer in the next one week. Pardon the typos and scattered thoughts at least for now.\n------------------------------------------------------------------------------------------------------------",
                "I am working as a Data Scientist myself therefore it makes me qualified enough to answer your question.\n\nAlso I will make sure to include the tricks in my answer that worked for me.\n\nSo Let's begin, Shall we?\n\nI will be answering this question, keeping in mind that a bunch of readers could be complete newbies into programming.\n\nSo addressing non-computer science students. Firstly, you need to work a lot on your problem-solving skills which is going to help you code effortlessly. You can achieve this by learning Data structures & Algorithms and coding in it. Also, DS & Algo are the building block of computer science so it will definitely help you on your Journey towards excellence in coding.\n\nAfter you are comfortable with problem-solving, you should stick to the below mentioned points:\n\n1. Opt for a good course on Machine learning and study it thoroughly to become well versed with all it\u2019s concepts.\n2. Practice machine learning problems on Kaggle: Your Machine Learning and Data Science Community [ https://www.kaggle.com/ ] which will help you gain confidence and give you enough hands-on skills.\n3. Post your projects on GitHub, LinkedIn and also you can use youtube to showcase your skills\n4. Now it\u2019s Time to market yourself. Make a clean and creative online portfolio and a strong resume based on ML. Start applying to your desired companies and surely circumstances will bend in your favour and soon you will become something you have worked so hard for and that is \u201cData scientist\u201d\n5. you can connect with me on LinkedIn\nPs: I am attaching my photo, in which you can see me working from home, just in case you are interested to know how a data scientist looks?! \ud83d\ude1b\n\n",
                "Ultimately it depends on the organization, but I would rank them by stress as follows, most to least:\n\n * Data Scientist\n * Data Engineer\n * Software Engineer\nI\u2019ll start with why I see Data Scientist as often the most stressful, then move onto data engineer and software engineer.\n\n\u201cData Scientist\u201d is currently an ill-defined title, meaning there a lot of misconceptions around what the role requires. Also, data science is highly experimental, which runs counter to how organizations like to manage risk and measure/track work. These issues can lead to stress and a high burnout rate among Data Scientists; something discussed at length in several articles (see references at end).\n\nHere are some consequences:\n\n * hiring managers bring in a Data Scientist to work on tasks ill-suited for the role;\n * organizations have unrealistic expectations about what Machine Learning can do for them and their products;\n * there are severe disconnects between what a data scientist produces and what product managers expect;\n * friction occurs at the intersection of experimental work like machine learning and well-defined, rules-based work like software engineering;\n * data scientists end up working in isolation.\nIf you are working on tasks you don\u2019t enjoy every day this will inevitably lead to stress. Countless data scientists in industry complain about being tasked with building reports and writing SQL, with little to no work in machine learning. Nobody becomes a data scientist to write SQL and build reports. This comes from an organization misconstruing data science with business intelligence/analytics.\n\nSometimes an organization does understand the proper role of data scientist. But then we often run into the unrealistic expectations scenario. AI is hyped beyond reason at this point, meaning companies tend to assume a Data Scientist will swoop in and magically convert their data into money-making predictions. The reality is no company has all the right data, and it takes a massive effort in experimentation to convert raw data into something that can live inside a real product and drive value, both of which have no guarantee of happening. There\u2019s little \u201csexy\u201d about this profession.\n\nThen there\u2019s the \u201cPM issue.\u201d Product managers are taught to manage traditional software projects. All their methodologies around ideation, story writing, estimation, acceptance criteria, and measuring/tracking progress grew out of managing rules-based engineering projects. Software engineers do explicitly defined tasks which produce exact outcomes. These outcomes can be tracked, measured, tested and of course discussed easily at \u201cstandup.\u201d Data science isn\u2019t rules-based, it requires messy exploration with unknown outcomes. Unless a PM learns how to manage projects that include data science the experience will be far less than ideal.\n\nBy the time the Data Engineer is ready to implement machine learning into the data pipeline the model has likely been validated at the proof-of-concept level, meaning there is much less uncertainty around whether machine learning can drive value. Data engineering is much more well-defined than Data Science since it is effectively rules-based engineering applied to machine learning. Specifically, data engineers help create the scaffolding that holds machine learning in place. But, I see this role as more stressful than software developer since the rules-based scaffolding must somehow mix deterministic with nondeterministic outputs. For example, while the software developer is tasked with unit testing explicit outputs, the data engineer must test for outputs that change when new data are ingested. This involves the use of thresholds and ranges, along with extensive monitoring of context drift and model performance.\n\nFinally, Software Engineers deal with traditional, rules-based programming. This is the technology that companies think of when we speak of software. There are much fewer unrealistic expectations, program managers are trained to manage this kind of work, and software developers are the most numerous technical position in the company meaning less isolation and the ability to share workloads.\n\nAs stated from the beginning, all this depends on the organization. Some companies have developed an appreciation for how different data science is from traditional, rules-based software development, and have taken steps to merge these disciplines into an effective practice. Data scientists working for these companies are likely much less stressed.\n\nReferences\n\nJason T Widjaja's answer to Is long term burnout as a data scientist common? [ https://www.quora.com/Is-long-term-burnout-as-a-data-scientist-common/answer/Jason-T-Widjaja ]\n\nr/datascience - How to combat \"burnt-out\" feelings in data science? [ https://www.reddit.com/r/datascience/comments/a17na3/how_to_combat_burntout_feelings_in_data_science/ ]\n\nWhy Data Scientists Should be More Careful About Professional Burnout [ https://datavestment.com/why-data-scientists-should-be-more-careful-about-professional-burnout/ ]",
                "This is a long\u2026really\u2026long answer. That will allow me to introduce newcomers, recruiters, seasoned professionals, C-level executives to the Data Science world. You need to persist, always. Read until the end, please.\n\nFirst of all, being a Data Scientist in a small company is TOTALLY DIFFERENT from being a Data Scientist on a multinational/large company. May I repeat? TOTALLY DIFFERENT\u2026We\u2019ll discover why together.\n\nBased on that, check out below my favorite posts about this different perspectives on \u201chow do I feel at work\u201d. It\u2019s clear that the authors described totally different scenarios that can exist between somewhere around a small company and a large corporation.\n\nSamson Hu, Data Scientist working in 500px - Small company - Total of 1 Data Scientist: Full Post [ https://medium.com/@samson_hu/building-analytics-at-500px-92e9a7005c83 ]\n\n * One sole warrior. Sometimes on \u201crogue\u201dor \u201cstand-alone\u201d mode;\n * Need to convince people about importance of data;\n * Do what needs to be done. ETL, (back)front-end, prototype, presentations etc.\nNishikant Dhanuka, Data Scientist working in Booking [ http://Booking.com ] - multinational - Total of 120+ Data Scientists: Full Post [ https://towardsdatascience.com/diary-of-a-data-scientist-at-booking-com-924734c71417 ]\n\n * Work in some narrow and closed scope;\n * Large team with each one specialized in some sort of data niche ;\n * \u201cEvangelized\u201d and aligned company;\n * Ps.: there is no right or wrong, if the environment and role are aligned with your profile.\nOn the other hand, we can still perceive some similarities between both worlds. I remember reading a post on Linkedin along these lines:\n\n\n%3E Ask the interview candidate about how much time he thinks that will be spent on data modeling.\n\nIf he answers more than 20%. Don\u2019t hire.\nThe fun part is that the post is telling us pretty much the truth.\n\nIt\u2019s true that some outliers are working on highly-specialized AI positions at IBM/Facebook/Google research departments, but in the end, for the mortals, its an accurate interview scenario.\n\nData Scientist main characteristics\n\nWhen I say main characteristics I need to be clear that we have an assumption that a Data Scientist is already good with math, statistics and computing plus a cherry on the cake, at least one year of experience in some market niche (domain knowledge).\n\nData Scientists ARE NOT plug and play. You can just unplug a Data Scientist from a cancer research program and plug into a stock market consulting role.\n\nSo with all said. That's the basics, that you see everywhere. No need do discuss or reinvent the wheel.\n\nData Scientists need to convince people of the importance, usefulness and accuracy of their work. Most of posts don\u2019t mention that. Let's think in hypothetical scenarios to illustrate the persuasion need:\n\n1. You show the output of a black-box neural network with maximum precision and recall. Apparently blindly ignoring your numbers, investors don't believe in the results and start an endless Q&A session. They say that they will invest millions of dollars on the idea only after a clear understanding of assumptions and a successful experiment\u2026and so on.\n2. Try to explain what is a moving average to a salesperson? They are supposed to know the difference between median, mean an mode, right? Or some basic thing like quartiles and a tricky question about bagplots. Probably by now, you noticed that there is a inherent evangelism role for every data science position. Tools are only good if people are using them. You can provide a set of outputs, predictions and reports, but they are useless if people don't understand or use. Adoption, adoptio\u2026adoption!\n3. You schedule a Friday meeting to transfer knowledge from Data Science to other departments like sales, marketing, maintenance and accounting. You have a naive goal to explain and teach what is a Fast Fourier transform (FFT), Taylor series, Laplace transforms. Near the end of the meeting, when you realize, people will be dead or in a coma. They were killed by boringness. Data is a bunch of fun only for data guys, remember it. A more successful approach would be to teach integrals by referring to them as the area below the curve. Geometry instead of algebric, Socrates! Visual communication instead of just words or equations.\nSo let\u2019s face it. We (analytical people) speak a different language and it needs to be translated into the world\u2019s lingua franca. Otherwise, no one will listen, or even worse, they will pretend to listen and understand.\n\nBy now you are supposed to be convinced that one of the most important characteristics of a Data Scientist is \u201cevangelism\u201d, \u201csell your fish\u201d. Not everyone can abandon \u201cgut feelings\u201d and be \u201cdata-driven\u201d by themselves.\n\nThere is another characteristics related do evangelism present in most of the well-know and seasoned Data Scientists. They can communicate very complex concepts in a way that people will understand (or maybe get a grasp of the actual meaning).\n\nOld versus new\n\nYou can be lucky to start working in a new company with all that fresh mentality of Business Intelligence, KPIs, predictive and prescriptive analytics, real-time dashboards, data access to everyone and the AI term being used every where (when actually is just machine learning. I forgive you, marketing department!).\n\nOr\u2026\nYou can start to work for a +100 years law company where your main job will be to convince others that your work is valuable and it\u2019s not just another \u201chype/trend\u201d being followed by a managerial decision.\n\nThe dream of every company, the central data repository of truth and a data-driven mindset can only be achieved with everyone embracing this long-term mission. A Data Scientist, can be the facilitator, a virus (in a good sense), but the virus needs to proliferate (create allies) in order to facilitate \u201cfever\u201d to shake down things.\n\nDomain Expertise/Knowledge\n\nThere was a competition for improving the Paypal fraud detection. In the middle of all the Kaggle obsession with algorithms, the company responsible for developing the winning solution was very clear. They didn\u2019t use any fancy algorithm or a deep neural network with thousands of hidden layers. They only used their past acquired experience of 10 years working with fraudulent transactions.\n\nIt\u2019s really important to understand this concept. People think in Data Scientists as the \u201cjoker\u201dcard, a wildcard character. It's true that some abilities that rely on programming, logic thinking and math/stats are portable to different industries. However, the domain expertise is highly related to the industry that the Data Scientist has more familiarity and years of work.\n\n\n%3E This is not supposed to be a new concept, right? It\u2019s been that way for all the history of humankind, for all professionals.\nThe problem is that some people took the \u201cgeneralization\u201d of certain intelligent models too seriously, as granted. Then, they applied to everything and everyone without restrictions.\n\nA Data Scientist with 5 years experience with banking will be totally different from one that spent one decade researching cancer. Different technology stacks (R or Python) and/or different approaches for similar problems. They can be interchangeable, but the learning curve is significant and painful. This needs to be take into account (special attention for HR recruiters reading this paragraph).\n\nData Science is all about controversy\n\nI don\u2019t personally like these two answers below, but I think that it\u2019s good to read them to hear different opinions about Data Scientists.\n\n * Michael Hochster [ https://www.quora.com/profile/Michael-Hochster ]'s anwser about Data Scientists, basically defines two types. Type A (Analysis) and Type B (Build) Data Scientists.\nI disagree with that statement. When I say disagree, I mean REALLY disagree. The main reason it\u2019s that the definition is too simplistic. The data market will not evolve with terms like type B or type A.\n\n\n%3E \u201cHey, I\u2019ve just landed a job position as Data Scientist Type B at the TypeC Company\u201d.\nThis is well aligned to the trend of IT industry of calling people by uninspired terms liks \u201carchitect\u201d or \u201cengineer\u201d. You can be a \u201csoftware engineer or a database architect\u201d, but if you have a computer science bachelors you are a \u201ccomputer scientist\u201d, not the single-word term \u201cengineer\u201d. I'm a mechanical engineer, thus an engineer. Same thing for architects. They are\u2026architects, simple as stated, architects.\n\nYes, it\u2019s good to discuss about these picky things for us to not lose control.\n\nAnother idea that I disagree but its good to understand is the conception from Ingo Mierswa [ https://rapidminer.com/author/ingo/ ]: Founder & President of RapidMiner.\n\n\n%3E Data scientists are people who apply all those analytical techniques and the necessary data preparation in the context of a business application.\nLet\u2019s go back to the case of an independent Data Scientist researcher fighting big corporations, mining data and trying to extract patterns that will help to cure cancer. I certainly think that it's not on the context of \u201cbusiness applications\u201d.\n\nDoes a Data Scientist develop products or product features?\n\nAnother great missconception. I really liked my summer internship at Cognitive Class  [ https://cognitiveclass.ai/ ](IBM Canada), because we were not developing products. We were developing courses and exploring algorithms and data.\n\nYes, you can improve the gamified leaderboard of an app. No problem with that, but we can\u2019t confine Data Scientists to work only with products.\n\nAs of today, I founded a consultancy company that have Data Scientist working with consulting and products.\n\nThe result of a Data Scientist analysis can affect a business decision, can motivate someone to change his/her career (like some of my posts and based on the private feedback that I'm receiving), endorse a hiring budget and many other possibilities. It doesn\u2019t need to be a product, it can be a service to external and/or internal clients!\n\nDoes A Data Scientist works near business, money flow, \u201cplata\u201d?\n\n\n%3E \u201cIt's All About the Money, Honey! \u201d\nOkay, now you want to talk about business.\n\nData Scientists working on Bussiness Intelligence, Strategy Consulting, banking and finance can have high-paying careers. The downside is that we can\u2019t take only these profiles into account to define a whole group of professionals.\n\nLet\u2019s get some data if we're talking about money:\n\nRef.: Oreilly 2016 data science salary survey [ http://www.oreilly.com/data/free/files/2016-data-science-salary-survey.pdf ]\n\nSometimes I can get astonishined by seing how the Central limit theorem  [ https://en.wikipedia.org/wiki/Central_limit_theorem ]really works for when you have a large  [ https://stats.stackexchange.com/questions/269/what-is-the-difference-between-a-population-and-a-sample ]sample of your population.\nThe survey looks approximately as a \u201cBell Curve\u201d (aka Normal distribution [ https://en.wikipedia.org/wiki/Normal_distribution ]) with the mean around 100k ~120k US Dollars.\n\nA lot of money!\n\nWe are getting closer on this post to find out why some companies are willing to pay that much for Data Scientists (if you are not convinced yet, remember, this is just the AVERAGE)\n\nDefinitions\n\nit\u2019s not only one, you are dealing with Data Science [ https://www.quora.com/What-is-data-science/answer/Luis-Martins-200 ]. Your eyes need to bleed for things start making sense.\n\nAfter this long post, I\u2019ll share my proposal that is more aligned with what I\u2019ve seen around my feed and what, at the same time, makes more sense (at least for me):\n\nData Engineer\n\n * Extract, transform and loads data from all types of sources\n * Maintain pipelines and databases (Some overlap with a DBA [ https://en.wikipedia.org/wiki/Database_administrator ])\n * Interface between back-end/database team and Data Scientists team\nMachine Learning Practitioner/Specialist/Engineer\n\n * Data Modeling. If the company is large enough, works only developing models;\n * Create containers to make the models \u201cproduction ready\u201d;\n * Test, Learn and deploy new algorithms (\n * \n * There is a lot of algorithms to learn, believe me, just Artificial Neural Networks would take all the 71.5 years of average life expectancy for a human being to be properly learned;\n\n * Optimization\nData Analyst/Business Analyst\n\n * Can be related to business, but not necessarily;\n * Can consume data from the models generated by the Machine Learning Practitioner or straight from data pipelines developed by Data Engineers;\n * Share insights with plain english.\nMathematicians and statisticians\n\n * Do everything that a Machine Learning Practitioner does but without all the glamour and hype;\n * Propose experiments to prove or reprove hypothesis (e.g. A/B Testing).\nData Scientist (The unicorn, The Asymptote [ https://en.wikipedia.org/wiki/Asymptote ], The GOAL!)\n\n * + 10 years of experience\n * Needs to have experience and proven knowledge in all the roles above and be capable of temporarily acting as one of the described positions, if in need. Doesn't mean that should;\n * The glue that put\u2019s everything together;\n * Direct report to upper management level (if in a business context);\n * Domain expertise focus\nOkay, One last chance for those of you that missed the Integral and differential Calculus [ https://en.wikipedia.org/wiki/Calculus ] class.\n\nI like to think in the following way to don\u2019t get too depressed while facing what a Data Scientist could know. To stay with sanity, I focus on what I should know and work to get better on that.\n\nRef.: my Wacom\n\nAccording to my own theory (US Patent pending), the Data Scientist Goal is an Red Dashed Asymptote, something that these blue and green lines will never achieve or get to know what is like.\n\n\u2026 Blue curve is a data professional working and studying 14hrs/day\n\u2026 Green curve is a data professional working and studying 8hrs/day\n\nThe only difference is the rate that they learn and evolve. The sad part is that both will not reach the Data Scientist Goal.\n\nThis approach is similar to:\n\n\n%3E \u201cShoot for the moon. Even if you miss, you'll land among the stars.\u201d - Norman Vincent Peale, author of The Power of Positive Thinking.\nAfter 1x Human life we start to see the prediction of the blue line. To give the blue one a chance, let\u2019s suppose that this poor data professional is actually immortal!\n\nThen, surprisingly, we'll have the dashed blue lines (projection) that still won\u2019t touch the Data Scientist Goal due to the mathematical nature of asymptotes. Theoretically, in the end of its immortality, this blue data professional would be a complete and up to date Data Scientist unicorn .\n\nI hope that this post familiarize you with lots of technical terms that you will face along the way if you are pursuing a data science career, planning to hire someone or invest in Data Science.\n\nPs.: asymptotes are not supposed to be new for you if you are planning to pursue an analytical career.\n\nConclusion\n\nAfter all, that\u2019s why I have the Jr. Data Scientist title on my Linkedin [ https://www.linkedin.com/in/luisotsm/ ] profile. It\u2019s something that reminds constantly my ego that we don\u2019t know anything, we don\u2019t have a clue of what is about to happen next and the only certainty is that we need to learn more. [Footnote: Update: 2018\u201303\u201326]\n\n\u201cEssentially, all models are wrong, but some are useful\u201d - George Box\n\n\nYou can know more about me on my personal site. Follow me if you would like to receive notifications about answers related to Data Science, Machine Learning, Artificial Intelligence and Engineering.\n\nAlways upvote answers that you find useful. Everyone can be wrong so be respectful and polite.\n\n\n[Update: 2018\u201303\u201326, I had to remove \u201cJr.\u201d from my profiles due to the increasing number of \u201cJr.\u201d job offers. People just don\u2019t get it. I conceptually still think in \u201cData Scientist\u201d as described on this answer]",
                "The data science field is booming, and many fresh graduates are eager to join the ranks of data scientists. But is it possible to land a job as a data scientist straight out of college? The short answer is yes. However, it requires dedication, skill development, and persistence. In this article, we'll explore the data science field, opportunities for fresh graduates, and tips for securing your first data science job.\n\nUnderstanding the Data Science Field\n\nWhat is Data Science?\n\nData science is a multidisciplinary field that combines statistical analysis, computer science, and domain expertise to extract valuable insights from large volumes of data. It helps organizations make data-driven decisions, improve processes, and uncover hidden patterns that can lead to innovation and growth.\n\nImportance of Data Science\n\nAs data becomes increasingly crucial in today's digital world, the demand for skilled data scientists has skyrocketed. Data science has applications across various industries, including finance, healthcare, retail, and manufacturing, making it a highly sought-after profession.\n\nSkills Required for a Data Scientist\n\nSome essential skills for a data scientist include:\n\n * Programming languages (Python, R, or Java)\n * Statistical analysis\n * Machine learning algorithms\n * Data visualization\n * Data wrangling and cleaning\n * Domain knowledge\nOpportunities for Fresh Graduates in Data Science\n\nEntry-Level Positions\n\nSeveral entry-level positions are available for fresh graduates in the data science field, such as data analysts, junior data scientists, and business intelligence analysts. These roles provide an excellent opportunity to gain experience and build your skills in a professional setting.\n\nInternships\n\nInternships are a great way to gain hands-on experience and build your network in the data science community. Many companies offer paid internships, which can eventually lead to full-time job offers.\n\nFreelance Projects\n\nFreelance projects provide an opportunity to work on diverse problems and build a portfolio of work that demonstrates your skills. Platforms like Upwork and Freelancer are excellent resources to find freelance data science projects.\n\nBuilding Your Data Science Skillset as a Fresh Graduate\n\nAcademic Background\n\nHaving a degree in a relevant field, such as computer science, statistics, or mathematics, can help you build a strong foundation for a career in data science. However, even if your degree is unrelated, you can still learn the necessary skills through online courses and certifications.\n\nOnline Courses and Certifications\n\nSeveral online platforms offer data science courses and certifications, such as Coursersera, edX, and Udacity. These courses cover various topics, including programming, machine learning, and data visualization, helping you build a strong foundation in data science.\n\nCoding Competitions and Hackathons\n\nParticipating in coding competitions and hackathons is an excellent way to improve your skills, work on real-world problems, and showcase your abilities. Platforms like Kaggle and HackerRank host regular competitions that can help you gain exposure and even win prizes.\n\nNetworking\n\nConnecting with professionals in the data science field can open doors to new opportunities. Attend industry events, webinars, and join online communities to build your network and learn from others.\n\nBuilding a Portfolio\n\nA strong portfolio can set you apart from other candidates and showcase your skills to potential employers. Include personal projects, freelance work, and academic research to demonstrate your capabilities as a data scientist.\n\nTips for Landing Your First Data Science Job\n\n1. Tailor your resume to each job application, highlighting relevant skills and experiences.\n2. Prepare for technical interviews by practicing data science problems and coding exercises.\n3. Network with professionals in the industry to learn about job openings and gain referrals.\n4. Leverage LinkedIn to showcase your skills, connect with recruiters, and apply for jobs.\n5. Continuously learn and stay updated with the latest trends and technologies in data science.\nConclusion\n\nGetting a job as a data scientist as a fresh graduate is possible with the right skillset, experience, and persistence. By developing your skills, building a strong portfolio, and leveraging networking opportunities, you can increase your chances of landing your dream data science job.",
                "Are you serious?\n\nHere are a few reasons I love my job.\n\n1. Make more than most doctors.\n2. Work remotely 100% of the time.\n3. Work with really smart people.\n4. Not a lot of human interaction. Very few meetings.\n5. A total package like C level execs. (Bonus, sick leave, paid time off)\n6. For the most part, I make my own hours. I\u2019m not a morning person so I block off my calendar so other\u2019s can\u2019t set up meetings at 8am. I\u2019m still sleeping then and don\u2019t want to be interrupted.\n7. Tons of jobs. If I don\u2019t like my job for any reason, I move on.\n8. A career for life. At 50, I\u2019ll never not have job in this space. Ever.\n9. Highly respected. This career is one of the most technical and respected in all of IT.\nThose are a few reasons that make machine learning a great career.\n\nNow, don\u2019t kid yourself. I\u2019ve been working with data and BI for 20 years. You won\u2019t be in a MLE role in a few months and not likely in a few years but I wouldn\u2019t change a thing.\n\nReady to learn real-world ML? Here you go.\n\nhttps://logikbot.quora.com/\n",
                "I am working as a Data Scientist myself therefore it makes me qualified enough to answer your question.\n\nAlso I will make sure to include the tricks in my answer that worked for me.\n\nSo Let's begin, Shall we?\n\nI will be answering this question, keeping in mind that a bunch of readers could be complete newbies into programming.\n\nSo addressing non-computer science students. Firstly, you need to work a lot on your problem-solving skills which is going to help you code effortlessly. You can achieve this by learning Data structures & Algorithms and coding in it. Also, DS & Algo are the building block of computer science so it will definitely help you on your Journey towards excellence in coding.\n\nAfter you are comfortable with problem-solving, you should stick to the below mentioned points:\n\n1. Opt for a good course on Machine learning and study it thoroughly to become well versed with all it\u2019s concepts.\n2. Practice machine learning problems on Kaggle: Your Machine Learning and Data Science Community [ https://www.kaggle.com/ ] which will help you gain confidence and give you enough hands-on skills.\n3. Post your projects on GitHub, LinkedIn and also you can use youtube to showcase your skills\n4. Now it\u2019s Time to market yourself. Make a clean and creative online portfolio and a strong resume based on ML. Start applying to your desired companies and surely circumstances will bend in your favour and soon you will become something you have worked so hard for and that is \u201cData scientist\u201d\n5. you can connect with me on LinkedIn\nPs: I am attaching my photo, in which you can see me working from home, just in case you are interested to know how a data scientist looks?! \ud83d\ude1b\n\n",
                "Ultimately it depends on the organization, but I would rank them by stress as follows, most to least:\n\n * Data Scientist\n * Data Engineer\n * Software Engineer\nI\u2019ll start with why I see Data Scientist as often the most stressful, then move onto data engineer and software engineer.\n\n\u201cData Scientist\u201d is currently an ill-defined title, meaning there a lot of misconceptions around what the role requires. Also, data science is highly experimental, which runs counter to how organizations like to manage risk and measure/track work. These issues can lead to stress and a high burnout rate among Data Scientists; something discussed at length in several articles (see references at end).\n\nHere are some consequences:\n\n * hiring managers bring in a Data Scientist to work on tasks ill-suited for the role;\n * organizations have unrealistic expectations about what Machine Learning can do for them and their products;\n * there are severe disconnects between what a data scientist produces and what product managers expect;\n * friction occurs at the intersection of experimental work like machine learning and well-defined, rules-based work like software engineering;\n * data scientists end up working in isolation.\nIf you are working on tasks you don\u2019t enjoy every day this will inevitably lead to stress. Countless data scientists in industry complain about being tasked with building reports and writing SQL, with little to no work in machine learning. Nobody becomes a data scientist to write SQL and build reports. This comes from an organization misconstruing data science with business intelligence/analytics.\n\nSometimes an organization does understand the proper role of data scientist. But then we often run into the unrealistic expectations scenario. AI is hyped beyond reason at this point, meaning companies tend to assume a Data Scientist will swoop in and magically convert their data into money-making predictions. The reality is no company has all the right data, and it takes a massive effort in experimentation to convert raw data into something that can live inside a real product and drive value, both of which have no guarantee of happening. There\u2019s little \u201csexy\u201d about this profession.\n\nThen there\u2019s the \u201cPM issue.\u201d Product managers are taught to manage traditional software projects. All their methodologies around ideation, story writing, estimation, acceptance criteria, and measuring/tracking progress grew out of managing rules-based engineering projects. Software engineers do explicitly defined tasks which produce exact outcomes. These outcomes can be tracked, measured, tested and of course discussed easily at \u201cstandup.\u201d Data science isn\u2019t rules-based, it requires messy exploration with unknown outcomes. Unless a PM learns how to manage projects that include data science the experience will be far less than ideal.\n\nBy the time the Data Engineer is ready to implement machine learning into the data pipeline the model has likely been validated at the proof-of-concept level, meaning there is much less uncertainty around whether machine learning can drive value. Data engineering is much more well-defined than Data Science since it is effectively rules-based engineering applied to machine learning. Specifically, data engineers help create the scaffolding that holds machine learning in place. But, I see this role as more stressful than software developer since the rules-based scaffolding must somehow mix deterministic with nondeterministic outputs. For example, while the software developer is tasked with unit testing explicit outputs, the data engineer must test for outputs that change when new data are ingested. This involves the use of thresholds and ranges, along with extensive monitoring of context drift and model performance.\n\nFinally, Software Engineers deal with traditional, rules-based programming. This is the technology that companies think of when we speak of software. There are much fewer unrealistic expectations, program managers are trained to manage this kind of work, and software developers are the most numerous technical position in the company meaning less isolation and the ability to share workloads.\n\nAs stated from the beginning, all this depends on the organization. Some companies have developed an appreciation for how different data science is from traditional, rules-based software development, and have taken steps to merge these disciplines into an effective practice. Data scientists working for these companies are likely much less stressed.\n\nReferences\n\nJason T Widjaja's answer to Is long term burnout as a data scientist common? [ https://www.quora.com/Is-long-term-burnout-as-a-data-scientist-common/answer/Jason-T-Widjaja ]\n\nr/datascience - How to combat \"burnt-out\" feelings in data science? [ https://www.reddit.com/r/datascience/comments/a17na3/how_to_combat_burntout_feelings_in_data_science/ ]\n\nWhy Data Scientists Should be More Careful About Professional Burnout [ https://datavestment.com/why-data-scientists-should-be-more-careful-about-professional-burnout/ ]",
                "My first inclination was to disclaim, \"I'm not top 1%, maybe top N%\" where N is some slightly higher integer, because false precision makes great humble-brags. In reality, I have a hard time identifying \"top 1%\" (where that line is, which side of it I would be on) because it depends on who's counted in the population as \"software engineers\" and who isn't. Do we count the data scientists using R? The business analysts writing Excel? Still, I think I understand the approximate level you're aiming for, and so I'm going to try to answer your question. You want to know how to become an elite engineer (2.0 on a scale I will soon introduce). \n\nFor further reading, I wrote a couple essays on it:\n\nThe trajectory of a software engineer... and where it all goes wrong. [ http://michaelochurch.wordpress.com/2012/01/26/the-trajectory-of-a-software-engineer-and-where-it-all-goes-wrong/ ]\nThe shodan programmer [ http://michaelochurch.wordpress.com/2013/04/22/gervais-macleod-23-the-shodan-programmer/ ]\n\nI'd focus on the second, and if you don't have time to read either, I'll get into the \"Cliff's Notes\" version of the theory (and the 0.0-to-3.0 scale of programmer competence) here. First, programmers tend to separate into three very broad (and overlapping) categories:\n\n * Adders (\"level 1\", 0.0 to 1.4) who can solve most programming problems, given enough time. These tend to be the ones who do line-of-business work, write scripts to fetch data and answer questions that executives or traders care about, etc. The good ones want to progress beyond this level, remove themselves from parochial details and work on more general \"core\" problems. \n * Multipliers (\"level 2\"; 1.5 to 2.4) who can select technologies, make architectural decisions, and write or maintain useful libraries. They tend to have open-source contributions to their name, speak at conferences, and are often executive-equivalent in importance to their companies. \n * Global Multipliers (\"level 3\"; 2.5+) who build communities, create platforms and come up with concepts like Google's MapReduce. Their ambitions tend to out-scope most companies so they're often in academia, consulting independently, or highly paid in large firms' R&D labs.\nThese three abstraction levels don't tell the whole story, and of course some \"adder\" tasks are very hard (like building a production trading system) and could not be done by most \"level 1\" programmers. Then there's the altogether different issue of unqualified people being given \"level 2\" (or 3) work, such as architecture of a complex software system. Perhaps a hard L1 task is really 1.3 or 1.5 or even higher. This is just the theory.\n\nTo assess programmer level and give more of a continuum (from 0.0 to 3.0) we add a decimal point and assume that, e.g., a 2.0 programmer will succeed at 95% of level-2 tasks (and likewise for L1 and L3). At 1.5, it's 50%. At 1.0, it's 5% (relative to L2) and 95% of L1 tasks. In building the model, I assumed a logistic S-curve-- so a 1.6 programmer will succeed at 64% of L2 tasks; 1.7, 76%; 1.8, 85%; 1.9, 91%-- but I don't think it can be taken that precisely. \n\nAt 0.7, you're able to do most straight coding tasks (although it may take a long time, and you're still not sure what this \"compiler\" thing is really doing). 1.0 is the level at which people are typically considered \"software engineers\" and 1.2 is the probably the median employed engineer, but at top tech companies it's closer to 1.6. A typical SWE 3 at Google is 1.6-1.8 but assigned 1.2-1.4 level work. A typical Sr. SWE at Google is 1.8-2.0 but assigned 1.4-1.7 level work. You might sense a pattern. The bad news is that you'll often be under-leveled at that sort of place. The good news is that the quality of most of the code at Google is quite high. On the other hand, there are plenty of companies (usually non-technical ones that still rely on software) out there that don't have a single 1.5+ engineer (or, at least, not one that they listen to) and they still need architects... so you often see those L2 decisions made by people at the 1.2-1.4 level (or by non-technical managers, which is worse) and the results are what you'd expect. \n\nAll of this might seem like a lot of verbiage, but it's useful in answering the question. The definition of \"top 1%\" depends on whom you include in the \"programmer\" population; I'm looking for something more objective. I'd guess that there are about 200,000 programmers alive at or above the 2.0 level. That's the level I'm going to focus on. (2.5, 3.0, I can't even answer.) So how do you get to the 2.0 level?\n\n * Budget 7 to 14 years. Yes, it's going to take time. (I've been programming, sadly not continuously, for 8 years; I'd guess that I'm 1.9-2.0 and it took work.) You have to write code, read code, experiment with new technologies, see a few architectural failures and (one hopes, but let's not get ahead of ourselves because most \"software architects\" are politically empowered shitheads) see an architectural success or two. I think you can probably get up to 0.7-0.8 (basic code fluency) in a dedicated summer slinging Javascript, and 1.0-1.1 in another year if you really dive in. Beyond that, 0.1 point per year is a pretty aggressive clip. Most employers will assign you work below your frontier of ability, so you may have to steal an education from your boss (reading papers on work time, experimenting with new technologies). Remember: it's always better to ask forgiveness than permission. \n * Study voraciously. Computer science is a deep field, and to be good at it, you need at least a working understanding of everything. If you think assembly code or linear algebra or strong static typing is \"scary\" or \"too deep\" you'll never reach 2.0. That doesn't mean you need to be an expert in everything because you can't. You can't have \"I won't go there\" areas. You probably don't want to be hand-writing assembly code very often, but if you take the attitude that it's \"black magic\" or \"grunt work\" or \"unclean\", then it will hinder your learning. (You see some Java programmers take this attitude toward manual-memory languages like C.) I would probably use Haskell (a high-level language with strong static typing) given a clean slate; but I've exposed myself to Clojure, C, and even Python (data science libraries) because they cover important topics from computer science. You can't have the \"Will this be on the test?\" mentality. You have to be curious about everything CS-related. (Mathematical curiosity helps.) You also need to learn about the industry itself. Why do so many software projects fail? What mistakes (technical and nontechnical) lead to that and how might they be prevented? What makes a good startup CTO? What things are worth building, and what patterns betray a death march? \n * Build things when you don't know that you'll succeed. How do you become a competent programmer? Or architect? Or Linux kernel hacker? Practice. If you know that you can do something, then you won't learn as much as if there's some chance of failure. Employers want you at a difficulty level where you'll succeed 95% of the time. Your best learning is in the 65-75% \"stretch\" range, I'd say. That means that you're eventually getting successful outcomes, but faltering a bit. There's progress, but it's not smooth and you definitely have to work. \n * Network, not for jobs but for ideas. Don't think of \"networking\" as something you do after you get laid off. We're social creatures and programmers are no different. The best way to become a great programmer is to go talk to other great programmers, and then get a sense of what problems they've solved and how. I'd be cautious at AI/machine learning talks in 2014 (some are very good, but there plenty of poseurs in \"data science\"; that will probably change in 3 years) but Haskell and Clojure user groups both draw a great crowd. \n * Job hop when you stop learning. Most programmers plateau around 1.2. Why? Because they aren't pushed, and they don't get opportunities to work on hard things. Or, the \"hard\" stuff they get pertains to difficult/ugly legacy code, not computer science itself. It's useful to spend ~3 months on a bad legacy module or two to learn about architectural mistakes, but not more. Ultimately, you'll want to work on successful projects, and you'll want to be surrounded by strong people. If you ever stop growing, it's time to move on. You know how people end up stagnant? One day at a time. That's also how you become a great programmer. One day at a time (for about 4,000 days).\n",
                "Imagine you are a product, and you want to sell yourself to your manager for the highest possible salary. You\u2019d want to\n\n1. Know your customers\n2. Know what you\u2019re selling\nWhat are you selling?\n\nJohn solves LeetCode questions in 10 minutes, and codes fluently in C++, Python, and HTML. He joined Google as a new grad, and was immediately assigned to refactor legacy code.\n\nHe wondered why he was getting boring work after his straight-A performance in school, and toyed with the idea of going back to school for a PhD.\n\nTo John\u2019s manager, John was a coder. That refactoring project needed a coder, and John fit the bill.\n\n\nManager Mike had a few more projects that needed doing.\n\nHe needed to replace the decision engine with a machine learning model. Obviously he needed a machine learning expert for that.\n\nHe needed to fix the emerging crisis that his VP was pinging him about. For this, he needed his fastest, most reliable engineer.\n\nAnd he needed a tech lead. For that, he needed someone with excellent technical judgment, great stakeholder relationships, and an eye towards growing junior colleagues.\n\nJohn didn\u2019t get these tasks because he sold himself as a coder. And because John was a coder, Mike gave him the tasks that required no other skills.\n\nTop 1% engineers know what value they bring to the table, and sell it to customers in sore need of that skillset.\n\n\nWho are your customers?\n\nDan joined the team, and was none of these. But he was less interested in who he was, and more interested in Mike.\n\nMike was constantly stressed. He worked nights and weekends, and he was most stressed at the end of each quarter, when he had to report on how close the team was to its metric goals.\n\nDan was also given refactoring work, which he completed quickly. But he knew that Mike really needed to hit those goals every quarter.\n\nHe came up with an idea, and ran a bunch of SQL queries to show it would bump the team above its quarterly goals. Mike was intrigued, and asked his team to work on it.\n\nThe wins were delivered as promised, and Mike breathed a sigh of relief.\n\nDan\u2019s wallet got so heavy he needed a new pair of pants.\n\n\nThe best engineers feel when people around them are struggling, and use their skills to sell them a solution. Dan could have sold himself as a SQL expert. Instead, he was the guy Mike could rely on to make sure the metrics went up.\n\nEngineering, like every other job, is a customer service role.\n\n",
                "The author of this content is: Mr. Praveen Kumar.\n\nA few years ago, when I was in college, I attended a workshop organized by FOSSEE [ http://python.fossee.in/ ] and it was in this workshop that I met \"Python\" and fell in love with her at first sight. I was amazed at how easy it was to write simple solutions to complex problems in Python. One of the utility I learnt at the workshop was an image to ASCII art generator.\n\nHow it works?\n\nWe scale a given image to a standard resolution that suitably represents the ASCII version of a given image. The scaled version is then converted to a grayscale image. In a grayscale image, there are 256 shades of gray, or in other words, each pixel carries only the intensity information which is represented by an 8 bit value. A pixel with a value of 0 is assumed to be black and the one with 255 is assumed to be white. We divide the whole range of 0-255 into 11 smaller ranges of 25 pixels each and then assign each pixel a character according to the range it falls in. The point is to assign a group of pixels with slightly varying intensity the same ASCII char. We use the PIL library to play with the images. The code given below is almost self explanatory. The default char mapping and resolution doesn't render good ASCII arts for every image size and so you should try modifying the char mapping and image size to the one that best represents the given image.\n\nHere are some ASCII arts:\n\nDependencies:\n\nPIL(Python Imaging Library)\n\nUnder Ubuntu\n\n[code]$ sudo pip install Pillow\n[/code]Code:\n\n[code]ASCII_CHARS = [ '#', '?', '%', '.', 'S', '+', '.', '*', ':', ',', '@']\n\ndef scale_image(image, new_width=100):\n    \"\"\"Resizes an image preserving the aspect ratio.\n    \"\"\"\n    (original_width, original_height) = image.size\n    aspect_ratio = original_height/float(original_width)\n    new_height = int(aspect_ratio * new_width)\n\n    new_image = image.resize((new_width, new_height))\n    return new_image\n\ndef convert_to_grayscale(image):\n    return image.convert('L')\n\ndef map_pixels_to_ascii_chars(image, range_width=25):\n    \"\"\"Maps each pixel to an ascii char based on the range\n    in which it lies.\n\n    0-255 is divided into 11 ranges of 25 pixels each.\n    \"\"\"\n\n    pixels_in_image = list(image.getdata())\n    pixels_to_chars = [ASCII_CHARS[pixel_value/range_width] for pixel_value in\n            pixels_in_image]\n\n    return \"\".join(pixels_to_chars)\n\ndef convert_image_to_ascii(image, new_width=100):\n    image = scale_image(image)\n    image = convert_to_grayscale(image)\n\n    pixels_to_chars = map_pixels_to_ascii_chars(image)\n    len_pixels_to_chars = len(pixels_to_chars)\n\n    image_ascii = [pixels_to_chars[index: index + new_width] for index in\n            xrange(0, len_pixels_to_chars, new_width)]\n\n    return \"\\n\".join(image_ascii)\n\ndef handle_image_conversion(image_filepath):\n    image = None\n    try:\n        image = Image.open(image_filepath)\n    except Exception, e:\n        print \"Unable to open image file {image_filepath}.\".format(image_filepath=image_filepath)\n        print e\n        return\n\n    image_ascii = convert_image_to_ascii(image)\n    print image_ascii\n\nif __name__=='__main__':\n    import sys\n\n    image_file_path = sys.argv[1]\n    handle_image_conversion(image_file_path)Source : https://www.hackerearth.com/notes/beautiful-python-a-simple-ascii-art-generator-from-images/\n[/code]Source: Beautiful Python: A Simple ASCII Art Generator from Images - Praveen Kumar [ https://www.hackerearth.com/notes/beautiful-python-a-simple-ascii-art-generator-from-images/ ]\n\nThank you\u2026. :)",
                "I read a lot of Cool tricks here,I wonder if anyone mentioned this, anyways! I am going to share You this Little trick which i find very pythonic!\n\nUnderstanding the underscore( _ ) of Python!\n\nThere are 5 cases for using the underscore in Python.\n\n1. For storing the value of last expression in interpreter.\n2. For ignoring the specific values. (so-called \u201cI don\u2019t care\u201d)\n3. To give special meanings and functions to name of vartiables or functions.\n4. To use as \u2018Internationalization(i18n)\u2019 or \u2018Localization(l10n)\u2019 functions.\n5. To separate the digits of number literal value.\nLet\u2019s look at each case.\n\nWhen used in interpreter\n\nThe python interpreter stores the last expression value to the special variable called \u2018_\u2019. This feature has been used in standard CPython interpreter first and you could use it in other Python interpreters too.\n\nFor Ignoring the values\n\nThe underscore is also used for ignoring the specific values. If you don\u2019t need the specific values or the values are not used, just assign the values to underscore.\n\nGive special meanings to name of variables and functions\n\nThe underscore may be most used in \u2018naming\u2019. The PEP8 which is Python convention guideline introduces the following 4 naming cases.\n\n_single_leading_underscore\nThis convention is used for declaring private variables, functions, methods and classes in a module. Anything with this convention are ignored in [code ]from module import *[/code]. \nHowever, of course, Python does not supports truly private, so we can not force somethings private ones and also can call it directly from other modules. So sometimes we say it \u201cweak internal use indicator\u201d.\n\nsingle_trailing_underscore_\nThis convention could be used for avoiding conflict with Python keywords or built-ins. You might not use it often.\n\n__double_leading_underscore\nThis is about syntax rather than a convention. double underscore will mangle the attribute names of a class to avoid conflicts of attribute names between classes. (so-called \u201cmangling\u201d that means that the compiler or interpreter modify the variables or function names with some rules, not use as it is) \nThe mangling rule of Python is adding the \u201c_ClassName\u201d to front of attribute names are declared with double underscore. \nThat is, if you write method named \u201c__method\u201d in a class, the name will be mangled in \u201c_ClassName__method\u201d form.\n\nAnd the best One is this:\n\nTo separate the digits of number literal value\n\nThis feature was added in Python 3.6. It is used for separating digits of numbers using underscore for readability.\n\nEdit 1\u2192 Extending Unpacking ,*_ is available in Python 3 Only!\n\nsrc\u2192Understanding the underscore( _ ) of Python \u2013 Hacker Noon [ https://hackernoon.com/understanding-the-underscore-of-python-309d1a029edc ] !\n\nHala Python!",
                "Python is the easiest language to learn among all languages because of it's syntax.\n\n\n%3E Edit: img updated\nHow long it takes to learn Python\n\nIt depends upon that how much effort you do but I would say, if you spend 2+ hours in a day daily, after 3 months, you will be able to write basic python program that doesn't give you a job. In order to get a job, you need to specialise in one area like web applications, machine learning etc.\n\nFor example if you want to become a web developer you should also learn HTML, CSS, some JavaScript and Django which is the popular Python framework for building web applications. To learn these things would take you another 6 months. So in total you need 10 - 13 Months to become a job ready as a junior developer. So let's start\n\nFirst, Basic of Python\n\n * Variables\n * Recieving inputs\n * Type conversation\n * Strings\n * Formatted strings\n * String methods\n * Arithmetic Operations\n * Maths functions\n * If else\n * Logical operators\n * Comparison Operators\n * While loops\n * For loops\n * Nested loops\n * Lists\n * 2D lists\n * List methods\n * Tuples\n * Unpacking\n * Dictionaries\n * Parameter\n * Keywords argument\n * Comments\n * Classes and Objects\n * Inheritance\n * Constructor\n * Modules\n * Packages, etc.\nFrom above you can make a some simple Programs and can Implement Data structures like Stack, Queue, Linked-list, sorting, searching, factorial, tik-tak-toe without graphics, etc.\n\nSecond, choose your specialise area\n\n * Django for web developer\n * Matplotlib, TensorFlow, Scikit-learn, Numpy for machine learning\n * Sci-pi, pandas etc for Data science\n * Pygame etc for game development etc.\nThird, learn your favourite library. As of now you will be able to get a job as a junior software developer.\n\nFollow me Ashish Pandey (\u0906\u0936\u0940\u0937 \u092a\u093e\u0902\u0921\u0947) [ https://www.quora.com/profile/Ashish-Pandey-1678 ] to get more programming and computer related answers. answers.",
                "Python is one of the most popular programming languages, known for its simplicity and versatility. If you're looking to learn Python for free online, there are several excellent resources available. Here are some of the best:\n\n1. Codecademy: Codecademy offers an interactive Python course that covers the basics of programming in Python. The course is beginner-friendly and allows you to practice coding directly in your browser.\n2. Coursera: Coursera provides free Python courses from universities such as the University of Michigan and Rice University. These courses cover various aspects of Python programming and are taught by experienced instructors.\n3. edX: Similar to Coursera, edX offers free Python courses from prestigious universities like MIT and Harvard. These courses cover a wide range of topics, from beginner to advanced Python programming.\n4. SoloLearn: SoloLearn offers a free Python course with interactive lessons, quizzes, and a supportive community. It's suitable for beginners who want to learn Python on the go.\n5. YouTube: Many YouTube channels, such as Corey Schafer, Sentdex, and Programming with Mosh, offer free Python tutorials. These channels cover a wide range of Python topics and are suitable for learners of all levels.\n6. Official Python Documentation: The Python website provides comprehensive and free documentation, including tutorials, guides, and references for learning Python. It's a valuable resource for both beginners and experienced programmers.\n7. Kaggle: Kaggle offers free Python tutorials and interactive coding exercises specifically geared towards data science and machine learning. It's a great platform to practice Python programming in real-world scenarios.\n8. GitHub: GitHub hosts numerous free Python repositories and projects that can serve as learning resources. You can find sample code, projects, and tutorials contributed by the community.\n9. Real Python: Real Python provides a plethora of free Python tutorials, articles, and resources for developers at all skill levels. It covers various Python topics, from basic syntax to advanced concepts.\n10. Google's Python Class: Google offers a free Python class that includes written materials, lecture videos, and exercises. It's a comprehensive resource for beginners who want to learn Python from scratch.\nThese resources offer a variety of learning materials, including tutorials, interactive exercises, videos, and projects, making them suitable for learners with different preferences and learning styles. Whether you're a complete beginner or an experienced programmer, there's a free online resource available to help you learn Python effectively.\n\nFinally, if you want to become an expert in Python, regular Python training is a must.",
                "***WARNING - Please do not jump to machine learning or data science or making a cool project just after learning some basics of Python. You need to practice some basic questions and try increasing your ability to think. ***\n\nTry to write code for some of these problems:\n\n * Find prime factors of a given number N\n * Check whether a string is palindrome or not\n * Find the H.C.F and L.C.M of two numbers\n * Print the frequency of each character occurring in a string\n * Convert a numeric string to integer\n * Two numbers are stored in a string. Write a program to sum these two numbers. You cannot convert the string to int. Eg: s1 = \u2018123\u2019, s2 = \u201830\u2019, sum = \u2018153\u2019\n * Calculate the factorial of a given number N\n * Check whether a number N is prime or not\n * Find length of a string without using len() function.\n * Convert a string to upper case and lower case. You cannot use built-in functions here.\n * Reverse a string without use any built-in functions\nMake an account on codechef [ https://www.codechef.com/ ] and start solving beginner level problems - beginner | CodeChef [ https://www.codechef.com/problems/school/?sort_by=SuccessfulSubmission&sorting_order=desc ]. Solve at least 50 problems to get good confidence.\n\nComing to how to learn Python\n\nLet me first tell you the important topics that you should study to get a good basic understanding of Python.\n\n * Variables in python - Strings and Numbers\n * Lists in python\n * if statements in python, for and while loop\n * Dictionaries\n * User inputs\n * Functions\n * Classes\n * File handling and Exceptions\nThese are the basic foundations of Python language. Take some time and understand all of these. I have started a course teaching the above topics in order. Do join my course and learn it - Python Tutorial For Beginners [ https://www.youtube.com/playlist?list=PLhqPDa2HoaAZN9pG0cUugTmgAddRtF3zK ]\n\nOne can refer to the the book - Python Crash Course, 2nd Edition: A Hands-On, Project-Based Introduction to Programming [ https://amzn.to/2ETveXz (https://amzn.to/2ETveXz) ]\n\nWhat to do after learning basic Python Programming\n\n * First, solve the questions that I have given at the top. Solve at least 50 beginner level problems on codechef [ http://beginner | CodeChef (https://www.codechef.com/problems/school/?sort_by=SuccessfulSubmission&sorting_order=desc) ]\n * Learn about data structures like linked list, binary search trees, heap, graphs etc. Try to write some simple problems like:\n * \n * Reverse a linked list\n * Find duplicates in a linked list\n * Print different traversals of a binary tree\n\n * Learn about implementing some basic algorithms\n * \n * Finding time complexity of an algorithm\n * Sorting - bubble, selection, merge, quick sort etc\n * Searching - linear and binary search\n * Graphs - BFS and DFS\n * Recursion\n\n * Solve some easy and medium level problems on codechef now\nWhat\u2019s next ?\n\n * Try to learn some python frameworks like flask or django. This will help you in building websites.\n * Learn about web scraping by using Beautifulsoup library in python\n * Get used to some common tools like git, IDE etc\n * Learn about how to use API in Python. Sites like twitter, facebook, google etc do provide their API\u2019s. Learn about how to encode and decode json.\n * Try to write some automation and cool scripts.\n * \n * Post a tweet on twitter using twitter api\n * Write an automation script that fetches you the live score of your favourite sport and shows it in your notifications.\n * Create online tic-tac game using django or flask\n * Create an online calculator\n\n * Learn about how to host your website online.\nSome additional things to try out\n\n * Learn about basics of operating system, computer networking and database. Try to connect each of these concepts to Python.\n * \n * Try to create, delete or rename file using Python\n * Write a client-server chat program\n * Create a database in mysql and try storing and accessing data with Python\n\n * Machine learning - take some course and see if that interests you\n * Data science - take some course and see if that interests you\n * Learn basics of AWS or Azure.\n * Checkout how DevOps/SRE uses Python to automate stuffs.\nDo not forget to follow me on linkedin [ https://www.linkedin.com/in/shivammitra/ ] and ask me doubts if you have any. Do not forget to subscribe to my channel FreeCodeSchool where I am teaching Python programming from scratch.",
                "There are several ways of learning Python in general for the people who are just a beginner to the aforesaid language. However, around 80% of the learning material available on the internet is generic and cannot provide complete in-depth knowledge to those who don\u2019t know the ABC of the language. Nevertheless, the remaining 20% are doing their best to provide the correct and technical guidance to the developers. You can find e-books, video tutorials, and illustrations to simplify the whole learning process.\n\nLearning Python is not a very hectic process and you can in minimum time learn the programming language in a span of 3 to 4 months. However, that is basic python if you are willing to become a professional Python developer and seeking monetization of your skills then you need to specialize in any of the areas like Machine Learning, AI, etc. Hence we have compiled the best resources helpful in studying the language.\n\nHow do I Study Python?\n\nLearning Python is not a difficult job but specializing in any of its branches is but you can begin your journey with these steps:\n\nStart with Basics\n\nAlways start your basics with video tutorials offered by companies like Google, Python, or the most recent and successful course Python Learning Program by Testbook. You can also consider reading books developed by trained Python coders and developers to brush up on your basic knowledge. Try to master the Python Syntax as much as you can because a stronger foundation provides a perfect and professional result.\n\nWrite basic Python Codes\n\nOnce your basics are clear and you get a hold of how this language works you can begin with writing Python Codes. This has to be mastered by intense practice and perseverance. You can start with writing codes for small games, applications, web pages to perfect your skill.\n\nJoin a Professional Specialization Course\n\nClearing just the basics of any programming language won\u2019t be helpful if you are looking for a full-time career in the same field. Similarly, for Python, you need to opt for a specialization course i.e. Machine Learning, Artificial Intelligence, etc, However, it is not possible to learn these easily and you need to get admission to any technical schools to pursue the same.\n\nTake-up Personal Projects\n\nNow that you have completed your specialization program you can now opt for writing Python codes for high-end projects. Just remember that Python is a programming language and there are updates every now and then. The more you work and practice the more your skill will be perfect with time.\n\nAre you seeking more information related to Python Syntax we recommend you to check out the Python Learning Course launched by Testbook under its Testbook Select Initiative. You can also access the online library full of online courses, video lessons, practice tests, assignments, etc through our app.",
                "1.) Practice Python Problems on Hackerrank: This is a good way to make sure you actually understand the material. Hackerrank makes it fun and the practice problems keeps you stimulated. Had I not done this, I would\u2019ve forgotten everything I\u2019d learn the very next week. Hackerrank problems help tremendously with retention.\n\n2.) Set Up iPython/Jupyter Notebook: It helps to be able to quickly execute a few lines of code every so often while you\u2019re learning. There are a lot of books that tell you to only execute an entire program with long lines of code. This is a bad way of learning. You need to get a good feel for how the computer reads each line of your code in the beginning.\n\n3.) Have a Goal/Purpose in Mind: To learn Python in record time, you need a sense of urgency. I was super-ambitious and obviously overestimated myself when I let potential employers send me Python \u201ctests\u201d after only a week of learning Python. Needless to say, I couldn\u2019t complete them after my first week of Python (but I could after my second or third week!). I still benefited greatly because this sense of urgency made forced me to sign up for video courses on Python Pandas, which I crammed for. The employer\u2019s \u201ctests\u201d gave me a pretty good sense of which particular Python-related skills the job market was looking for.\n\n4.) Build Real-World Applications/Programs: After you\u2019re done with learning the Python basics/fundamentals, focus your attention on courses/materials that walk you through how to make specific real-world applications. I paid for some (inexpensive) Udemy courses after my first week that taught me how to make 8 or 10 real-world applications. This will give you a much better sense of the scope of Python. Some of the applications I built were standalone GUI interfaces with databases inside them, others were web scraping programs or websites. To broaden my net for jobs in the future, I also spent some time learning Machine Learning using Python on Kaggle (which hosts lots of Machine Learning/Data Science competitions).\n\nAll of this gave me an understanding that Python is used for literally everything - from making interactive websites (with Django) to Machine Learning (with Scikit-Learn) to creating financial/trading algorithms.",
                "Absolutely you can.\n\nIf this is your first programming language you have to learn first how to solve problems using programming. If you already know any other language it will be much faster.\n\nI believe to learn any programming language better, you have to do projects. Learning the syntax of a language is not time-consuming. But to understand it and figuring out some special techniques, it is very important to do projects. It is always good to choose easier and smaller project when you are learning so that you can start and finish the project.\n\nFor Python, it is important to learn the libraries as well. Choose any field of your interest ( i.e Machine learning, Web development, task automation, etc), then learn some important libraries for that field. For example, for machine learning/ data science ( if you know the theories already ), you have to know Numpy, Matplotlib, Pandas at least. Then, of course, some other libraries like Sklearn, Tensorflow, etc. So learn those and starting solving problems from your project using those libs. For web development at least you have to know any web framework like Flask or Django and of course other libs what can be necessary.\n\nAll the best and happy coding",
                "1.) Practice Python Problems on Hackerrank: This is a good way to make sure you actually understand the material. Hackerrank makes it fun and the practice problems keeps you stimulated. Had I not done this, I would\u2019ve forgotten everything I\u2019d learn the very next week. Hackerrank problems help tremendously with retention.\n\n2.) Set Up iPython/Jupyter Notebook: It helps to be able to quickly execute a few lines of code every so often while you\u2019re learning. There are a lot of books that tell you to only execute an entire program with long lines of code. This is a bad way of learning. You need to get a good feel for how the computer reads each line of your code in the beginning.\n\n3.) Have a Goal/Purpose in Mind: To learn Python in record time, you need a sense of urgency. I was super-ambitious and obviously overestimated myself when I let potential employers send me Python \u201ctests\u201d after only a week of learning Python. Needless to say, I couldn\u2019t complete them after my first week of Python (but I could after my second or third week!). I still benefited greatly because this sense of urgency made forced me to sign up for video courses on Python Pandas, which I crammed for. The employer\u2019s \u201ctests\u201d gave me a pretty good sense of which particular Python-related skills the job market was looking for.\n\n4.) Build Real-World Applications/Programs: After you\u2019re done with learning the Python basics/fundamentals, focus your attention on courses/materials that walk you through how to make specific real-world applications. I paid for some (inexpensive) Udemy courses after my first week that taught me how to make 8 or 10 real-world applications. This will give you a much better sense of the scope of Python. Some of the applications I built were standalone GUI interfaces with databases inside them, others were web scraping programs or websites. To broaden my net for jobs in the future, I also spent some time learning Machine Learning using Python on Kaggle (which hosts lots of Machine Learning/Data Science competitions).\n\nAll of this gave me an understanding that Python is used for literally everything - from making interactive websites (with Django) to Machine Learning (with Scikit-Learn) to creating financial/trading algorithms.",
                "1.) Practice Python Problems on Hackerrank: This is a good way to make sure you actually understand the material. Hackerrank makes it fun and the practice problems keeps you stimulated. Had I not done this, I would\u2019ve forgotten everything I\u2019d learn the very next week. Hackerrank problems help tremendously with retention.\n\n2.) Set Up iPython/Jupyter Notebook: It helps to be able to quickly execute a few lines of code every so often while you\u2019re learning. There are a lot of books that tell you to only execute an entire program with long lines of code. This is a bad way of learning. You need to get a good feel for how the computer reads each line of your code in the beginning.\n\n3.) Have a Goal/Purpose in Mind: To learn Python in record time, you need a sense of urgency. I was super-ambitious and obviously overestimated myself when I let potential employers send me Python \u201ctests\u201d after only a week of learning Python. Needless to say, I couldn\u2019t complete them after my first week of Python (but I could after my second or third week!). I still benefited greatly because this sense of urgency made forced me to sign up for video courses on Python Pandas, which I crammed for. The employer\u2019s \u201ctests\u201d gave me a pretty good sense of which particular Python-related skills the job market was looking for.\n\n4.) Build Real-World Applications/Programs: After you\u2019re done with learning the Python basics/fundamentals, focus your attention on courses/materials that walk you through how to make specific real-world applications. I paid for some (inexpensive) Udemy courses after my first week that taught me how to make 8 or 10 real-world applications. This will give you a much better sense of the scope of Python. Some of the applications I built were standalone GUI interfaces with databases inside them, others were web scraping programs or websites. To broaden my net for jobs in the future, I also spent some time learning Machine Learning using Python on Kaggle (which hosts lots of Machine Learning/Data Science competitions).\n\nAll of this gave me an understanding that Python is used for literally everything - from making interactive websites (with Django) to Machine Learning (with Scikit-Learn) to creating financial/trading algorithms.",
                "Umm.. let me first tell you about programming languages. Actually programming languages are just a tool to implement programming concepts. Learning a programming language like python, is just learning a kind of syntax to implement programming concepts. How much time will it take to learn python depends on how much you know about programming!\n\nIf you have had experience with some other programming languages it\u2019ll not take more than 3\u20134 days to understand the python syntax. Well, in case if you're new to programming it may take some time to delve into this field to learn all sorts of operations, manipulations and other concepts. With python, I think it\u2019ll be good to begin learning programming. This language resembles the English language making it less cryptic to eyes and a lot easier to understand.\n\nI cannot blindly make a guess about the time to learn it, as it also depends on you, how much time you devote everyday and also what your end-goal is. With end-goal what I am referring to is, what you wanna achieve after learning python. Python is such a great language that it has its application almost everywhere. From simple desktop apps to data-analysis and machine-learning, python has purely become a general purpose language.\n\nPersonally, I love python and would always suggest beginners to start with this. It's easier to learn, you might find it more interesting than C or C++, they somewhat bring nightmares for beginners although they are wonderful too.\n\nAgain, how much time will it take, it all depends on you. A fair guess, if you just want to get a hang of some basic functionality and data structures, it would take around a month or so. Now when you're done with basics you are ready to choose your path towards your end-goal whether it's web development, data science, machine learning or what not!\n\nWish you all the best! DM me if you need any help."
            ]
        },
        {
            "tag": "other",
            "patterns": [
                "Which are the popular algorithms asked in tech interviews?",
                "How can I learn algorithms and data structures from scratch?",
                "If technology is liberating, what does it free us from?",
                "How do you think technology will save mankind?",
                "Will technology replace humans one day?",
                "Is AI an existential threat to humanity?",
                "Who do you think has really made the biggest difference for humanity, from the world of technology?",
                "Does technological advancement mean the end of all human freedom?",
                "If technology is liberating, what does it free us from?",
                "How can I learn mathematics?",
                "Will AI replace physicists? Is it safe to major in physics and choose a research career in physics in the new age of AI?",
                "How hard is the work of a data scientist?",
                "How hard is the work of a data scientist?",
                "What are the top 1% programmers you know? What can I do like them?",
                "What distinguishes the Top 1% of product managers from the Top 10%?",
                "What are the characteristics of a top 1% software developer?",
                "What separates the top one percent of computer programmers from the top ten percent?",
                "How do I become an expert level programmer in a week?"
            ],
            "responses": [
                "If the expectation is to see an encyclopedia of algorithms or techniques listed out as answers here, you'd likely be disappointed. There are really infinite possibilities for questions in tech interviews across companies in the world. A large number of new questions are invented everyday. There is nothing like a 'list of questions' that when leaked in here, is guaranteed to help you ace through interviews.\n\nOne's best bet to crack tech interviews, would be to 'try and understand' the fundamentals of algorithms, data structures and complexity calculations, and try and 'apply' this knowledge to problems in tech interviews. Also translate this into code and try that out. Definitely familiarize oneself with the basic techniques (divide-and-conquer, dynamic programming, graph algos, etc...) and the fundamental data structures (search trees, lists, stack, graphs, etc...). Try and solve as many problems as one can. Best of luck!",
                "Day [math]-\u221e[/math] to 0: Stick to a programming language like C or C++. Make sure that you are comfortable with pointers/objects.\n\nDay 1: Understand the concept of Algorithmic complexity [ https://en.wikipedia.org/wiki/Algorithmic_complexity ]. Skip the theory for now, but for every piece of code you write, you should be able to derive both time and space complexity.\n\nDay 2 - 10: Let\u2019s start with some simple data structures,\n\n1. Arrays\n2. Linked Lists\n3. Strings\n4. Stacks\n5. Queues\nUnderstand their basic operations (insert, delete, search, traversal) and their complexity - Big-O Algorithm Complexity Cheat Sheet [ http://bigocheatsheet.com/ ], and code them all.\n\nDay 11 - 25: Let\u2019s now learn some simple algorithms,\n\n1. Sorting - Insertion sort [ https://en.wikipedia.org/wiki/Insertion_sort ], Merge sort [ https://en.wikipedia.org/wiki/Merge_sort ], Quick sort [ https://en.wikipedia.org/wiki/Quicksort ], Heap sort [ https://en.wikipedia.org/wiki/Heapsort ], Bucket sort [ https://en.wikipedia.org/wiki/Bucket_sort ], Counting sort [ https://en.wikipedia.org/wiki/Counting_sort ], Radix sort [ https://en.wikipedia.org/wiki/Radix_sort ], External sorting [ https://en.wikipedia.org/wiki/External_sorting ]\n2. Search - Linear search [ https://en.wikipedia.org/wiki/Linear_search ], Binary Search [ https://www.topcoder.com/community/data-science/data-science-tutorials/binary-search/ ] (along with its variants).\n3. Prime Numbers - Sieve of Eratosthenes [ https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes ], Primality test [ https://en.wikipedia.org/wiki/Primality_test ]\n4. Strings - String searching [ https://en.wikipedia.org/wiki/String_searching_algorithm ], LCS [ https://en.wikipedia.org/wiki/Longest_common_subsequence_problem ], Palindrome detection [ https://www.rosettacode.org/wiki/Palindrome_detection ]\n5. Miscellaneous - Euclidean algorithm [ https://en.wikipedia.org/wiki/Euclidean_algorithm ], Matrix multiplication [ https://en.wikipedia.org/wiki/Matrix_multiplication ], Fibonacci Numbers [ https://en.wikibooks.org/wiki/Algorithm_Implementation/Mathematics/Fibonacci_Number_Program ], Pascal's Triangle [ http://www.geeksforgeeks.org/pascal-triangle/ ], Max Subarray problem [ https://en.wikipedia.org/wiki/Maximum_subarray_problem ]\nDay 26 - 50: Once you are comfortable with everything above, start doing problems from,\n\n1. Cracking the Coding Interview [ https://www.amazon.com/Cracking-Coding-Interview-Programming-Questions/dp/0984782850 ]\n2. Elements of Programming Interviews [ https://www.amazon.com/Elements-Programming-Interviews-Insiders-Guide/dp/1479274836 ]\n3. Programming Interviews Exposed: Secrets to Landing Your Next Job [ https://www.amazon.com/Programming-Interviews-Exposed-Secrets-Landing/dp/1118261364 ]\n4. GeeksforGeeks [ http://www.practice.geeksforgeeks.org/ ]\n5. HackerRank [ https://www.hackerrank.com/ ]\n6. InterviewBit [ https://www.interviewbit.com/invite/afaf ]\nStick to chapters of arrays, linked lists, strings, stacks, queues and complexity.\n\nDay 51 - 60: Let\u2019s learn some non-linear data structures,\n\n1. Tree\n2. \n1. Binary Tree, Binary Search Tree - Tree traversals [ https://en.wikipedia.org/wiki/Tree_traversal ], Lowest common ancestor [ https://en.wikipedia.org/wiki/Lowest_common_ancestor ], Depth, Height & Diameter [ http://stackoverflow.com/questions/2603692/what-is-the-difference-between-tree-depth-and-height ], Finding k-th smallest element [ http://www.geeksforgeeks.org/find-k-th-smallest-element-in-bst-order-statistics-in-bst/ ]\n2. Heaps\n\n3. Hash table - 4 sum problem [ http://www.sigmainfy.com/blog/4sum-problem-analysis-different-time-complexity.html ], Checking if sudoku solution is valid [ http://stackoverflow.com/questions/5484629/check-if-sudoku-solution-is-valid ]\n4. Graph - Breadth-first search [ https://en.wikipedia.org/wiki/Breadth-first_search ], Depth-first search [ https://en.wikipedia.org/wiki/Depth-first_search ], Topological sorting [ https://en.wikipedia.org/wiki/Topological_sorting ], Minimum spanning tree [ https://en.wikipedia.org/wiki/Minimum_spanning_tree ], Shortest path problem [ https://en.wikipedia.org/wiki/Shortest_path_problem ],\nDay 61- 90: Refer to the previous resources and start doing problems from trees, hash tables, heaps and graphs.\n\nDay 91 - 100: Understand Computational complexity theory [ https://en.wikipedia.org/wiki/Computational_complexity_theory ] and NP-completeness [ https://en.wikipedia.org/wiki/NP-completeness ], Knapsack problem [ https://en.wikipedia.org/wiki/Knapsack_problem ], Travelling salesman problem [ https://en.wikipedia.org/wiki/Travelling_salesman_problem ], SAT problem [ https://en.wikipedia.org/wiki/Boolean_satisfiability_problem ] and so on.\n\nDay 101 - [math]\u221e[/math]: You are now better than most of the CS undergrads. Keep revising the above topics and start competitive programming! Good luck!\n\nThanks for the A2A Meghna Bhasin [ https://www.quora.com/profile/Meghna-Bhasin-1 ]",
                "\"Technology\" is just a word for objects and systems that make certain jobs easier. A pencil is a technology that makes writing easier (e.g. you don't need ink) and email is technology that makes written-at-a-distance communication easier. \n\nTechnology changes the world, and change isn't necessarily liberating or binding. It's often both. Sometimes a technology liberates people one way but binds them in another way. Email frees me from lines at the post office but keeps me shackled to my computer. Sometimes a technology liberates one class of people while binding another. Coal liberated many from having to chop wood in the winter while trapping others in coal mines. \n\nThere are few universal or general statements one can make about \"technology,\" because it doesn't really exist. What exists are individual tools: pencils, wheels, tooth brushes, toilets, garbage cans, trucks, atomic bombs, x-ray machines, saran wrap, DVD players, bandages, microprocessors, shoes, etc. Each has caused specific changes and each has liberated or bound us in a variety of ways. \n\nIs paper liberating?\n\nWhat about the gas chambers in Nazi death camps? \n\n\"Technology\" is just an umbrella term -- one that is useful to a point. But if we want to say anything specific, we need to at least narrow things down to a class of technology, such as \"communications\" or \"food preparation.\"\n\nAnyone who answers this question in a simple way, saying \"It's liberating, because...\" or something similar, is likely thinking about a specific technology or group of technologies used in specific contexts, even if he's not explicit about what he means. \n\nI've found that when I say \"technology\" to people, they tend to think, \"You know, like iPhones and stuff.\" They don't think of combs and toasters, though those objects were once as cutting edge as Kindles.",
                "Only by providing humanity with the tools it needs to survive and adapt.",
                "Would you consider this human enough?\n\nHere the brain of a child is merged to a much bigger machine.\n\nElon Musk, thanks to his new company, Neuralink, has a clear objective:\n\nMerge the man and machine together.\n\nMusk explained that the firm's goal is to turn cloud-based AI into an extension of the human brain. The company will aim to create (and bring to market) a product that can help those with severe brain injuries. It's product, which will be developed within the next four years will eventually lead humans to be able to communicate by \"consensual telepathy\".\n\nHowever, as opposed to the representation of the image above, let's hope we end up being \u2018The Ones\u2019 that control it, not the other way around.",
                "AI generated images are one of the scariest things I can imagine. They may eventually render all photographic evidence obsolete. Can you imagine the effect that could have on the criminal justice system? Photography and film is about to all be inadmissable in court. AI generated content is notoriously bad at hands, for some reason.\n\nThey can manage faces, typically, and they can manage bodies decently enough. But hands? They suck at them. It\u2019s fairly easily for a thief to wear a fake finger, or two, and make it appear he has an extra digit. It\u2019ll throw juries for a loop, no doubt. And since \u201cdeepfakes\u201d are now a thing, you make anyone say anything, do anything\u2026\n\nhttps://www.youtube.com/watch?v=cIQ4lf6vkJo\nWhat to make of voices? AI can make pre-recorded voices say anything. You can make Arnold Schwarzenegger sing Whitney Houston. Or Celine Dion. Or have him play in Titanic. But, less innocently\u2026 you could have him say horribly racist or criminal things, too. Or admit to things he didn\u2019t do, things he never said. And people can do that with anyone.\n\nSo while I wouldn\u2019t say AI is necessarily an existential threat to us as a species\u2026 it can definitely be a massive disruption to our society. It\u2019s the ultimate culmination of the \u201cpost truth era\u201d\u2026 in a world where anyone is already eager to dismiss any unfavorable story as \u2018fake news\u2019, AI makes it a million times easier to do so. And with good reason, too. That should scare anyone.",
                "Alan Turing.\n\nThis guy is considered to be one of the fathers of the modern computer. He worked with British intelligence during ww2 to help develop the first computers ( the turing machine) to allow for the breaking of the enigma code.\n\nHis work helped allow for the entirety of modern computer technology to occur. Considering all that we have achieved with computers, including modern space travel, the internet and by extension all of modern communication, the safest era in aviation to date, and of course video games, his impact on modern society is rather profound.",
                "This is bullshit.\n\nA \u201cpoker face\u201d is only present when the person is actively perpetrating deception, which leaves out all forms of self deception, open aggression and psychotic intent. Coming from Dolby Laboratories, one would expect an audio technology, yet from what \u201cchief scientist Poppy Crum\u201d says (doesn\u2019t that name sound made up? edit: so I looked up Poppy Crum, she\u2019s a real person. I apologize for thinking you were fictional.) one would think she just discovered micro-expressions. Yet again, this is on Yahoo! News.\n\nI think your poker face is quite safe.",
                "It solves whatever problems it's designed to solve.\n\nThat doesn't mean it doesn't create new problems, or at least turn up problems you never realized you had. We used to cope with a lot of problems, like having too little food or spending vast amounts of our days washing clothing. Having plenty of food has turned up obvious problems that we're not prepared to cope with having food too plentiful, and our new fetish for cleanliness means we put a lot of chemicals into the environment. But we are liberated from the initial issues.\n\nNo, we haven't invented the one magic technology that washes your cat and walks re-seals your driveway. And even if we did, we'd complain about being bored. But I don't see \"technology hasn't solved every problem ever therefore we shouldn't bother with it\" as much of an argument.",
                "I think by \"practical\", you mean in a clear, concise way you can understand, yes?\n\nBegin with a subject like pre-algebra. That's where we start all college students entering as freshmen who score very low on their mathematics compass scores.\n\nPre-algebra should get you familiar with the language we use when discussing mathematics. It will also take you through what the variables mean, and then will begin to show you how we use basic logic to solve problems.\n\nEDIT: Another thing my article above says is to begin at the beginning: with pre-algebra. In fact, another one of my most favourite sites that I didn't mention the first time is called Purplemath. THIS link is absolutely WONDERFUL for what you need! https://www.purplemath.com/modul... [ https://www.purplemath.com/modules/ordering.htm ]And if you look down the side on the left at those links, guess what you'll see? Pre-algebra!\n\nBut the cool thing about this page is that everything is given to you IN THE ORDER IN WHICH YOU NEED IT! ALL the guesswork is removed for you so you don't have to wonder if you're learning everything in the correct order. The thing they don't tell you when you\u2019re in the classroom learning the mechanics and equations/formulae is how the thing you're learning is going to fit into future things, so this is what I talk about later in this article under the subject of worst-taught subject: learning things in a highly-specific order is crucial for retaining and homing those maths skills. And this page will solve that mystery for you!\n\nSee, something folks don't understand is that math isn't about the numbers: the numbers just pay the bills. Math is mostly about LOGIC--the fundamental way we attack life's problems. As mathematicians, we're given X, and then we're shown a ton of different tools to prove X. Did you catch that?? It took me three years of maths classes before I realised there wasn\u2019t just ONE way to solve for X, but they were all right! This meant what we were being taught was a set of TOOLS to solve various problems. But no matter what the tools are called, it's still ALL based on that underlying idea of LOGIC. Mathematics majors enter law school in droves each year, because they know how to argue and prove an argument--big skill needed to be a competent lawyer.\n\nLook up Numb3rs on Hulu, too. (It\u2019s also airing every Friday in the States on a station called H&I TV Network [ http://HandITV.com ] . They run 9 episodes back-to-back all day long, so it\u2019s easy to catch up on the story line. My favourite show, and I'm friends with most of its stars. Cheryl Heuton developed this show to show people that you can do more with math than just teach it. It ran on CBS for 6 years.\n\nPatrickJMT on YouTube got me through Calculus II, so his web-site is one of the absolute best placse to begin\u2014especially for lower- and upper-division mathematics students, because he is the best there is--even better than Khan. Khan still tends to use more math-speak than I like, but I quickly realised that Patrick doesn't. He used to teach Applied Mathematics at Vanderbilt and got sick of the politics, so began doing free tutoring videos. His web-site is: Just Math Tutoring [ http://www.justmathtutoring.com ] Let me know if that doesn't work for you.\n\nThe second thing I would tell you is be prepared to PRACTICE UNTIL YOU CAN'T THINK ABOUT ANOTHER NUMBER ANYMORE. Math is not a spectator sport. You have to work problems until you're sick of them in order to master it. I even go to other mathematics professors outside my own class professor and beg them to give me more problems. You only get better at it when you work it. Math isn't one of those mystical gifts bestowed only on the lucky few from the math gods. That's a bunch of crap, and I'm tired of seeing that and people believing it. Math CAN be learned by anyone if they're willing to work hard at it. Maybe you will begin and just get it. That's great! But you still might need to work harder at the higher, more advanced mathematics. (It's sad to me that the government is currently whining for more people to enter the STEM disciplines, but at the same time, they're currently telling teachers they're no longer ALLOWED to assign math homework. Srsly?? THAT'S the idea you're going with to accomplish this lofty goal?)\n\nEDIT: I need to talk a little about studying. If you follow both of these techniques you will NEVER have to study or cram for another exam in your entire life. I found something of incredible value since editing this article the last time, and I need to share it with you now, because it supports almost word-for-word what I say about studying without cramming for exams! Plus, it\u2019s getting me through some tough times. See, I suffer with Systemic Lupus Erythmatosus and now have two pinched nerves in my neck. Not only do they cause intractable pain every day, but they\u2019re making my dominant hand weaker and is causing mobility issues. For instance, now without warning, I throw pencils across the room for no reason, and there\u2019s no indication it\u2019s coming. ALL of this interrupts my ability to concentrate, focus, and remember equations. That\u2019s why the following is SO important for you!\n\nREPETITION MATTERS! (This even works for non-maths classes like chemistry and physics!)\n\nStudy Techniques\u2014\n\n * Day 1. Read through section titles and skim for key concepts. (Normally, at the end of your chapters there will be a section for review that will actually say \u201ckey concepts\u201d or \u201csection review\u201d. Read these FIRST, before going to your lecture the next day; read those even before reading your regular chapters. It will get you familiar with the material about to be covered, which will help set it in concrete for you to recall at a later date. Every single time I pick up my book, the first place I go is NOT the chapter to read it, but I go to the end of the chapter and section and read through the review stuff. Plus, you can be assured no one else is going to pre-study a chapter, so it will make you look like an extra-smart GOD to answer questions!)\n * \n * Read through lecture sections, taking notes on concepts. Work ALL examples, sample problems, and some of the problems. You won\u2019t know how to do them all, but that\u2019s the point. It will make sense in a bit.\n\n * Lecture Day 1. Attend lecture, take notes. Then after returning home, it is imperative that you REWRITE ALL OF YOUR NOTES! Normally, professors speak so quickly that in order to capture what you want, you\u2019re writing very sloppily. Being able to understand what you wrote and what you meant by that little comment \u201csee iLearn, pp. 20\u201322\u201d means dealing with it shortly after class. That\u2019s why it\u2019s imperative that you rewrite your notes WITHIN TWENTY MINUTES OF CLASS TIME. Any length of time past twenty minutes and you\u2019ve lost 1/3 of what you just learned.\n * \n * Review the sections of the lecture and book that gave you the most trouble with understanding, then work additional problems at the end of the chapter. I also suggest reworking your example problems, your sample problems, and the homework problems again, but this time, add a few new problems to it. NOTE: ANY TIME YOU\u2019RE DOING HOMEWORK AND FIND YOURSELF FLIPPING BACK TO THE EXAMPLES OR TEXT, THIS MEANS YOU STILL DO NOT UNDERSTAND WHAT YOU\u2019RE DOING! This is key to knowing if you\u2019re truly learning the material or not. If this is what you\u2019re doing, and you\u2019re not understanding the problems, stop working the problems, put your pencil down, and go back and re-read your chapter again. Re-read the review sections at the end of the chapter/sections, refamiliarise yourself with the equations, and re-work all examples. THEN go back and start the homework again. If you still can\u2019t do it, repeat this process until you can work problems without needing additional help.\n\n * Day 3. Repeat day 1. The professors generally tell you what sections will be discussed on what day, so do for tomorrow\u2019s upcoming sections/lecture what you did for the day before.\n * \n * Review the information at the end of the section, work examples, write down definitions (now\u2019s the perfect time to add them to note/index cards! At the end of this I\u2019ll give my favourite notecard app for Android, too), key concepts, and new problems for that section. Then, this is key: go back to the previous section and work MORE of those problems!\n\n * Lecture Day 2. Do everything you did for lecture day 1, but with new material. Repeat this cycle.\nGet in the habit of reviewing what you\u2019ve learned within 1\u20132 days of learning it. You\u2019re not only keeping up with new material coming at you, but you\u2019re now in the habit of automatically reviewing previous material. As long as you remain in this habit, YOU WILL NEVER HAVE TO STUDY FOR ANOTHER EXAM IN YOUR ENTIRE LIFE.\n\nStructure your learning in a way that always reviews previous material; you won\u2019t have to relearn it in a midnight cram session before an exam!\n\n\nMath is one of THE worst-taught subjects on the planet. Why, and how do I know this? Well, let's tear it apart and find out.\n\nThink back on how you began learning math in school. What were the first things they had you doing? Probably learning to count, memorising the names of the numbers and the order in which they came, right?\n\nNext, you moved on to basic operations, like +, -, x, /.\n\nNow you're older and introduced to the equation: =, and shown that whatever you do on one side needs to be done on the other.\n\nGreat. Then what? They upped the degree of difficulty by now throwing in something called variables--the bane of your future existence.\n\nNext they're using variables to determine quantities of things you've never heard of, all the while adding in OTHER, more complicated ideas and equations, like imaginary numbers, functions, square roots that don't exist, more complicated terms like polynomials and rules of exponents, logarithms and the list goes on with each new idea increasing in difficulty, continuing through your senior year and AP Calculus.\n\nNow, let me ask you one VERY crucial question: Was there EVER a point in time in which even ONE of those teachers actually bothered to tell you when this information would be needed, or why? Did even one of those teachers have a half-way decent answer to your constant question, \"What good is math if I'm not going to teach it?\" Did you enter your first Calculus class and feel like someone had lied to you all those years before, because they never, ever told you that it would be like that, because now, after SO many years, you were expected to APPLY those stupid-ass geometric proofs from 6 years ago? Did you completely freak out when they gave you your first problem that included the word \"Physics\" and a bunch of other words??\n\nMy guess is nope; nope; yep; yep and YEP!\n\nWelcome to the worst-taught subject. What no one failed to make clear to you is what I said earlier: that math isn't about numbers, it's about teaching you the LOGIC needed to reason your way through any subject on the planet, even if it isn't math. And they never told you that because sadly, no one ever told them. They were only teaching you what had been taught to them, and frankly, no one knew what they were doing.\n\nWhen you force yourself to learn the rules of these equations and how to work them, it's logic you're learning. Learning the theory behind it through rote memorisation has its merits--if you can recall the theory during a quiz, and know HOW to apply the theory (hopefully, through all the homework you've done), then trying to remember it is one less beast you have to fight; being able to have all your derivatives and integrals memorised BEFORE an exam means you won't waste your time fumbling to remember that the derivative of sin(x) is equal to the negative integral of sin(x).\n\nBut the thing I wanted to impress upon you right now is that you need to be cognisant of ALL of it. Make sure that when you're learning this stuff, you realise that it fits into a much bigger picture called APPLICATION. Every single exercise you learn to do right now will quantitatively prepare you for the day when you stare down at a physics application problem and have to calculate how quickly the water is leaking from a 30-kL tank that was half-full when it started to leak, and the water level is lowering at a certain rate in the tub with a specific diameter.\n\nBut it will also qualitatively prepare you for the day when you're facing a situation in which you have no clue how to get yourself out alive. The stronger your logic skills are at that time, the greatest chance you have at surviving. (I was homeless for 2-1/2 years, living out of my car with two cats, and if it hadn't been for my brain, I might have been dead, but I survived and am here, and grateful every single day.)\n\nPeople are losing the ability to THINK in this country, and frankly, it's because of math anxiety, called Arithmophobia (which makes us hate it even more, and it's a proven fact that you tend to do poorly in subjects that you dislike), and poor teachers who probably had it and were too ashamed to ask someone for help, combined with the fact that they would have rather been outside coaching the women's softball team instead of sitting in your seventh period algebra class, making sure you did well.\n\nI started the \"4 out of 10 Majority-Hater's Club\" at our University, and it's for people who are afraid of, or intensely hate math. In it I show frequent clips of the show Numb3rs that I mentioned earlier, along with very cool mathematical facts/oddities that not many might realise. There is a sheer, simple beauty and symmetry to math. I had no idea that there were tons of careers in math that had nothing to do with teaching or being an engineer. And when THAT world opened up to me, I began to see math's beauty far outside the numbers. (Did you know the seeds on a sunflower and raspberry, along with the bracht on a pinecone and pineapple are arranged in a perfect Fibonacci sequence?) But more than that, the club is a SAFE place students can come and bitch/vent about their fears/dislikes, because if you're a student at one of the top engineering schools in the nation and you hate math, then you're suddenly their dirty little secret and no one wants to acknowledge your existence. At least this way, they don't feel like they're alone.\n\n\nI keep the repetition review in my disc-bound notebook right in front of my index cards. I also keep it in my study notebooks so it\u2019s always in front of me, reminding me that for material that doesn\u2019t come easily or intuitively to me, REPETITIOUS REVIEW IS THE KEY!\n\nI found an Android app a few years ago called \u201cFlashcard Machine\u201d. It\u2019s got very powerful features, but my favourite is being able to declare the front of a card and the back of a card. You can save them in specific groups, and enter either \u201cstudy\u201d mode, or my favourite, \u201cquiz\u201d mode! It will shuffle the cards for you so you\u2019re not automatically remembering the answers based on the answer before or after it (something I tend to do easily with Asperger\u2019s), and then will give you multiple choice with 4 cards. Don\u2019t just take your quiz once and decide you\u2019ve learned it. Why? Because repetitious review is the key! Take the quizzes ten or twelve times! Since it\u2019s on your phone you don\u2019t have to write anything, and the more you look at it the more you\u2019ll remember them. I\u2019m sure Apple has comparable apps, but with this one, you can even log into their web-site and sync changes you make in one place or the other, keeping everything up to date. At the end of this article, I\u2019ll share photos of my discbound planner and how I use index cards to help me remember even the earliest things.\n\nI wanted to also share photos of the way I organise my note cards, because organisation of both physical properties and time is another key to success.\n\nIt\u2019s called the \u201cDisc-bound planner system\u201d and several places sell them. Martha Stewart, ARC, and the Happy Planner are just a few. I think Leverage even has one\u2014I have some of their translucent teal discs. Pages are cut down the side where a 3-hole punch would be. They\u2019re cut-outs that allow a disc to fit in them. The beauty of this system is that you can move pages around all you want! FOR ME, it has the most freedom of organisation.\n\nThis is the actual page I took the above from. Now, me being a girly-girl, I use washi tapes and coloured pens to decorate my pages, but with me being an artist, the different colours and patterns also help me to remember things. For example:\n\nYou can see the purple discs down the left side. These are for an 8.5 x 5.5-sized planner, but you can get them nearly ANY size you wish, all the way up to A3. As you can also see, the left sides of the cards have been punched so the discs fit, and the washi tape helps me categorise the different subjects.\n\nThey make all sorts of accessories for these planners, including the plastic dividers you see behind the cards.\n\nAny time I wish, I can move these cards around, remove less, add more, it\u2019s completely up to me and I LOVE that kind of freedom! I keep all of these in the same notebook with the purple discs behind the calendar/agenda section of the planner. I designed it that way so you wouldn\u2019t have to carry more than one calendar/notebook to class with you. I wanted all the academic things together in ONE notebook.\n\nI am currently working on a highly specific academic planner to sell in my Etsy shop right now. It will be specific for anyone in STEM, including engineering pages, graph papers that coordinate with the calendar portion, pages with index spaces so you don\u2019t need to carry index cards with you to class, you just pop an idea down in your planner and then re-copy it over when you get home, this kind of thing. If this sounds like something you\u2019d like or be interested in, just shoot me a note here and we\u2019ll talk!\n\nThis entire subject could make a full book alone, but I'll stop now, and I hope something I've said will help you. Please let us know how you're progressing when you get a break in homework, and I wish you great studying!",
                "Any article that starts with \u201cAI is a threat to\u2026\u201d tends to be sensationalist. And using that as a basis of even claiming that choosing a career in physics research is in any way going to be problematic in the \u201cnew age of AI\u201d is a misunderstanding of what AI actually does.\n\nFor example, I shall start with how computers, and programming revolutionised astronomy. Even about 40 years ago, astronomers used to look at thousands of photographic plates, or several hundred metres of graphing papers with light curves etched onto them, to detect various astronomical bodies by eye.\n\nThis was an extremely tedious task, and slowed astronomy down quite a bit. Further, it limited the data processing capabilities to human levels, which further put a constraint on the amount of data collected at a time. However, in came computers which could be made to read the data rapidly. Programming allowed faster detections, as the data could be run at blazing fast speeds, which have improved enough to allow terabytes of data to be processed almost daily.\n\nNow that the astronomers weren\u2019t responsible for directly detecting sources, were they suddenly irrelevant? The answer is no. Programming was a tool using which repetitive work, or work that can be done faster through computers was achieved. Knowing what to look for, knowing why we are looking for them, their physical significance, error corrections to results from the program and more were the astronomer\u2019s input.\n\nComing to the AI Physicist in the article, it is definitely a great tool. It can be presented with environments based on which it can figure out the laws necessary, sure. But it is far from a replacement. It needs to be accurately coded. It works with electromagnetic potentials, harmonic potentials etc. on a rigid object. But how about particulate matter? How well does it handle hydrodynamics of a magnetic fluid? How \u201cphysical\u201d are its solutions? All of this needs to be checked and fed in by a physicist.\n\nFor example, using unsupervised learning, I could get a great fit for some experiment. However, what may happen that the fit while minimising residuals, settles into a local minima, and thus ends up throwing unphysical answers, when actually tested.\n\nFig: My AI has found the solution by minimising errors to the local minimum. As the errors rise on either side, it may not choose to explore any further. Unless given a push. Who gives the push?\n\nWhat is thus needed is to push the solution to the global minima of errors. And who can correct for this? The physicist.\n\nIn today\u2019s world, physics is seeing an increasing amount of usage of AI and ML. We can use them to \u201cteach\u201d our machines physics, so that it can handle lots of data. But who teaches them? Who corrects them when they make errors? Who evaluates the physical significance of their results? Who even gives them the data to analyse? And who decides what is data?\n\nThe answer is: the physicists.\n\nSo right now, it is not only safe, but an incredible time to major in physics. We have the best tools to our disposal, and can use it to revolutionise the field, and human life at large. But to exaggerate the tools to imagine them to have capabilities they don\u2019t have, would be an engagement with unnecessary sensationalism.\n\nHence, go for it.",
                "It\u2019s an office job. It\u2019s not hard work.\n\nI\u2019ve been watching the HBO series Chernobyl recently, about the 1986 nuclear accident. A team of coal miners were brought in to seal off the nuclear core by digging a tunnel under the power plant. They worked around the clock in deadly levels of radiation and extreme temperatures. They couldn\u2019t set up fans, because that would swirl up radioactive dust.\n\nNow, that\u2019s hard work.\n\nI, on the other hand, sit in a comfortable chair in my office with two large, friendly monitors, sipping a cup of coffee. Sometimes, I eat lunch, play foosball or have an informal chat with colleagues.\n\nAs a data scientist, I don\u2019t have hard deadlines to deal with. I\u2019m an experimenter, an explorer. I massage the data and ask it questions. I don\u2019t have to work long evenings to get a new release finished. I release code whenever I have something new. I don\u2019t get called in the middle of the night because some system is down. I monitor the performance of my models and update them if they degrade.\n\nI have to keep up to date with technical knowledge, but learning about new technologies and techniques is what got me into this line of work in the first place. I do it because it\u2019s fun, not because I have to.\n\nGenerally speaking, the eight hours I spend at work is the easiest part of my day.",
                "It\u2019s an office job. It\u2019s not hard work.\n\nI\u2019ve been watching the HBO series Chernobyl recently, about the 1986 nuclear accident. A team of coal miners were brought in to seal off the nuclear core by digging a tunnel under the power plant. They worked around the clock in deadly levels of radiation and extreme temperatures. They couldn\u2019t set up fans, because that would swirl up radioactive dust.\n\nNow, that\u2019s hard work.\n\nI, on the other hand, sit in a comfortable chair in my office with two large, friendly monitors, sipping a cup of coffee. Sometimes, I eat lunch, play foosball or have an informal chat with colleagues.\n\nAs a data scientist, I don\u2019t have hard deadlines to deal with. I\u2019m an experimenter, an explorer. I massage the data and ask it questions. I don\u2019t have to work long evenings to get a new release finished. I release code whenever I have something new. I don\u2019t get called in the middle of the night because some system is down. I monitor the performance of my models and update them if they degrade.\n\nI have to keep up to date with technical knowledge, but learning about new technologies and techniques is what got me into this line of work in the first place. I do it because it\u2019s fun, not because I have to.\n\nGenerally speaking, the eight hours I spend at work is the easiest part of my day.",
                "I only know (of) one: Linus Torvalds, founder and lead of the Linux project. I highly recommend watching interviews with him, including at TED. I read his biography in high school and have generally been a \u201cfan\u201d for a long time.\n\nWhat I appreciate about him is his integrity and insistence on high quality. Both in design and implementation of systems. He emphasizes having good \u201ctaste\u201d i.e. seeing a problem in it\u2019s simplest possible form when solving it, to be able to make the solution clean and elegant. Which is why he and his contributors makes the software that basically runs the world with very little competition.\n\nhttps://www.youtube.com/watch?v=o8NPllzkFhE\n",
                "The top 10% of product managers excel at a few of these things. The top 1% excel at most or all of them:\n * Think big - A 1% PM's thinking won't be constrained by the resources available to them today or today's market environment. They'll describe large disruptive opportunities, and develop concrete plans for how to take advantage of them.\n * Communicate - A 1% PM can make a case that is impossible to refute or ignore. They'll use data appropriately, when available, but they'll also tap into other biases, beliefs, and triggers that can convince the powers that be to part with headcount, money, or other resources and then get out of the way.\n * Simplify - A 1% PM knows how to get 80% of the value out of any feature or project with 20% of the effort. They do so repeatedly, launching more and achieving compounding effects for the product or business. \n * Prioritize - A 1% PM knows how to sequence projects. They balance quick wins vs. platform investments appropriately. They balance offense and defense projects appropriately. Offense projects are ones that grow the business. Defense projects are ones that protect and remove drag on the business (operations, reducing technical debt, fixing bugs, etc.). \n * Forecast and measure - A 1% PM is able to forecast the approximate benefit of a project, and can do so efficiently by applying past experience and leveraging comparable benchmarks. They also measure benefit once projects are launched, and factor those learnings into their future prioritization and forecasts.\n * Execute - A 1% PM grinds it out. They do whatever is necessary to ship. They recognize no specific bounds to the scope of their role. As necessary, they recruit, they produce buttons, they do bizdev, they escalate, they tussle with internal counsel, they *.\n * Understand technical trade-offs - A 1% PM does not need to have a CS degree. They do need to be able to roughly understand the technical complexity of the features they put on the backlog, without any costing input from devs. They should partner with devs to make the right technical trade-offs (i.e. compromise).\n * Understand good design - A 1% PM doesn't have to be a designer, but they should appreciate great design and be able to distinguish it from good design. They should also be able to articulate the difference to their design counterparts, or at least articulate directions to pursue to go from good to great.\n * Write effective copy - A 1% PM should be able to write concise copy that gets the job done. They should understand that each additional word they write dilutes the value of the previous ones. They should spend time and energy trying to find the perfect words for key copy (button labels, nav, calls-to-action, etc.), not just words that will suffice.\n\nI'm not sure I've ever met a 1% PM, certainly not one that I identified as such prior to hiring. Instead of trying to hire one, you're better off trying to hire a 10% PM who strives to develop and improve along these dimensions.",
                "I don\u2019t think there is any such thing as a top 1% developer. I certainly do not consider myself anywhere close to that good. But I am a professional. I try to keep abreast with technological changes, I try to write \u201cgood\u201d code. Again, there is no such thing as \u201cgood\u201d code \u2014 all code has faults \u2014 but I strive to make my code readable and understandable. I use simple rather than complex constructs \u2014 I know that many programmers argue that efficiency is code \u201cshortness\u201d (i.e. terseness) is superior to readability. I respectfully disagree. I write comments. I explain both the overall function of my code as well as the individual things that the code does. Perhaps many programmers do not need comments and perhaps code is \u201cself-documenting\u201d, but I prefer to explain my code in plain English \u2014 if I cannot explain it in English something is wrong.\n\nA top 1% coder? Bill Joy? Bill Gates? Andy Herzfeld? Seymour Cray? Donald Knuth? Steve Wozniac? My guess is that these guys have IQs in the 200 range, so I doubt if anyone without super human ability could get to that level.",
                "my2cents, Adding to Lior:\n\n * C programming experience- yes, C is rare and just the thought of coding my product in C gives me the chill... but a great developer knows what's going on under the hood...You can't build or plan large scale projects without this solid knowledge. You should code in higher languages, but you should also know why the most efficient code you'll ever create will be in C.\n * Multidisciplinary - The more, the merrier... Knowing several languages, frameworks and environments is a must. You should pick one you like and master it for ~2 years and than learn another one. Being able to create any prototype (website, software, mobile app, server..) using your skills is pure gold. \n * Don't be an alien - read some tech news, tech blogs, anything you like.. don't be an alien and ask what is Instagram or Hadoop or GitHub You don't have to use it, just know what it is.\n\n * Some QA and testing experience -  \"If you created the problem, how can you be the one to fix it?\" - A great developer spends at least 3 hours before writing any code by planning and forecasting future problems (scalability, compatibility, security, future features etc) \n * Know the vision and the road map - Know what the company is going after in the long run, what's the business model, what's planned for the future, Localization and more languages? API for developers? Thousands of users? know what's going on and understand the product.\n\n * Some basic UX / UI skills\n * Be organized - Organize your builds, source code, resources, links etc.. so that even if you are sick, another developer can easily continue your work.\n * Never say 'No' or Can't '- Anything is possible, it's just a matter of tradeoffs and resources. So don't say 'No, I can't do that', instead, say 'I can do that but {explain the tradeoffs} or 'let me see what I can do'...\n\n * 'Self-managing' - manage yourself, your code and your tasks, don't be a burden on your managers / partners / co-workers.\n * Consider contributing to Open Source projects - just because it's an ocean of knowledge and opportunities. Start small, fix some bugs or contribute a small feature.\n",
                "Day 1: Intro. Read The Art of Computer Programming. [ https://en.wikipedia.org/wiki/The_Art_of_Computer_Programming ] Make sure to do all the exercises. (If you\u2019re slow or crunched for time just do volumes 1\u20133.)\n\nDay 2: C++. Read and memorize the C++14 standard [ http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4296.pdf ]. Make sure you fully understand lambdas, move semantics and templates.\n\nDay 3: System operator skills. Read all the Linux man pages [ http://man.he.net/ ]. Also, pick your editor. (My personal preference is Emacs [ https://www.gnu.org/software/emacs/ ].)\n\nNow that you\u2019ve got the basic knowledge it\u2019s time to gain some practical skills\u2026\n\nDay 4: System programmer skills. Implement an OS from scratch. Make sure it supports multiprocessing and some form of reliable transmission protocol as you\u2019ll need this for day 5. (A fully operational TCP/IP stack might be a bit much in a single day, but kudos if you manage it.)\n\nDay 5: Distributed systems/cloud computing. Implement a cloud framework with hardware provisioning, durable storage and a distributed database. Extra credit: metrics, billing, edge cache.\n\nDay 6: App/Web development. Build an app that runs natively on iOS, Android and a traditional web client. (It\u2019s probably fine to skip desktop clients as they\u2019re on their way out.)\n\nDay 7: Review. Congratulations! You\u2019ve learned a lot in the last week. Review your work, fill in any areas you feel are deficient, and start sending out your resume!"
            ]
        },
        {
            "tag": "deep_learning",
            "patterns": [
                "What are the common questions asked in an interview about deep learning?"
            ],
            "responses": [
                "I am mentioning some basic questions that I was asked during deep learning interview.\n\nNOTE: if you have done some project in deep learning then most of the questions will be related to it.\n\n1. What's Lambda Function?\n2. What's Dropout?\n3. How would you came to know that you have to use Dropout?\n4. What are the weights?\n5. What's activation function?\n6. What will happen if activation function will be removed?\n7. Explain PCA?\n8. What's CTC loss?\n9. In which scenerio we need to use CTC?\n10. Can RNN handle variable length sequence?\n11. What's RNN explain?\n12. What are different CNN building blocks?\n13. What will happen if pooling layers will be removed?\n14. What does convolution mean? Explain math behind it?\n15. Batch Normalization?\n16. Explain your one favorite classical algorithm?\n17. Explain Vanishing gradient descent problem and how it can be resolved?\nYour Probability and statistics concepts should be strong they will surely give you extra edge.\n\n"
            ]
        },
        {
            "tag": "python",
            "patterns": [
                "What is data science?",
                "What are some projects I can do to learn Python?",
                "What amazing things can Python do?",
                "What are some amazing things you can do with Python?",
                "What are some real-life projects that can be made with Python? To prevent confusion on what I mean by \u201creal-life projects\u201d, this is generally what I mean: Projects that can be used to do real tasks.",
                "What are some real-life projects that can be made with Python? To prevent confusion on what I mean by \u201creal-life projects\u201d, this is generally what I mean: Projects that can be used to do real tasks.",
                "How can I pursue both computer science and artificial intelligence in college?",
                "How can I learn data science given that I have zero knowledge of it?",
                "What is data science?",
                "What distinguishes a great software engineer from a good one?",
                "What are some projects I can do to learn Python?",
                "What amazing things can Python do?",
                "What are some amazing things you can do with Python?",
                "What are some real-life projects that can be made with Python? To prevent confusion on what I mean by \u201creal-life projects\u201d, this is generally what I mean: Projects that can be used to do real tasks.",
                "What are some real-life projects that can be made with Python? To prevent confusion on what I mean by \u201creal-life projects\u201d, this is generally what I mean: Projects that can be used to do real tasks.",
                "How can I make learning Python interesting and exciting?",
                "How can I make learning Python interesting and exciting?",
                "What are the best resources to start learning Python?",
                "How do I learn to code?",
                "Why do some people say Python is the easiest programming language?",
                "How do I learn Python?",
                "How do I go from a beginner python developer to intermediate then advanced?",
                "How do I learn Python?",
                "How do I learn Python in depth?",
                "Why do some people say Python is the easiest programming language?",
                "How do I learn Python?",
                "How do I learn Python?",
                "What are the best resources to start learning Python?",
                "Why do some people say Python is the easiest programming language?",
                "Is Python better than Java? Why or why not?",
                "How can I learn Python in 1 month?",
                "Why do some people say Python is the easiest programming language?",
                "What are the best tips for learning Python within one month?",
                "How do I learn Python Programming the most effective way?",
                "Can I learn Python in a month?",
                "How can I learn Python in 1 month?",
                "Can I learn Python in a month?",
                "What are the best tips for learning Python within one month?",
                "On average, how long does it take for a newbie to learn the general fundamentals and functions of Python if it\u2019s their first language?",
                "How can I learn Python in 1 month?",
                "What are the best tips for learning Python within one month?",
                "How do I learn Python?",
                "How can I learn python to crack an interview in 2 months?",
                "Can I learn Python in a month?",
                "Why do some people say Python is the easiest programming language?",
                "On average, how long does it take for a newbie to learn the general fundamentals and functions of Python if it\u2019s their first language?",
                "On average, how long does it take for a newbie to learn the general fundamentals and functions of Python if it\u2019s their first language?",
                "On average, how long does it take for a newbie to learn the general fundamentals and functions of Python if it\u2019s their first language?",
                "How long does it take to learn Python?",
                "How long did it take you to master Python?",
                "How long did it take you to master Python?",
                "How much time does it take to master python from scratch?",
                "How long did it take you to master Python?",
                "On average, how long does it take for a newbie to learn the general fundamentals and functions of Python if it\u2019s their first language?",
                "How long would it take to master Python?",
                "How long would it take to master Python?"
            ],
            "responses": [
                "\n%3E Data science is searching for a diamond in a coal mine. Yes, it is as straightforward as that. Diamond is the result, and coal mine is the collected data.\nDefinitions, flow charts and articles about \u201cwhat is data science\u201d sometimes cause confusion. It is okay, and there are many aspects of data science. If a person learns every aspect of data science, they will likely forget many aspects within a week. The data science field is very vast, and you need skills and experience to be called a proficient data scientist. The experience one requires to be domain-oriented.\n\nWe are glad to live in a world full of information. You can have the freedom to learn whatever you want.. However, in data science, you must know the reason for choosing it as a career. When your reason is strong enough, and you face any setbacks or challenges, it will keep you on track. Another important task before stepping into data science is knowing which domain you want to grow your expertise in.\n\nI will help you learn everything about data science. What direction should you take in your professional life? What will your responsibilities be as a data scientist? What is the most effective data science training programme?\n\nIt doesn't matter how you learn data science; doing projects is the most important factor. Projects will give you a practical understanding of data science. Many beginners and experts compete on kaggle performing projects, which is fun. To make your profile stand out, a project experience is more essential than a top university certificate.\n\nThe roadmap to getting started as a data scientist are:\n\n1. Knowing the reason for choosing data science\n2. Choosing domain of interest\n3. Learning the basics of must-have skills\n4. Implementing gained skills on real projects\n5. Getting relevant experience in the main domain\n6. Earning a recognized certificate\n7. Preparing for the interview and scoring a job.\nThese are ideal steps every data scientist has followed. As the experience increases, your expertise will grow. You can also switch to different domains of interest.\n\nDay to day task you would be performing in data science:\n\nThere are four types of tasks you would be performing such as collecting and cleaning the data, insight generation, performing ML algorithms, and communicating with managers or clients.\n\nCollecting and cleaning the data: The data collection and cleansing seems easy, but data scientists have to spend days on this process. In a project, when you perform, you get created and clean data, but in real life, working as a data scientist, you need to collect and clean the data independently.\n\nInsight generation: This step defines the quality of a data scientist. Deriving the useful insight from cleaned data makes a data scientist creative and gameplay of the strategies. In this step, mathematical, statistical and visualization come into use.\n\nML algorithm: In this, a data scientist feeds the computer insightful collected data. The methods used in ML algorithms are recommender system, supervised and unsupervised learning, natural language processing (NLP), computer vision problems.\n\nCommunication: This is the process of expression; a data scientist explains the created strategies to managers or clients. Explaining complex data in a simple way, a data scientist needs in-depth understanding. Of course, these things get improved over time, but you should also be improving your communication skills.\n\nWhich is the best institute for data science courses:\n\nIn terms of certification, ease of learning and exposure, I would say Learnbay is currently offering the best courses on data science.\n\nLet's check out the feature offered by Learnbay in its courses:\n\nOnline and offline type in courses:\n\n * There is a chain of bases to attend your classes offline while still performing real-time projects.\n * Their centres are located in Hyderabad, Pune, Mumbai, Delhi, Chennai, and Bangalore.\n * So, depending on your learning preferences, you may attend live interactive sessions from home or join your classmates in offline classrooms.\nLearn in the choice of your domain:\n\n * Learnbay offers courses in which you may first pick your domain and then change it at any moment.\n * They provide courses in the following industries: E-commerce, Telecom, Finance, Insurance, Banking, Services, Healthcare, Manufacturing, Supply Chain, Retail, Oil Gas, Energy, media, HR, Transportation, Sales, Marketing.\n * The best thing is that you may consult with professionals to determine which domain is the best fit for you before enrolling in their course.\nProjects and costing:\n\n * The cost is between \u20b950,000 and \u20b980,000, which is reasonable when compared to other universities.\n * You will be trained for 300 hours or more and will have the opportunity to work on live projects and capstone projects.\n * Learnbay's courses are best suited for working professionals, with special attention given to newcomers learning from the ground up.\nTraining session for interview:\n\n * The experts will help you build your CV and include essential information to make it more appealing.\n * You'll learn how to present yourself during a mock interview as well.\n * Finally, you will be guided to worldwide businesses that can help you achieve your career goal of becoming a data scientist, following comprehensive preparation.\nAdvantages of subscription:\n\n * The subscription will be for a duration of two years.\n * Therefore, switch between the live trainers easily.\n * In weekday and weekend sessions, you can attend lectures in several batches and change batches at any time.\n * The recordings will be available to you for the rest of your life.\n * Interact with professors in real-time courses and participate in after-class one-on-one question sessions with a 24-hour support system.\nGet IBM certified:\n\n * Learnbay designed its courses in collaboration with IBM experts\n * This certification will allow you to pursue a profession in any sector around the world.\n * Faculty members come from companies including Facebook, Amazon, Apple, Netflix, and Google.\nNon-programmers receive special assistance:\n\n * A group of people will teach a beginner or teaching someone with a non-technical background how to programme in Python and other programming languages\n * Actual Business Cases from various sectors to assist you in gaining a better understanding of how the business industry operates.\nThe courses you take will save your time and give you experience in your domain. Though you can also learn either way from books and free courses, free stuff is made for everyone, not particular persons. Therefore, it's better to enrol in an online course that fulfils your requirements and get you hired in your desired domain.\n\nThanks a lot for reading!",
                "Instead of just blindly doing some sample projects others provided for you. I strongly suggest that you pause for a sec and ask yourself this question \"If I were to start my own startup today, what product should I build.\", and start building something that you love, something that you need.\nRemember, Woz built the microcomputers because he wanted one for himself; Matz created Ruby because he needs a programming language that satisfied his needs.\nStudy other people's Python source codes at Github and learn about how they write codes, how they structure their projects.\nHave a good Python reference handy (there're many good ones online, say, Python online documentation)\nIf you get stuck, which undoubtedly you will, Google it, If you can't find the answer using search engine, post your question at Stack Overflow or Quora, these two sites are invaluable for getting help, it's like having your own programming mentor and getting technical help for FREE.",
                "Python can be used to so much of serious stuff when you get to know it better. It is good at fun stuff too! Recently I did this using a module in python called \"Turtle Graphics\". I drew a tree using Fractals! \n\n[code]import turtle\nimport random \nimport math \nturtle.screensize(900,900)\nc = (0.1,0.01,0.3)\nturtle.bgcolor(c)\na = turtle.Turtle (\"classic\",1000,False) \nb = turtle.Turtle (\"classic\",1000,False) \nb.up()\nb.goto(-180,200)\nb.write(\"The Power of Recursion!!\\n FRACTALS --%3E Order in Chaos!!\",False,\"center\",(\"Arial\",20,\"normal\"))\nb.goto(180,180)\nb.write(\"\\n\\n\")\nb.write(\"\\n\\nJust run the program,\\n come and watch after\\n 15 mins!!\",False,\"center\",(\"Arial\",20,\"normal\"))\na.left(90)\na.up()\na.goto(0,-200)\na.down()\na.speed(0)\ndef fractal (a,length ,size,angle,t) : \n    if (size%3C2):\n        tup = (1-t,1-t,1-t)\n        tup2= (1-t,1-t,1-t)\n        k = random.randint(1,100)\n        if (k %3C 45):\n            a.pencolor(tup2)\n        elif(k %3E 45 & k %3C= 75) :\n            a.pencolor(tup2)\n        else : \n            a.pencolor(tup)\n    \n        a.pensize(1.5)\n        a.fd(5)\n        return\n    \n    a.speed(0)\n    tup2 = (1-t,t,1-t)\n    a.begin_fill()\n    k = random.randint(1,100)\n    if (k %3C 70):\n        a.pencolor(tup2)\n    else : \n        a.pencolor(tup2)\n    a.pensize (size) \n    a.fd(length)\n    a.end_fill()\n    flag = False\n    aLeft = turtle.Turtle('classic',1000,False)\n    aLeft.speed(0)\n    \n    aLeft.up()\n    aRight= turtle.Turtle('classic',1000,False)\n    aRight.speed(0)\n    aRight.up() \n    aLeft.goto(a.xcor(),a.ycor())\n    aRight.goto(a.xcor(),a.ycor())\n    aLeft.down()\n    aRight.down() \n    aLeft.left(angle+45)\n    aRight.left(angle-45)\n   \n    \n    fractal(aLeft,length*0.75,size*0.75,angle+45,t+0.05)\n    fractal(aRight,length*0.75,size*0.75,angle-45,t+0.05)\nfractal(a,100,40,90,0.1) \nb.clear()   \ninput()\n[/code]And the result was so beautiful !!\n\nPython is amazingly cool if only we learn to use it well!\n\n\nUpdate : 10216. \n\nLast night, I lost interest in everything and was looking for some new ways to kill time.  And as I was thinking, something struck me like a thunder striking a lonely tree in a desert! I played the Snake game a lot as a kid and ever since I took Computer Science I wanted to write this game. I knew that the logistics were simple but every time I start something used to get in way. But last night, that didn't happen. So, I sat straight from 9PM to 5:10 AM in the morning and completed writing the code. It was one of the yureka moments of my life. I used a specialized module in python called Pygame. \n\n+1 for Pygame. And I am looking forward to write more of my childhood fantasy games. \n\nHere is the link to the code if someone wants to give it a try. \n\nrajiv256/Pygames [ https://github.com/rajiv256/Pygames/tree/master/Snake ] \n\nI did it in Linux and I just had to install pygame and run the .py file as a regular file on the command line. \n\nTo install Pygame from command line :\n\n\n\n%3E sudo apt-get install python-pygame\nPeace. :) ",
                "There is a non-exhaustive list of amazing things that you can do with python there is so much you can do.\n\nsome most amazing things include...\n\nCreating  a file server with just \n\n[code]python -m SimpleHTTPServer #default port 8080\n[/code]this line creates  file server, now any device that is on the network can access files from your P.C. I do it all the times when i have to shift some movies to my mobile ,i can browse my whole computer from my mobile. \n\nHere are some screenshots\n\nThis is my PC\n\nand here's my phone,full access to every folder\n\n\nthis is amazing..\n\nSome other things that i've managed to do keeping file in Well Organised manner. I am really very bad at keeping files in their correct folder, after a week or two there is   chaos but what i managed to do was create a script that can take files and keep them in their respective folders like all images in one folder,python files in another, office files and so on with just one click. some screeshots...\n\nBefore\n\nAfter file_organiser.py(you can see it ) was clicked\n\nNow, automating daily life things are other field where python performs better and can surely be amazing. like i am currently working on this one , reading all the movie   files names( i have an movies folder) and downloading all information about it from imdb which include story line, cast, reviews and rating and saving them alongside movies. you can do this all with python. \n\nCricket score notification is yet another amazing thing for which you can write a script and i've also did so. it just pops up the score after a period of time. libraries like beautifulsoup are such a great help in these cases.(screenshots are not available because that script is in ubuntu (i've dual booted the system) i will edit the answer later).\n\n\nWant to read zen of python: just do this\n\n[code]%3E%3E%3Eimport this\nBeautiful is better than ugly.\nExplicit is better than implicit. \nSimple is better than complex. \nComplex is better than complicated. \nFlat is better than nested. \nSparse is better than dense. \nReadability counts. \nSpecial cases aren't special enough to break the rules. \nAlthough practicality beats purity. \nErrors should never pass silently.\nUnless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. \nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch. \nNow is better than never. \nAlthough never is often better than *right* now. \nIf the implementation is hard to explain, it's a bad idea. \nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those! \n[/code]#Want to know about love\n\n[code]%3E%3E%3E love = this\n%3E%3E%3E this is love \nTrue \n%3E%3E%3E love is True \nFalse \n%3E%3E%3E love is False \nFalse \n%3E%3E%3E love is not True or False \nTrue \n%3E%3E%3E love is not True or False; love is love \nTrue \nTrue\n[/code]",
                "One day I was driving back from work and I wanted to know which movies where playing at the theater near my house. So, I did the search on my phone and it took me about 5 to 7 minutes to find the movies that were playing on that day. I hate searching my phone while driving because now I have to \u201cMulti Task\u201d and worry about not wrecking the car. So, I thought why not write a program that will send me one text message everyday with details of the movies (name, date playing, show timings ). So, I wrote first part of the program that goes to the website and parse the information that I want and store it in Json format. Now all I have to do is use Twilio API and have it send to my phone and add a cron job.\n\nHere the code, just in case you are interested. You will need to download PhantomJs and put it in the same directory as this file.\n\nThis is just a small example and what you can do is limited to your imagination:)\n\n[code]import json\nfrom operator import itemgetter\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\n\ndata = {}\n\nbrowser = webdriver.PhantomJS(\"./phantomjs.exe\")\nbrowser.get('http://www.galaxytheatres.com/Browsing/Cinemas/Details/3')\n\n# Parse html\nsoup = BeautifulSoup(browser.page_source, \"html.parser\")\ngData = soup.find_all('div', {'class': 'film-showtimes'})\n\n# Total number of movies\nmovieTotal = len([movie.text for movie in soup.find_all('h3', {'class': 'film-title'})])\n\nfor i in range(movieTotal):\n    # Get movie name\n    movieTitle = gData[i].find('h3', {'class': 'film-title'}).text\n\n    # Get date when movie is showing\n    movieDate = gData[i].find('h4', {'class': 'session-date'}).text\n    movieDate = movieDate.split(',')[1].strip()\n\n    # Get movie Timings\n    timing = list(gData[i].find('div', {'class': 'session-times'}))\n    timing = [element for element in timing if element != '\\n']\n    movieTiming = [time.find('time').text for time in timing]\n\n    # Storing everything in json format\n    data.update({i: {'name': movieTitle, 'date': movieDate, 'timing': movieTiming}})\n\n# Sorting dictionary by date\nsortedData = sorted(data.values(), key=itemgetter('date'))\n\n# Converting [ {}, {} ] to { {}, {} }\njsonData = {i:item for i, item in enumerate(sortedData)}\n\n# Writing dictionary to .json file\nwith open('movie.json', 'w') as f:\n    json.dump(jsonData, f, indent=2)\n[/code]",
                "I made and uses :\n\n1. A script that go into my download folder every hours and tidy up everything, order by resolution, by names, removes known bad files, etc (first script I did, I was a student back then, ugliest code I\u2019ve ever written, but still works :p)\n2. A script that scan a whole folder and subfolders and will tell you exactly what modifications where made later\n3. a script that retieve from my 3d scanner 21 pictures, order them, if one of the camera failed it uses the timestamp meta data to know in what scan the pictures is suposed to be, make photo preview, etc\n4. A bot that bypass captcha using a OCR librairy and request a lot from bitcoin faucet :p, I gave up however, too high maintenance for too little gain.\n5. A funny name generator : 40 names, huhdreds of adjectives, you get suff like combat-butterfly, stealth-cannibal, chemical-octopus, I use it everytime I need a name somewhere, around 21000 unique names\nRight now I\u2019m looking at little internet 2d games that allow you to earn money, if they are just based on agility I could make a python bot that read the screen and play the game\u2026..then get money.\n\nPS : oh I forgot, theere is a high traffic website that allow people to sell their stuff to other people; I made a script that search the website every 5 minutes for objects I wants and email me as soon as it find something at interesting prices.\n\n(Some times you can find high end DSLR 10 times cheaper than their market value, those are usually sold in the first 30 minutes)\n\nThe said website has this as a service but you have to pay 15\u20ac/month.\n\nWhy would I pay, when I can code this in 4 hours?",
                "Well , it isn\u2019t seems like a problem till now to me . If you have a crystal clear mindset for AI , start learning any language , preferably start with C then jump to C++ or Python . For sure Python is more used and preferred for AI over C++ but data structures can be done efficiently with C++ ( my own experience ) .\n\nAlso , don\u2019t allow your most enthusiastic mindset which one carries after listening to VEVO dj songs by Justin or Selena :p . Cool down . It needs practise , you cannot skip anything in between but thorough in depth learning everything else ( talking here of the other non core subjects other than Maths ) will be foolishness . Also , they can be done by proper time management techniques and \u201cyou will have good GPA to show and shine before Gupta JI ka ladka :p.\u201d\n\nSo after three languages , make some projects over them , Side wise start learning AI from edx or coursera . EDX is Great though .\n\nFind a way to start with , I prefer Natural Language Processing , Vision , Robotics , Locomation , Predictive Analysis etc. They are exciting to get into with . Vision I prefer as a large number of projects can be made with openCV algo only and can be implemented in short run . Go theoretical too . Most of the So called world Class developers only make projects and always be ignorant to theory part . Trust me , there is one in a million who crack it without touching any book .\n\nSo , C , C++ , Dynamic Programming , Paradigms of Programming ( OOP , Procedural , functional , Imperative ) , Algorithm design and Analysis , Python are mandatory prerequisites .\n\nAlso , AI is something taught in the last stages of B.tech , if you aren\u2019t comfortable with it and still finds time in your daily routine , learn\n\nLINUX , SEO , Microprocessor , Crypto Currencies , UI/UX etc.\n\nPS: you can start AI at parallel online but never ever ruin something which is being taught for something which is going to be or something which already has been.\n\nBest of LUCK :)",
                "As other answers have already mentioned, there are a variety of things one must learn to become a proficient data scientist - programming languages such as R or Python, statistics, linear algebra, databases, the list goes on.\n\nInstead of rehashing those answers, I\u2019d like to focus on the life cycle of a data scientist, and the steps you\u2019ll need to master in order to excel in your job.\n\nData Gathering\n\nUnless you\u2019re at a company with great data governance you\u2019re likely going to have some trouble accessing the data you want. Whether thats because your company has neglected to put the necessary systems in place to gather data, or the data that they are collecting is fragmented and scattered across the organization, you\u2019ll have to first spend some time gathering whatever data you\u2019ll need to do your job. That means having discussions with relevant stakeholders, and getting the necessary credentials to access databases within your organization.\n\nData Preparation\n\nOnce you have access to data, you\u2019ll need to spend some time cleaning and formatting it. This is where Data Science can often become more of an art, then a science. Unlike datasets you\u2019ll find in competitions, the real world has very messy data sets. Missing values, error in data collection, data formatting, normalization, outliers - these are all issues that you\u2019ll have to learn to deal with.\n\nExploration\n\nBefore diving into building any models, you\u2019ll want to explore the data to try to glean some insights. Clustering algorithms, scatterplots, bar graphs, Chernoff faces are all interesting ways of visualizing data that will lead to a better understanding of the structure of your data and aid you in your model building step.\n\nModel Building\n\nWith your data cleaned and formatted, you\u2019ll have an opportunity to explore a variety of models to see which one works best. Random Forests, SVM\u2019s, Bayesian Predictors Neural Networks, Deep Learning, K-Nearest Neighbours - all models you should familiarize yourself with. There is no one model fits all, and so you again will need to develop intuition on which model suits your particular problem.\n\nModel Validation\n\nPrediction accuracy is a common benchmark for whether your model is performing well, however often times there are other things to consider. False positives and false negatives are important to think about from the perspective of the problem you\u2019re working on. If you\u2019re predicting disease, you\u2019ll care more about minimizing false negative, since it may result in a persons death - whereas a false positive will only lead to additional testing.\n\nModel Deployment\n\nFinally you\u2019ll deploy your model into the wild, as you gather more data and feedback on how its doing you\u2019ll be able to tweak and improve it as time goes on.\n\nThis is by no means a comprehensive list of steps, and there are certainly other things you\u2019ll need to do to be able to do well in your job - however this is a good high level overview of the steps involved in tackling data science problems.",
                "\n%3E Data science is searching for a diamond in a coal mine. Yes, it is as straightforward as that. Diamond is the result, and coal mine is the collected data.\nDefinitions, flow charts and articles about \u201cwhat is data science\u201d sometimes cause confusion. It is okay, and there are many aspects of data science. If a person learns every aspect of data science, they will likely forget many aspects within a week. The data science field is very vast, and you need skills and experience to be called a proficient data scientist. The experience one requires to be domain-oriented.\n\nWe are glad to live in a world full of information. You can have the freedom to learn whatever you want.. However, in data science, you must know the reason for choosing it as a career. When your reason is strong enough, and you face any setbacks or challenges, it will keep you on track. Another important task before stepping into data science is knowing which domain you want to grow your expertise in.\n\nI will help you learn everything about data science. What direction should you take in your professional life? What will your responsibilities be as a data scientist? What is the most effective data science training programme?\n\nIt doesn't matter how you learn data science; doing projects is the most important factor. Projects will give you a practical understanding of data science. Many beginners and experts compete on kaggle performing projects, which is fun. To make your profile stand out, a project experience is more essential than a top university certificate.\n\nThe roadmap to getting started as a data scientist are:\n\n1. Knowing the reason for choosing data science\n2. Choosing domain of interest\n3. Learning the basics of must-have skills\n4. Implementing gained skills on real projects\n5. Getting relevant experience in the main domain\n6. Earning a recognized certificate\n7. Preparing for the interview and scoring a job.\nThese are ideal steps every data scientist has followed. As the experience increases, your expertise will grow. You can also switch to different domains of interest.\n\nDay to day task you would be performing in data science:\n\nThere are four types of tasks you would be performing such as collecting and cleaning the data, insight generation, performing ML algorithms, and communicating with managers or clients.\n\nCollecting and cleaning the data: The data collection and cleansing seems easy, but data scientists have to spend days on this process. In a project, when you perform, you get created and clean data, but in real life, working as a data scientist, you need to collect and clean the data independently.\n\nInsight generation: This step defines the quality of a data scientist. Deriving the useful insight from cleaned data makes a data scientist creative and gameplay of the strategies. In this step, mathematical, statistical and visualization come into use.\n\nML algorithm: In this, a data scientist feeds the computer insightful collected data. The methods used in ML algorithms are recommender system, supervised and unsupervised learning, natural language processing (NLP), computer vision problems.\n\nCommunication: This is the process of expression; a data scientist explains the created strategies to managers or clients. Explaining complex data in a simple way, a data scientist needs in-depth understanding. Of course, these things get improved over time, but you should also be improving your communication skills.\n\nWhich is the best institute for data science courses:\n\nIn terms of certification, ease of learning and exposure, I would say Learnbay is currently offering the best courses on data science.\n\nLet's check out the feature offered by Learnbay in its courses:\n\nOnline and offline type in courses:\n\n * There is a chain of bases to attend your classes offline while still performing real-time projects.\n * Their centres are located in Hyderabad, Pune, Mumbai, Delhi, Chennai, and Bangalore.\n * So, depending on your learning preferences, you may attend live interactive sessions from home or join your classmates in offline classrooms.\nLearn in the choice of your domain:\n\n * Learnbay offers courses in which you may first pick your domain and then change it at any moment.\n * They provide courses in the following industries: E-commerce, Telecom, Finance, Insurance, Banking, Services, Healthcare, Manufacturing, Supply Chain, Retail, Oil Gas, Energy, media, HR, Transportation, Sales, Marketing.\n * The best thing is that you may consult with professionals to determine which domain is the best fit for you before enrolling in their course.\nProjects and costing:\n\n * The cost is between \u20b950,000 and \u20b980,000, which is reasonable when compared to other universities.\n * You will be trained for 300 hours or more and will have the opportunity to work on live projects and capstone projects.\n * Learnbay's courses are best suited for working professionals, with special attention given to newcomers learning from the ground up.\nTraining session for interview:\n\n * The experts will help you build your CV and include essential information to make it more appealing.\n * You'll learn how to present yourself during a mock interview as well.\n * Finally, you will be guided to worldwide businesses that can help you achieve your career goal of becoming a data scientist, following comprehensive preparation.\nAdvantages of subscription:\n\n * The subscription will be for a duration of two years.\n * Therefore, switch between the live trainers easily.\n * In weekday and weekend sessions, you can attend lectures in several batches and change batches at any time.\n * The recordings will be available to you for the rest of your life.\n * Interact with professors in real-time courses and participate in after-class one-on-one question sessions with a 24-hour support system.\nGet IBM certified:\n\n * Learnbay designed its courses in collaboration with IBM experts\n * This certification will allow you to pursue a profession in any sector around the world.\n * Faculty members come from companies including Facebook, Amazon, Apple, Netflix, and Google.\nNon-programmers receive special assistance:\n\n * A group of people will teach a beginner or teaching someone with a non-technical background how to programme in Python and other programming languages\n * Actual Business Cases from various sectors to assist you in gaining a better understanding of how the business industry operates.\nThe courses you take will save your time and give you experience in your domain. Though you can also learn either way from books and free courses, free stuff is made for everyone, not particular persons. Therefore, it's better to enrol in an online course that fulfils your requirements and get you hired in your desired domain.\n\nThanks a lot for reading!",
                "Joe is a GREAT Software Engineer. He continues to be better than many other GOOD Software Engineers because :\n\n1. He sticks to his programming fundamentals. He still can reverse a queue or implement a bubble sort. He takes advantage of appropriate search sort algorithms and data structures wherever needed.\n2. He is not language or technology bound. He meets a lot of engineers who are Python, Java, PHP developers and yet keeps himself open to exploring any old or new technology. He simply considers himself as a Software Engineer.\n3. He knows what open source software are. He loves sharing his knowledge. If he learns a new thing he puts it up online so that many average Joe\u2019s like me benefit from it.\n4. Debugger and him are BFF\u2019s. Bugs are like his high school sweetheart; no dirty tricks or shortcuts taken. He strives to keep the relationship healthy.\n5. He is open to criticism. He know he is not perfect and for sure that his code isn't. He does not jump to a conclusion, start writing code from scratch or fixes bugs without considering all known scenarios.\n6. He looks at the world as a Model. Every other thing to him is an object. He understands each of it has its own characteristics and properties. No wonder his code reflects it too.\n7. Most importantly Joe has life outside work. He balances both worlds. His family and friends are happy to spend quality time with him. This peace of mind reflects in his work.\n",
                "Instead of just blindly doing some sample projects others provided for you. I strongly suggest that you pause for a sec and ask yourself this question \"If I were to start my own startup today, what product should I build.\", and start building something that you love, something that you need.\nRemember, Woz built the microcomputers because he wanted one for himself; Matz created Ruby because he needs a programming language that satisfied his needs.\nStudy other people's Python source codes at Github and learn about how they write codes, how they structure their projects.\nHave a good Python reference handy (there're many good ones online, say, Python online documentation)\nIf you get stuck, which undoubtedly you will, Google it, If you can't find the answer using search engine, post your question at Stack Overflow or Quora, these two sites are invaluable for getting help, it's like having your own programming mentor and getting technical help for FREE.",
                "Python can be used to so much of serious stuff when you get to know it better. It is good at fun stuff too! Recently I did this using a module in python called \"Turtle Graphics\". I drew a tree using Fractals! \n\n[code]import turtle\nimport random \nimport math \nturtle.screensize(900,900)\nc = (0.1,0.01,0.3)\nturtle.bgcolor(c)\na = turtle.Turtle (\"classic\",1000,False) \nb = turtle.Turtle (\"classic\",1000,False) \nb.up()\nb.goto(-180,200)\nb.write(\"The Power of Recursion!!\\n FRACTALS --%3E Order in Chaos!!\",False,\"center\",(\"Arial\",20,\"normal\"))\nb.goto(180,180)\nb.write(\"\\n\\n\")\nb.write(\"\\n\\nJust run the program,\\n come and watch after\\n 15 mins!!\",False,\"center\",(\"Arial\",20,\"normal\"))\na.left(90)\na.up()\na.goto(0,-200)\na.down()\na.speed(0)\ndef fractal (a,length ,size,angle,t) : \n    if (size%3C2):\n        tup = (1-t,1-t,1-t)\n        tup2= (1-t,1-t,1-t)\n        k = random.randint(1,100)\n        if (k %3C 45):\n            a.pencolor(tup2)\n        elif(k %3E 45 & k %3C= 75) :\n            a.pencolor(tup2)\n        else : \n            a.pencolor(tup)\n    \n        a.pensize(1.5)\n        a.fd(5)\n        return\n    \n    a.speed(0)\n    tup2 = (1-t,t,1-t)\n    a.begin_fill()\n    k = random.randint(1,100)\n    if (k %3C 70):\n        a.pencolor(tup2)\n    else : \n        a.pencolor(tup2)\n    a.pensize (size) \n    a.fd(length)\n    a.end_fill()\n    flag = False\n    aLeft = turtle.Turtle('classic',1000,False)\n    aLeft.speed(0)\n    \n    aLeft.up()\n    aRight= turtle.Turtle('classic',1000,False)\n    aRight.speed(0)\n    aRight.up() \n    aLeft.goto(a.xcor(),a.ycor())\n    aRight.goto(a.xcor(),a.ycor())\n    aLeft.down()\n    aRight.down() \n    aLeft.left(angle+45)\n    aRight.left(angle-45)\n   \n    \n    fractal(aLeft,length*0.75,size*0.75,angle+45,t+0.05)\n    fractal(aRight,length*0.75,size*0.75,angle-45,t+0.05)\nfractal(a,100,40,90,0.1) \nb.clear()   \ninput()\n[/code]And the result was so beautiful !!\n\nPython is amazingly cool if only we learn to use it well!\n\n\nUpdate : 10216. \n\nLast night, I lost interest in everything and was looking for some new ways to kill time.  And as I was thinking, something struck me like a thunder striking a lonely tree in a desert! I played the Snake game a lot as a kid and ever since I took Computer Science I wanted to write this game. I knew that the logistics were simple but every time I start something used to get in way. But last night, that didn't happen. So, I sat straight from 9PM to 5:10 AM in the morning and completed writing the code. It was one of the yureka moments of my life. I used a specialized module in python called Pygame. \n\n+1 for Pygame. And I am looking forward to write more of my childhood fantasy games. \n\nHere is the link to the code if someone wants to give it a try. \n\nrajiv256/Pygames [ https://github.com/rajiv256/Pygames/tree/master/Snake ] \n\nI did it in Linux and I just had to install pygame and run the .py file as a regular file on the command line. \n\nTo install Pygame from command line :\n\n\n\n%3E sudo apt-get install python-pygame\nPeace. :) ",
                "There is a non-exhaustive list of amazing things that you can do with python there is so much you can do.\n\nsome most amazing things include...\n\nCreating  a file server with just \n\n[code]python -m SimpleHTTPServer #default port 8080\n[/code]this line creates  file server, now any device that is on the network can access files from your P.C. I do it all the times when i have to shift some movies to my mobile ,i can browse my whole computer from my mobile. \n\nHere are some screenshots\n\nThis is my PC\n\nand here's my phone,full access to every folder\n\n\nthis is amazing..\n\nSome other things that i've managed to do keeping file in Well Organised manner. I am really very bad at keeping files in their correct folder, after a week or two there is   chaos but what i managed to do was create a script that can take files and keep them in their respective folders like all images in one folder,python files in another, office files and so on with just one click. some screeshots...\n\nBefore\n\nAfter file_organiser.py(you can see it ) was clicked\n\nNow, automating daily life things are other field where python performs better and can surely be amazing. like i am currently working on this one , reading all the movie   files names( i have an movies folder) and downloading all information about it from imdb which include story line, cast, reviews and rating and saving them alongside movies. you can do this all with python. \n\nCricket score notification is yet another amazing thing for which you can write a script and i've also did so. it just pops up the score after a period of time. libraries like beautifulsoup are such a great help in these cases.(screenshots are not available because that script is in ubuntu (i've dual booted the system) i will edit the answer later).\n\n\nWant to read zen of python: just do this\n\n[code]%3E%3E%3Eimport this\nBeautiful is better than ugly.\nExplicit is better than implicit. \nSimple is better than complex. \nComplex is better than complicated. \nFlat is better than nested. \nSparse is better than dense. \nReadability counts. \nSpecial cases aren't special enough to break the rules. \nAlthough practicality beats purity. \nErrors should never pass silently.\nUnless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. \nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch. \nNow is better than never. \nAlthough never is often better than *right* now. \nIf the implementation is hard to explain, it's a bad idea. \nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those! \n[/code]#Want to know about love\n\n[code]%3E%3E%3E love = this\n%3E%3E%3E this is love \nTrue \n%3E%3E%3E love is True \nFalse \n%3E%3E%3E love is False \nFalse \n%3E%3E%3E love is not True or False \nTrue \n%3E%3E%3E love is not True or False; love is love \nTrue \nTrue\n[/code]",
                "One day I was driving back from work and I wanted to know which movies where playing at the theater near my house. So, I did the search on my phone and it took me about 5 to 7 minutes to find the movies that were playing on that day. I hate searching my phone while driving because now I have to \u201cMulti Task\u201d and worry about not wrecking the car. So, I thought why not write a program that will send me one text message everyday with details of the movies (name, date playing, show timings ). So, I wrote first part of the program that goes to the website and parse the information that I want and store it in Json format. Now all I have to do is use Twilio API and have it send to my phone and add a cron job.\n\nHere the code, just in case you are interested. You will need to download PhantomJs and put it in the same directory as this file.\n\nThis is just a small example and what you can do is limited to your imagination:)\n\n[code]import json\nfrom operator import itemgetter\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\n\ndata = {}\n\nbrowser = webdriver.PhantomJS(\"./phantomjs.exe\")\nbrowser.get('http://www.galaxytheatres.com/Browsing/Cinemas/Details/3')\n\n# Parse html\nsoup = BeautifulSoup(browser.page_source, \"html.parser\")\ngData = soup.find_all('div', {'class': 'film-showtimes'})\n\n# Total number of movies\nmovieTotal = len([movie.text for movie in soup.find_all('h3', {'class': 'film-title'})])\n\nfor i in range(movieTotal):\n    # Get movie name\n    movieTitle = gData[i].find('h3', {'class': 'film-title'}).text\n\n    # Get date when movie is showing\n    movieDate = gData[i].find('h4', {'class': 'session-date'}).text\n    movieDate = movieDate.split(',')[1].strip()\n\n    # Get movie Timings\n    timing = list(gData[i].find('div', {'class': 'session-times'}))\n    timing = [element for element in timing if element != '\\n']\n    movieTiming = [time.find('time').text for time in timing]\n\n    # Storing everything in json format\n    data.update({i: {'name': movieTitle, 'date': movieDate, 'timing': movieTiming}})\n\n# Sorting dictionary by date\nsortedData = sorted(data.values(), key=itemgetter('date'))\n\n# Converting [ {}, {} ] to { {}, {} }\njsonData = {i:item for i, item in enumerate(sortedData)}\n\n# Writing dictionary to .json file\nwith open('movie.json', 'w') as f:\n    json.dump(jsonData, f, indent=2)\n[/code]",
                "I made and uses :\n\n1. A script that go into my download folder every hours and tidy up everything, order by resolution, by names, removes known bad files, etc (first script I did, I was a student back then, ugliest code I\u2019ve ever written, but still works :p)\n2. A script that scan a whole folder and subfolders and will tell you exactly what modifications where made later\n3. a script that retieve from my 3d scanner 21 pictures, order them, if one of the camera failed it uses the timestamp meta data to know in what scan the pictures is suposed to be, make photo preview, etc\n4. A bot that bypass captcha using a OCR librairy and request a lot from bitcoin faucet :p, I gave up however, too high maintenance for too little gain.\n5. A funny name generator : 40 names, huhdreds of adjectives, you get suff like combat-butterfly, stealth-cannibal, chemical-octopus, I use it everytime I need a name somewhere, around 21000 unique names\nRight now I\u2019m looking at little internet 2d games that allow you to earn money, if they are just based on agility I could make a python bot that read the screen and play the game\u2026..then get money.\n\nPS : oh I forgot, theere is a high traffic website that allow people to sell their stuff to other people; I made a script that search the website every 5 minutes for objects I wants and email me as soon as it find something at interesting prices.\n\n(Some times you can find high end DSLR 10 times cheaper than their market value, those are usually sold in the first 30 minutes)\n\nThe said website has this as a service but you have to pay 15\u20ac/month.\n\nWhy would I pay, when I can code this in 4 hours?",
                "I would definitely suggest you to learn the following Coursera courses provided by the Rice University. They teach you those basics and advanced tools with a nice programming environment developed by themselves. You will build a little game each week and eventually build the asteroid game that you can play. Sounds cool, right :-)\nAn Introduction to Interactive Programming in Python (Part 1) - Rice University | Coursera [ https://www.coursera.org/learn/interactive-python-1 ]\n\nAn Introduction to Interactive Programming in Python (Part 2) - Rice University | Coursera [ https://www.coursera.org/learn/interactive-python-2 ]",
                "I think you can use Python to write some useful programs to solve your daily problem, I start to learn Python because I want to play with Raspberry Pi  embedded board, it is an embedded device, you can write a short program to light a LED, so it's so exciting.\nThen you can write some useful scripts on PC, such as a little download script.\nWhen you get familiar with Python, you may want to study some useful web frame, such as Django, and Tornado, then you can build a website youself.",
                "I have been teaching myself programming and Python. Some resources I found helpful:\n\n1. Learn Python The Hard Way [ http://learnpythonthehardway.org/ ] is usually recommended enthusiastically, and I thought it was good. It very much puts the onus on you to investigate and be creative. This can seem daunting at first; I started with LPTHW, quit and did Codeacademy for a while (mostly because it provides instant gratification), and then came back to LPTHW and felt like I got quite a bit from it..\n2. Codecademy's Python course is OK, but I really didn't like the IDE; oftentimes it would reject working code. Like I mentioned above, I came to this after starting \u2013 and stopping \u2013 LPTHW. It is fun and easy to make progress at Codecademy, but I don't think it's an optimal learning environment, especially if you haven't done any programming before. \n3. Think Python [ http://www.greenteapress.com/thinkpython/ ] &  How To Think Like A Computer Scientist: Learning With Python (Interactive Edition) [ http://interactivepython.org/runestone/static/thinkcspy/index.html ]\nThink Python is published by O'Reilly but is also available free, direct from the author. I'm learning from this textbook now after having done (1) and (2), and feel like I'm learning quite a few new things, having important concepts reinforced, and getting an idea of some of the basic ideas of computer science. I've made a Memrise \"course\" to help me to remember all the vocabulary: Think Python vocabulary [ http://www.memrise.com/course/173951/think-python-vocabulary/ ].\nOne of the things that makes Think Python nice is that you do some work with graphics; that adds some spice to material that has become familiar.\nThe text of Think Python has also been used to develop the interactive textbook How to Think Like A Computer Scientist, which is pretty fantastic. You are able to do everything in the browser, and the course uses Python 3. I've done some work in it. If I'd found How to Think Like A Computer Scientist first, I probably would have only used it.\n\nHope that's helpful. \n\nI've also written this blog post about my experience learning to program with Python: Some thoughts on learning Python and how to program [ http://www.lukewrites.com/coding/thoughts-codecademy-learn-python-hard-way ].",
                "Don't just read the books. The best way to learn how to program is by doing it. It's easy to spend hours reading about syntax, but computer languages like human languages require you to use them in order to understand them. \n\nTry starting with simple projects. If you're going the HTML / CSS route, play with JavaScript and get a simple div to move around on the page when you click on something. Then work your way up to more complex ones. For instance, write a simple todo list with HTML form elements. You have to learn DOM manipulation to properly handle when the user interacts with the page. \n\nAt Stanford, they teach beginning CS with games. Try building a hangman game using JS, or even just on the command prompt using Ruby or Python. \n\nEdit: A new YC company called Codecademy (http://codecademy.com) just released an all-Javascript tutorial that is quite good. (Disclosure: I'm a venture partner at YC and worked with the founders this past summer.)",
                "Consider Hello World program in C,JAVA and PYTHON\n\nC CODE\n\n[code]#include %3Cstdio.h%3E\nint main()\n{\n\n   printf(\"Hello, World!\");\n   return 0;\n}\n[/code]JAVA CODE\n\n[code]public class HelloWorld {\n\n    public static void main(String[] args) {\n     \n        System.out.println(\"Hello, World\");\n    }\n\n}\n[/code]PYTHON CODE\n\n[code]print \"Hello, World\"\n[/code]The major difference between all three is syntax and Python is easy to understand compared to other languages.",
                "The easiest way to learn a programming language is to first learn the basics and then try to build something with it (learn by doing). And it's better if you are building something you are actually interested in rather than something out of a book because it will get you to think about the problem and be more meaningful. \n\nPython is easy to learn (not much syntax), easy to read (explicit vs implicit), has a big ecosystem (more packages/libraries), is taught at universities so it's easy to find good programmers to help, and is used by many large websites/companies (e.g., Quora is programmed in Python) so it's a good language to know.\n\nOnline Python Tutorials (in order from introductory to more advanced):\n\n1. \"A Byte of Python\" http://www.swaroopch.com/notes/Python\n2. Google's Into to Python Class (online) - http://code.google.com/edu/languages/google-python-class/\n3. \"Dive Into Python\", by Mark Pilgrim http://diveintopython.org/toc/index.html\n4. \"The New Boston\" Programming Python Tutorials - http://www.youtube.com/user/thenewboston#g/c/EA1FEF17E1E5C0DA\n5. \"Building Skills in Python\", by Steven F. Lott - http://homepage.mac.com/s_lott/books/python/html/index.html\n6. \"Think Python: How to Think Like a Computer Scientist\" - http://www.greenteapress.com/thinkpython/thinkpython.html\n7. \"Code Like a Pythonista: Idiomatic Python\"  -http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html\n8. OpenCourseWare: MIT 6.00 Introduction to Computer Science and Programming - http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-00-introduction-to-computer-science-and-programming-fall-2008/video-lectures.\n9. MIT 6.01 Course Readings (PDF) - http://mit.edu/6.01/mercurial/spring11/www/handouts/readings.pdf\n10. Google's \"Understanding Python\" (more advanced talk) - http://www.youtube.com/watch?v=HlNTheck1Hk\n11. \"A Guide to Python's Magic Methods\" - http://www.rafekettler.com/magicmethods.html\n12. \"Metaclasses Demystified\" -http://cleverdevil.org/computing/78/\n\nBook to Get: \"Python Cookbook\", by Alex Martelli (http://www.amazon.com/Python-Cookbook-Alex-Martelli/dp/0596007973/)\n\nAnd if you're building something Web based, look at using the Flask Web Framework (http://flask.pocoo.org/docs/).\n\nFlask is a modern, lightweight, and well-documented Python Web framework so you won't have to spend much time learning it or fighting with it -- you won't find yourself asking, \"Will I be able to do what I want in the framework without hacking it?\" Flask let's you program in Python rather than writing to the framework like you typically have to in larger, opinionated framework's like Django and Rails.",
                "Thanks for A2A.\n\nKeeping it simple, practice practice and practice.\n\nBeing a beginner is easiest, if you have done other language you are already here. Start with online tutorials & videos. Experiment with simplest of problems that uses string manipulation, arithmetic operations and function calls. Personal suggestion: Don\u2019t use IDE (PyCharm, etc..) during this time, I started with Sublime Text and simple code highlighter only. As for now don\u2019t bother with Py2.7, use Py3 it is the future.\n\nFor intermediate, explore python. By explore I mean, play with structural programming and move to OOP. Make a habit of consulting pydoc first (online docs and pydoc terminal command). Implement most used/popular data structure and design patterns, at the same time study which type of problem could that pattern be most useful for. Study how different libraries (simpler ones you used in beginner phase) are developed. Try to improve scripts you wrote during beginner phase using new found skills, make is functional or even implement it with class. Work with multiple files, split you single long scripts in different classes, use your class as libraries. Play with virtualenv. Get yourself acknowledged (just know about these, not necessarily development) with most powerful python libraries (Flask/Django for Web, NumPy for Computational, pandas for Data Manipulation, Keras/ScikitLearn for AI, NLTK for NLP, MatPlotLib for Visualization,\u2026 you got the gist.) Finally, find and solve online programming challenges.\n\nNow advanced for me is specific, ie, where you want to take your python expertise. Overall python pro is too vague and a lot to grasp. For most part, intermediate level will do. If you have made your mind about where your next step lies, may it be Web, Data, AI or stuff, you know enough which you need to learn in deep.\n\nFor example you wish to move toward Web development, you need to have knowledge of web technologies and how they work. Then learn how those are implemented in python or any of its framework. Django and its security vs Flask and its simplicity, what your web app needs more. Jinja templates is another must. Frontend is another beast. One can never be JUST backend developer, you should at least know your way around JS, CSS & HTML. Also get yourself acquainted about different web architecture like MVC.\n\nOn other hand, you choose python for AI and stuff; techs mentioned in above paragraph might be useless for you. Here one have to have extensive knowledge of Stats & Probability. Mathematics is heart of AI. Start getting deep on NumPy, pandas, Scikitlearn,\u2026 Rather than just using libraries, first try to understand it. If you can develop a version of it on your own it helps you to get grasp on underlying calculation processes. Solve problems from kaggle, study others\u2019 solution and identify your weak point. Check for math problems & solutions, code the solution in python (without libraries).\n\nLast one, Python Guru (just a term I use). They are ones who develop and support python. Derive new libraries, improve/optimize existing codes. To be here, learning or even being advance python dev is just a tip of hidden iceberg inside vast concept of programming, mathematics & computational theories. Anyway we are not talking about this, move on.\n\nIn short, intermediate is hardest to master. By the time you are in upper part of intermediate, you can tackle almost any problem using python. Before stepping to advance libraries on your desired path, I suggest you to learn ins-and-outs of general python convention.\n\nLastly, \u201cLanguage is just a tool to develop. Data structure and algorithm should be your main concern.\u201d Code can be found anywhere once you know what and how to tackle the problem.\n\nDon\u2019t Just Code, Solve Problem.",
                "The easiest way to learn a programming language is to first learn the basics and then try to build something with it (learn by doing). And it's better if you are building something you are actually interested in rather than something out of a book because it will get you to think about the problem and be more meaningful. \n\nPython is easy to learn (not much syntax), easy to read (explicit vs implicit), has a big ecosystem (more packages/libraries), is taught at universities so it's easy to find good programmers to help, and is used by many large websites/companies (e.g., Quora is programmed in Python) so it's a good language to know.\n\nOnline Python Tutorials (in order from introductory to more advanced):\n\n1. \"A Byte of Python\" http://www.swaroopch.com/notes/Python\n2. Google's Into to Python Class (online) - http://code.google.com/edu/languages/google-python-class/\n3. \"Dive Into Python\", by Mark Pilgrim http://diveintopython.org/toc/index.html\n4. \"The New Boston\" Programming Python Tutorials - http://www.youtube.com/user/thenewboston#g/c/EA1FEF17E1E5C0DA\n5. \"Building Skills in Python\", by Steven F. Lott - http://homepage.mac.com/s_lott/books/python/html/index.html\n6. \"Think Python: How to Think Like a Computer Scientist\" - http://www.greenteapress.com/thinkpython/thinkpython.html\n7. \"Code Like a Pythonista: Idiomatic Python\"  -http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html\n8. OpenCourseWare: MIT 6.00 Introduction to Computer Science and Programming - http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-00-introduction-to-computer-science-and-programming-fall-2008/video-lectures.\n9. MIT 6.01 Course Readings (PDF) - http://mit.edu/6.01/mercurial/spring11/www/handouts/readings.pdf\n10. Google's \"Understanding Python\" (more advanced talk) - http://www.youtube.com/watch?v=HlNTheck1Hk\n11. \"A Guide to Python's Magic Methods\" - http://www.rafekettler.com/magicmethods.html\n12. \"Metaclasses Demystified\" -http://cleverdevil.org/computing/78/\n\nBook to Get: \"Python Cookbook\", by Alex Martelli (http://www.amazon.com/Python-Cookbook-Alex-Martelli/dp/0596007973/)\n\nAnd if you're building something Web based, look at using the Flask Web Framework (http://flask.pocoo.org/docs/).\n\nFlask is a modern, lightweight, and well-documented Python Web framework so you won't have to spend much time learning it or fighting with it -- you won't find yourself asking, \"Will I be able to do what I want in the framework without hacking it?\" Flask let's you program in Python rather than writing to the framework like you typically have to in larger, opinionated framework's like Django and Rails.",
                "1. Read the documentation. This is by far the most underrated thing that could blow up your knowledge about a language. I can't count how many people ask me about problems that are well documented.\n2. Learn the most popular libraries, why they were made, how they work. (I.e: Flask, Django, requests)\n3. Try writing concurrent code, learn about threads, process, how they work, their weakness in Python.\n4. Learn about sockets, network libraries, async features.\n5. Learn a little about scipy and numpy.\n6. Learn about interpreters, how they work, why there are so many different implementations of Python. (Python is written in English not C as many state, the most common implementation is a C based interpreter). This is a very important concept.\n7. Learn about Python ecosystem. Please do not be a hipster writing code in the notepad, at least not yet. Learn to use PyCharm. Learn about PEP8. Learn about PIP, setuptools, virtualenv.\n8. Learn about Python production brothers. Docker containers. UWSGI. Gunicorn. NGINX. All the stuff.\n9. Learn tips and tricks from Google. Lambda functions. Decorators, descriptors, iterators, generators, metaclasses (Thanks Adrian Carpenter [ https://www.quora.com/profile/Adrian-Carpenter ]!). One liners like: \"python -m http.server\".\n10. Write a lot of buggy code.\nNow you are ready. Good luck :)",
                "Consider Hello World program in C,JAVA and PYTHON\n\nC CODE\n\n[code]#include %3Cstdio.h%3E\nint main()\n{\n\n   printf(\"Hello, World!\");\n   return 0;\n}\n[/code]JAVA CODE\n\n[code]public class HelloWorld {\n\n    public static void main(String[] args) {\n     \n        System.out.println(\"Hello, World\");\n    }\n\n}\n[/code]PYTHON CODE\n\n[code]print \"Hello, World\"\n[/code]The major difference between all three is syntax and Python is easy to understand compared to other languages.",
                "The easiest way to learn a programming language is to first learn the basics and then try to build something with it (learn by doing). And it's better if you are building something you are actually interested in rather than something out of a book because it will get you to think about the problem and be more meaningful. \n\nPython is easy to learn (not much syntax), easy to read (explicit vs implicit), has a big ecosystem (more packages/libraries), is taught at universities so it's easy to find good programmers to help, and is used by many large websites/companies (e.g., Quora is programmed in Python) so it's a good language to know.\n\nOnline Python Tutorials (in order from introductory to more advanced):\n\n1. \"A Byte of Python\" http://www.swaroopch.com/notes/Python\n2. Google's Into to Python Class (online) - http://code.google.com/edu/languages/google-python-class/\n3. \"Dive Into Python\", by Mark Pilgrim http://diveintopython.org/toc/index.html\n4. \"The New Boston\" Programming Python Tutorials - http://www.youtube.com/user/thenewboston#g/c/EA1FEF17E1E5C0DA\n5. \"Building Skills in Python\", by Steven F. Lott - http://homepage.mac.com/s_lott/books/python/html/index.html\n6. \"Think Python: How to Think Like a Computer Scientist\" - http://www.greenteapress.com/thinkpython/thinkpython.html\n7. \"Code Like a Pythonista: Idiomatic Python\"  -http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html\n8. OpenCourseWare: MIT 6.00 Introduction to Computer Science and Programming - http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-00-introduction-to-computer-science-and-programming-fall-2008/video-lectures.\n9. MIT 6.01 Course Readings (PDF) - http://mit.edu/6.01/mercurial/spring11/www/handouts/readings.pdf\n10. Google's \"Understanding Python\" (more advanced talk) - http://www.youtube.com/watch?v=HlNTheck1Hk\n11. \"A Guide to Python's Magic Methods\" - http://www.rafekettler.com/magicmethods.html\n12. \"Metaclasses Demystified\" -http://cleverdevil.org/computing/78/\n\nBook to Get: \"Python Cookbook\", by Alex Martelli (http://www.amazon.com/Python-Cookbook-Alex-Martelli/dp/0596007973/)\n\nAnd if you're building something Web based, look at using the Flask Web Framework (http://flask.pocoo.org/docs/).\n\nFlask is a modern, lightweight, and well-documented Python Web framework so you won't have to spend much time learning it or fighting with it -- you won't find yourself asking, \"Will I be able to do what I want in the framework without hacking it?\" Flask let's you program in Python rather than writing to the framework like you typically have to in larger, opinionated framework's like Django and Rails.",
                "Totally depends on what type of learner you are...\n\nIf you like classroom learning:\n     Check out Udacity, a free and top-notch online course video series on programming in Python taught by Stanford & UVA professors. http://www.udacity.com/\n\nIf you prefer books:\n    Check out Michael Dawson's \"Python Programming for the Absolute Beginner,\" an exceptionally clear and engaging read. Dawson is a game programmer and the book is structured such that each chapter culminates in designing a game. http://www.amazon.com/Python-Programming-Absolute-Beginner-Edition/dp/1435455002\n\nIf you want the best of both worlds:\n    Check out \"Learn Python the Hard Way.\" an online chain of 50-or-so learning exercises, each of which builds on the last. If you run into any walls (which you will) Zed Shaw, the site creator, will answer your questions or give you hints. It's called the hard way because it forces you to jump in head first, but it saves you time in the long run since you're learning things the right way. http://learnpythonthehardway.org/\n\nRegardless, make sure that you're getting your hands dirty from the start with some mini projects. Programming, as much as anything, is something you learn by doing.   \n\nGood luck!\n\nFor tips on boosting your focus, mood, and brainpower, follow my new Quora blog The Mental Edge [ https://wellnesshacks.quora.com/ ]",
                "I have been teaching myself programming and Python. Some resources I found helpful:\n\n1. Learn Python The Hard Way [ http://learnpythonthehardway.org/ ] is usually recommended enthusiastically, and I thought it was good. It very much puts the onus on you to investigate and be creative. This can seem daunting at first; I started with LPTHW, quit and did Codeacademy for a while (mostly because it provides instant gratification), and then came back to LPTHW and felt like I got quite a bit from it..\n2. Codecademy's Python course is OK, but I really didn't like the IDE; oftentimes it would reject working code. Like I mentioned above, I came to this after starting \u2013 and stopping \u2013 LPTHW. It is fun and easy to make progress at Codecademy, but I don't think it's an optimal learning environment, especially if you haven't done any programming before. \n3. Think Python [ http://www.greenteapress.com/thinkpython/ ] &  How To Think Like A Computer Scientist: Learning With Python (Interactive Edition) [ http://interactivepython.org/runestone/static/thinkcspy/index.html ]\nThink Python is published by O'Reilly but is also available free, direct from the author. I'm learning from this textbook now after having done (1) and (2), and feel like I'm learning quite a few new things, having important concepts reinforced, and getting an idea of some of the basic ideas of computer science. I've made a Memrise \"course\" to help me to remember all the vocabulary: Think Python vocabulary [ http://www.memrise.com/course/173951/think-python-vocabulary/ ].\nOne of the things that makes Think Python nice is that you do some work with graphics; that adds some spice to material that has become familiar.\nThe text of Think Python has also been used to develop the interactive textbook How to Think Like A Computer Scientist, which is pretty fantastic. You are able to do everything in the browser, and the course uses Python 3. I've done some work in it. If I'd found How to Think Like A Computer Scientist first, I probably would have only used it.\n\nHope that's helpful. \n\nI've also written this blog post about my experience learning to program with Python: Some thoughts on learning Python and how to program [ http://www.lukewrites.com/coding/thoughts-codecademy-learn-python-hard-way ].",
                "Consider Hello World program in C,JAVA and PYTHON\n\nC CODE\n\n[code]#include %3Cstdio.h%3E\nint main()\n{\n\n   printf(\"Hello, World!\");\n   return 0;\n}\n[/code]JAVA CODE\n\n[code]public class HelloWorld {\n\n    public static void main(String[] args) {\n     \n        System.out.println(\"Hello, World\");\n    }\n\n}\n[/code]PYTHON CODE\n\n[code]print \"Hello, World\"\n[/code]The major difference between all three is syntax and Python is easy to understand compared to other languages.",
                "You can't compare python vs java, each has its own +ve's and -ve's..\n\nPython is easy to learn and use, we can build apps in short time than java. \n\nWell there is an old infographic java vs python\n\n",
                "Well, the answer is damn Yes.\n\nYes, you can learn Python within one month. But you have to read me carefully. There are a few steps you have to follow during this 1 month. So, here we go.\n\nStep 1: Don't go for any tutorial on the internet.\n\nWhy I'm saying is that,\n\n\n%3E You are the only Guru of yourself to get things very fast.\nBuy a book or collect from anywhere of Python 3. Yes, I'll repeat, go for Python 3. Please don't go for Python 2.\n\nI'll recommend you to take learn Python the Hard Way [ https://learnpythonthehardway.org/book/nopython3.html ] book as your guide. This is one of the best books in Python for beginners like you.\n\nStep 2: Download an app DataCamp - Learn R, Python & SQL - Apps on Google Play [ https://play.google.com/store/apps/details?id=com.datacamp ] from play store( If you are an Android user) or DataCamp - Learn R & Python on the App Store [ https://itunes.apple.com/us/app/datacamp-learn-r-python/id1263413087 ] from App store (If you are a Richie Rich)\n\nAnd keep doing practice on hand when you'll be free.\n\nStep 3: Stop watching some useless video and start watching some Python stuff on YouTube.\n\nAnd, that's it. After a month you'll see yourself as a Python programmer.\n\nYou've taken a great step, my dear mate. Go for it, because Python will tell you what you want to be after one month.\n\nHope it'll help you. Do follow me for a new programming relationship. All the best for your new beginning.",
                "Consider Hello World program in C,JAVA and PYTHON\n\nC CODE\n\n[code]#include %3Cstdio.h%3E\nint main()\n{\n\n   printf(\"Hello, World!\");\n   return 0;\n}\n[/code]JAVA CODE\n\n[code]public class HelloWorld {\n\n    public static void main(String[] args) {\n     \n        System.out.println(\"Hello, World\");\n    }\n\n}\n[/code]PYTHON CODE\n\n[code]print \"Hello, World\"\n[/code]The major difference between all three is syntax and Python is easy to understand compared to other languages.",
                "Here is a Roadmap:\n\nWeek 1\n\nGoogle Python programming fundamentals and pick on any good site. Tutorialspoint or any other should be fine. This should take you about a week say 8 hours per day while doing small scripts for practice to enforce your understanding. Note: never memorize the syntax, it will come naturally and looking up each time when you need it until you finally internalize it is the way to go.\n\nWeek 2, 3, 3.5\n\nPick up a project and start working to finish it. Some general tips when working on the project:\n\n * Don\u2019t overthink, just pick on anything e.g basic calculator with a UI or could be anything that makes sense or google \u201cbeginner python projects\u201d or something.\n * You don\u2019t have to memorize or remember any syntax. So look up on the internet when you are stuck. Try to get hints on your problems.\n * Use an IDE (subjective but I prefer it since it is easier to debug and run scripts)\n * Breakdown the project to smaller components e.g if you are creating a calculator then:\n * \n * Graphical User Interface - further into various buttons\n * Addition, subtraction, division and multiplication functions assigned to various buttons (note: there is no golden rule on how to break down projects, as long as they are manageable chunks)\n\n * Stackoverflow is a good friend. But make sure you don\u2019t copy paste without understanding what the script does.\n * It will be hard and you need to be mentally strong to push through problems. When stuck on a problem:\n1. \n1. \n1. Look it up on google and try finding the solution by being more precise with your search queries\n2. If you don\u2019t find a solution then post it on a forum where you might find some help.\n\n\n * You need small breaks when programming. It helps to get away from the computer for a while and come back.\n * \n * May be this helps: Pomodoro Technique - Wikipedia [ https://en.wikipedia.org/wiki/Pomodoro_Technique ]\n\n * Invest time to learn version control. Should not take you more than a day or two. Basics should be fine. I prefer git.\n * Learn debugging as you go. Its not rocket science. Can take you literally take you only couple of hours to learn how to do it.\n * Ensure that you have a theoretical approach to solving the problem before you start writing any code. It won\u2019t magically give you the correct result. Step aside from wishful thinking and be more practical. You can draw diagrams or write down what you want to achieve the result.\n * If you get time try investing a few hours learning testing and implement some tests in your code.\n * Don\u2019t be frustrated if a problem takes long to solve but be aware of the time, you can move on to other components of the project to complete and come back later.\nWeek 3.5 / 4\n\nTry smoke testing and ensuring that bugs are fixed. Now, if you have not complete then its alright, many at times we might not depending on some factors. In that case:\n\n * \n * Allocate yourself more time (i.e more than a month)\n * Prioritize features that you might need for the project.\n\nCongrats you are better than many programmer that have been going through tutorials for a year claiming they have learnt a lot. May be even better than a big percentage of 1st and 2nd year CS undergrads.\n\nImagine the possibilities you keep building projects for the year!\n\nPS: To keep going you can start learning how databases work and learn some python frameworks to achieve what you want and build what you like.",
                "In my opinion the most effective way is by actually doing. Pick a project that interests you and start working on it. If you're just starting out, just focus on basics, I wrote post on that in another question which also might be useful to you What books, in what order, should I read to learn Python? [ https://www.quora.com/What-books-in-what-order-should-I-read-to-learn-Python ] After you're familiar with basic concepts, just dive into topic of your choice or maybe solve a problem that you have or try to automate repetitive tasks with Python script.\n\n",
                "Claim: You can\u2019t learn Python in one month.\n\nProof: If you can grok this extract in five minutes then yes, by all means you can learn Python programming in one month. But even reading this answer will take more than five minutes (let alone learning and practicing it), so we conclude that Python cannot be learnt in one month.\n\nWhat if you\u2019re an experienced programmer? Then you\u2019d not have asked this question. ;)\n\nChapter 0\n\nWhat do you think will be the result of dividing 7 over 5? Will it be a whole number or a decimal? In Python, a whole number is called an Integer (hereafter referred to as int) whereas a decimal is called a Float.\n\n[code]7 / 5\n%3E 1.4\n[/code]What about 10/5? Will the result be an int?\n\n[code]10 / 5\n%3E 2.0\n[/code]To our surprise, int division produced a float result. If we wanted to produce an int division, we should have done this:\n\n[code]10 // 5\n%3E 2\n[/code]The // is called floor division. here\u2019s why:\n\n[code]6 // 4\n%3E 1\n[/code]Because floor division must produce integers, whatever is fractional or decimal is cut down. In the case above, the mathematical answer should have been 1.5, but since // produces only an int, the 0.5 part was truncated, leaving only the 1.\n\nAlong the same topic, we also have % for remainder. If you divide 16 over 5, the result is 3, but the remainder is 1. In Python, % means mod or modulus, and it works like this:\n\n[code]16 % 3\n%3E 1\n[/code]The rest of the operations are:\n\n[code]1 * 2 \n%3E 2\n\n4 - 2 \n%3E 2\n\n(4 + 5) * 8\n%3E 72\n\n# raising to a power is done with **\n\n2**3\n%3E 8\n[/code]You must have noticed a comment (about raising to a power with **) that started with #. In programming, we write short comments within the code, so that we our future selves or others whose task is to read our code can make sense of some tricky and not immediately-obvious parts of our program. In Python, one-line comments start with #.\n\nSo far, we\u2019ve used the numbers freely. As soon as we computed some mathematical expression, the task was done. Sometimes, however, we need to save numbers in containers that should interact with other containers to produce increasingly-complex results.\n\nA container is merely a name that contains the data type\u2014here a number\u2014within itself.\n\nFor instance, let\u2019s measure some cube. The cube\u2019s height is 30 cm and its width is 15 cm, whereas its length is 40 cm.\n\nThus, we have three containers here: height, length and width. In python, locking a number within a container is called assignment. When you assign 10 to [code ]width[/code], you can then use width instead of typing 15 each time. Normally, a container is called a variable and that\u2019s what we\u2019ll say too.\n\nGiven the three variables, we can now compute the cube\u2019s \u201cVolume\u201d. Then we can save (assign) the result to a variable called [code ]volume[/code].\n\n[code]height = 30\nwidth  = 10\nlength = 40\n\nvolume = height * width * length\n[/code]So now that we have [code ]volume[/code], what can we do with it? In programming, the \u2018verb\u2019 which is something we do with variables goes by many names depending on the language. In Python, and more generally even, this is called a function.\n\nA function does something useful and can be already existing in Python\u2014called built-in\u2014or it can be made from scratch by us. Note that each function has a unique name and you must never assign to a custom function you wrote, a name that belongs to a built-in function.\n\nSuppose we wanted to inspect the contents of [code ]volume[/code]. The built-in function [code ]print[/code] will be enough:\n\n[code]print(volume)\n%3E 12000\n[/code]You also notice that our function print has an opening and a closing parenthesis. All functions in Python have this, even when there\u2019s nothing inside them. The thing inside the function\u2019s parenthesis is called function argument, and if it exists, then we can say a function will be a transitive verb whereas a function argument will be the object of that verb: in other words, the function \u201cdoes\u201d what it\u2019s supposed to do \u201cto\u201d the function argument.\n\nIn the jargon, we say \u201ccall a function\u201d instead of using a function, and adding an argument to a function is referred to as \u201cpassing an argument to a function\u201d.\n\nHere is another built-in function: round(). It\u2019s purpose is to round a decimal number. For instance, 900.9558 rounded to two decimals can be written as:\n\n[code]round(900.9558, 2)\n%3E 900.96\n[/code]Now we will turn to the last item before we have a scaffolding in place to jump into programming: strings.\n\nSimply, a string is something that is not considered a number and it is usually a letter, word, or a sentence. In Python, we use single or double quotes to represent a string. Here are some examples of strings:\n\n[code]\"j\"\n\"Jan\"\n\"I am Jan Meww, and you?\"\n\"39\"\n[/code]Note that \u201c39\u201d is not a number or not considered a number because of being enclosed in quotes. The main thing that differentiates a number from a string is this: when you type a number in the console (interactive session), Python will automatically accept it as a number, but if you type a word or a letter, Python will throw or declare an error.\n\nNow you\u2019re ready. If you recall from high school, a Fibonacci sequence is sequence of numbers such, that as long as our number is more than 1, the next number is a sum of the previous two:\n\n[math]0, 1, 2, 3, 4, 5[/math]\n\nThus, [math]0+1 = 1, 1+1 = 2, 1 + 2 = 3,  2 + 3 = 5,  3 + 5 = 8[/math], and [math]5 + 8 = 13[/math] and onwards. Note, as we add [math]1 + 2 = 3[/math], we take the two numbers on both sides of the equal sign and add them up: [math]2 + 3 = 5[/math]. Now the two numbers on both sides of the = are [math]3[/math] and [math]5[/math], so we add these up, and so forth.\n\nProgramming is giving well-defined instructions to a computer so that there are no ambiguities that might lead the computer to make a mistake. On the contrary, among humans, instructions can be ambiguous and still understood because our body language, tone, cultural references, and the context can provide plenty of cues to our minds.\n\nThere\u2019s a well-known joke (well-known among programmers, though) about this:\n\n\n%3E A woman asks her husband, a programmer, to go shopping:\n\n-Dear, please, go to the nearby grocery store to buy some bread. Also, if they have eggs, buy a dozen.\n\n-OK, hun.\n\nTwenty minutes later the husband comes back bringing 12 loaves of bread.\n\nHis wife is flabbergasted:\n\n-Why on earth did you buy a dozen loaves of bread?\n\n-They had eggs\nThe joke reflects how computers think and how programmers are affected by that precise level of thinking. This is not such a bad thing as the joke implies; in fact, the art of providing clear, concise instruction to a computer will sharpen your brain and also fine-tune it to detect weak arguments.\n\nThe problem in the above joke is that the wife said: \u201cif they have eggs, buy a dozen\u201d. For humans it is easy to see that the comma in the sentence above implies that the dozen is connected to the eggs, but computers cannot infer commas. A computer deals with sentences and instructions as they are, which makes it a little difficult to get used to at first.\n\nRecasting the same joke to make a computer understand it as we do should have resulted in:\n\n\u201cif they have eggs, then in addition to buying some bread, buy a dozen eggs, too\u201d.\n\nEven this has a problem, because the computer won\u2019t buy bread if the shop did not have eggs. We can rephrase it:\n\n\u201cFirst, buy some bread. Then, if they have eggs, buy a dozen eggs, too.\u201d\n\nNow this is unambiguous.\n\nReturning to the Fibonacci, our Fibonacci sequence is well-defined because it contains no ambiguities and the computer can make sense of it; we say \u201ctake two numbers [code ]a[/code] and [code ]b[/code], turn [code ]a[/code] to [code ]b[/code], and turn [code ]b[/code] to [code ]a+b[/code]\u201d. And most likely we will also add a stopping point so that it does not go on forever: \u201ccontinue to do this up to some limit.\u201d\n\nHow would we write this as code? We need a proper way to inform Python that we want to add the previous two numbers and present them in a sequence. This requires a loop.\n\nA loop is a programming concept that can be approximately called a \u201cdo this during this duration\u201d. One loop we will see now is called [code ]while[/code]. So let\u2019s first see this program and then dissect it:\n\n[code]a, b = 0, 1\nwhile a %3C 20:\n\ta, b = b, a+b\n\tprint(a)\n[/code]You will have noticed that we can assign several variables at the same time. We assigned [math]0[/math] to [code ]a[/code] and [math]1[/math] to[code ] b[/code] at the same time, separated by a comma. Next, we instruct Python that as long as [code ]a[/code] is less than [math]20[/math], do the following tasks. The colon at the end of [math]20[/math] is represents the concept of \u201cfollowing tasks\u201d.\n\nAnother thing you noticed after the colon is that the code automatically will go 4 spaces to the right (in most Python editors it does this automatically. If it does not, you have to do this manually yourself, but in practice this is done automatically to prevent errors in miscalculated spaces).\n\nMore specifically, this spacing is called indentation, and the four-space indentation is unique only to Python; because Python cares about the white space around the code, the core idea behind indentation is to make the code easier on the eyes, especially if we have long pages of code.\n\nNext, as long as [code ]a[/code] is less than [math]20[/math], we will start by assigning the value of [code ]b[/code] to [code ]a[/code], and the value of[code ] a+b[/code] to [code ]b[/code]. This is very similar to adding the two numbers on the two sides of the = sign I showed earlier. If [code ]a[/code] was [math]2[/math] and [code ]b[/code] was [math]3[/math], then the next [code ]a[/code] will be [math]3[/math] whereas the next [code ]b[/code] will be [math](2+3)[/math] or [math]5[/math]. Thus, for the next round, the next [code ]a[/code] will change from [math]3[/math] and become [math]5[/math], while the next [code ]b[/code] will become [math](3+5)[/math] or [math]8[/math], and so forth.\n\nFinally, we also instruct Python to display all the values that [code ]a[/code] takes during its [code ]while[/code] journey:\n\nNow that you\u2019re exposed to the [code ]while[/code] loop, another important foundation concept is the conditional; in Python, the conditional is [code ]if[/code], and acts like \u201cif\u201d as humans use it. Suppose I am 40 years old, and one of my acquaintances is Python. If we were to ask Python whether Jan is 40, we would make use of [code ]==[/code] which stands for \u2018equal to\u2019. Let\u2019s see what Python says about this:\n\n[code]# first we make sure Jan is 40\njan = 40\n\n# then we check whether this is really so\njan == 40\n%3E true\n[/code]Python agrees. What about:\n\n[code]Jan == 40\n[/code]Will it evaluate to [code ]true[/code] as well? No, because variable names are case-sensitive. jan and Jan are not the same thing.\n\nI also want to mention that assigning a value to a variable, say:\n\n[code]Meww = 40\n[/code]And then assigning 39 to Meww will result in Meww becoming 39, because that is the last value we assigned to Meww; A container can only contain one value and when we assign 39 to Meww, 40 has to go. (so that 39 can take its place.)\n\nBut go where? Python automatically clears the space for the newly assigned 39, and 40 goes to the virtual \u2018thrash can\u2019. In the jargon, this is called automatic garbage collection, a concept Python borrowed from Lisp. (And not all languages have this.)\n\nNext, you will see an error that was caused by mistyping. This is the most frequent cause of beginner level mistakes in code:\n\nAs you can see, Jann is something unrecognized by Python, and it will mention this to us in its error message, even though, Python\u2019s error messages are rather cryptic.\n\nNow, let\u2019s use the conditional[code ] if[/code]. If Jan is 40, Python should say something more than true; perhaps we can ask it to say: \u201cCongrats, you\u2019re still young!\u201d:\n\n[code]Jan = 40\nif Jan == 40:\n\tprint(\u201cCongrats, you\u2019re still young!\u201d)\n[/code]That is it. Now what happens if Jan is not 40? We can also insert additional conditional clauses in there: suppose we wanted this: if Jan is 40 then print \u201choorah\u201d and if Meww is 39, then print \u201ccongrats\u201d.\n\n[code]Jan, Meww = 40, 39\nif Jan == 40:\n\tprint(\"hoorah!\")\nif Meww == 39:\n\tprint(\"congrats!\")\n[/code]The conditional does not stop there. We can add further conditions. Suppose you instruct your robot to go to the Mall and buy milk and eggs. You could say something like this:\n\n[code]# define the variables:\n\neggs = \"eggs\"\nbread = \"bread\"\n\nif eggs == true:\n\tbuy(eggs)\nelse:\n\tbuy(bread)\n[/code]First, we instruct the robot to recognize eggs and bread when it sees them. In the solution space\u2014which is the page where we write our Python code\u2014this means assigning strings \u201ceggs\u201d and \u201cbread\u201d to the variables of the same name, so that we can use those names directly, without attaching quotes to them each time. This enhances readability but also prevents errors.\n\nNext, we add a conditional which is the equivalent of \u201cif eggs are available in the Mall\u201d. In Python, existence and availability is synonymous with [code ]true[/code]. If something is [code ]true[/code], then it exists and can be used.\n\nThe final step was instructing the robot what to do; we learned earlier that doing something was a function, and functions look like this: [code ]function(argument)[/code] if transitive but also [code ]function()[/code] if intransitive. While the syntax of our code above is correct, the function [code ]buy(something)[/code] is not already defined in Python, and the robot will not recognize it. Thus, we will also have to define\u2014create\u2014this function and include correct and clear instructions in the body of this function so that Python can execute it according to our wishes.\n\nA function is made with a keyword [code ]def[/code], a function name, and a body. Here the function name is [code ]greet[/code], and the body\u2019s task is to say \u201cHello!\u201d.\n\nLike so:\n\n[code]def greet():\n\tprint(\"Hello!\")\n[/code]This is an argumentless function (intransitive), so when we call (access or put it to use) it, it must also be argumentless:\n\n[code]greet()\n%3E Hello!\n[/code]We can also make functions that take arguments\u2014what I earlier termed transitive. Suppose the robot will greet by name. Thus, the name of the person it greets is the argument:\n\n[code]def greet(name):\n\tprint(\"Hello!\", name)\n[/code]Let\u2019s see it in action:\n\n[code]greet(\"Jan\")\n%3E Hello! Jan\n[/code]Note that the placeholder argument [code ]name[/code] must be a string, since names are made of letters\u2014and not numbers. This is why Jan was written as [code ]\u201cJan\u201d[/code]. But we could have done this too:\n\n[code]greet(5)\n%3E Hello! 5\n[/code]This is both a strength and a weakness in Python but beyond the scope of this post. Consider this, though: in certain languages, like Julia, argument type can be specified at the time we define a function, but as of this writing, not in Python.\n\nSo when you define\n\n[code]def greet(name):\n  print(\"hi\", name)\n[/code]In Python, you cannot force-specify what type of input name could be. This leads to more flexibility as you saw earlier, but also to potentially hidden bugs or errors.\n\nFinally, note the comma between name and \u201cHello!\u201d. This is the first time you see [code ]print()[/code] taking two arguments separated by a comma. However, the ability of a function to take n arguments is restricted by its construction.\n\nThus, [code ]print()[/code] is built-in and constructed to take more than one argument, whereas [code ]greet()[/code] is not because we only passed [code ]name[/code] as the maximum number of arguments it should accept.\n\nHow did you do? How long did it take?\n\n\nAll the best,\n\nJan",
                "Well, the answer is damn Yes.\n\nYes, you can learn Python within one month. But you have to read me carefully. There are a few steps you have to follow during this 1 month. So, here we go.\n\nStep 1: Don't go for any tutorial on the internet.\n\nWhy I'm saying is that,\n\n\n%3E You are the only Guru of yourself to get things very fast.\nBuy a book or collect from anywhere of Python 3. Yes, I'll repeat, go for Python 3. Please don't go for Python 2.\n\nI'll recommend you to take learn Python the Hard Way [ https://learnpythonthehardway.org/book/nopython3.html ] book as your guide. This is one of the best books in Python for beginners like you.\n\nStep 2: Download an app DataCamp - Learn R, Python & SQL - Apps on Google Play [ https://play.google.com/store/apps/details?id=com.datacamp ] from play store( If you are an Android user) or DataCamp - Learn R & Python on the App Store [ https://itunes.apple.com/us/app/datacamp-learn-r-python/id1263413087 ] from App store (If you are a Richie Rich)\n\nAnd keep doing practice on hand when you'll be free.\n\nStep 3: Stop watching some useless video and start watching some Python stuff on YouTube.\n\nAnd, that's it. After a month you'll see yourself as a Python programmer.\n\nYou've taken a great step, my dear mate. Go for it, because Python will tell you what you want to be after one month.\n\nHope it'll help you. Do follow me for a new programming relationship. All the best for your new beginning.",
                "Yes learning python in a month is possible.\n\nIn a month its will be only learning, you will learn to do basic level programming like examples that you came across while learning. So my suggestion is to learn optimising the code while you learn basic examples. You can write a code that does same job in few lines as well 100s of lines.\n\nTo learn optimising join hacker rank python, they have easy problems to solve, but in discussions sections you will see how experts written same job in few lines. Learn from those lessons.\n\nIn short, this is how I started and learning.\n\n1. YouTube tutorials\n2. Hacker rank\n3. Apply python on you daily task.\n4. Help others in stack overflow and learn from it.\n",
                "Here is a Roadmap:\n\nWeek 1\n\nGoogle Python programming fundamentals and pick on any good site. Tutorialspoint or any other should be fine. This should take you about a week say 8 hours per day while doing small scripts for practice to enforce your understanding. Note: never memorize the syntax, it will come naturally and looking up each time when you need it until you finally internalize it is the way to go.\n\nWeek 2, 3, 3.5\n\nPick up a project and start working to finish it. Some general tips when working on the project:\n\n * Don\u2019t overthink, just pick on anything e.g basic calculator with a UI or could be anything that makes sense or google \u201cbeginner python projects\u201d or something.\n * You don\u2019t have to memorize or remember any syntax. So look up on the internet when you are stuck. Try to get hints on your problems.\n * Use an IDE (subjective but I prefer it since it is easier to debug and run scripts)\n * Breakdown the project to smaller components e.g if you are creating a calculator then:\n * \n * Graphical User Interface - further into various buttons\n * Addition, subtraction, division and multiplication functions assigned to various buttons (note: there is no golden rule on how to break down projects, as long as they are manageable chunks)\n\n * Stackoverflow is a good friend. But make sure you don\u2019t copy paste without understanding what the script does.\n * It will be hard and you need to be mentally strong to push through problems. When stuck on a problem:\n1. \n1. \n1. Look it up on google and try finding the solution by being more precise with your search queries\n2. If you don\u2019t find a solution then post it on a forum where you might find some help.\n\n\n * You need small breaks when programming. It helps to get away from the computer for a while and come back.\n * \n * May be this helps: Pomodoro Technique - Wikipedia [ https://en.wikipedia.org/wiki/Pomodoro_Technique ]\n\n * Invest time to learn version control. Should not take you more than a day or two. Basics should be fine. I prefer git.\n * Learn debugging as you go. Its not rocket science. Can take you literally take you only couple of hours to learn how to do it.\n * Ensure that you have a theoretical approach to solving the problem before you start writing any code. It won\u2019t magically give you the correct result. Step aside from wishful thinking and be more practical. You can draw diagrams or write down what you want to achieve the result.\n * If you get time try investing a few hours learning testing and implement some tests in your code.\n * Don\u2019t be frustrated if a problem takes long to solve but be aware of the time, you can move on to other components of the project to complete and come back later.\nWeek 3.5 / 4\n\nTry smoke testing and ensuring that bugs are fixed. Now, if you have not complete then its alright, many at times we might not depending on some factors. In that case:\n\n * \n * Allocate yourself more time (i.e more than a month)\n * Prioritize features that you might need for the project.\n\nCongrats you are better than many programmer that have been going through tutorials for a year claiming they have learnt a lot. May be even better than a big percentage of 1st and 2nd year CS undergrads.\n\nImagine the possibilities you keep building projects for the year!\n\nPS: To keep going you can start learning how databases work and learn some python frameworks to achieve what you want and build what you like.",
                "I've been learning python for about six months now. Not spending 8 hours a day on it but more as a hobby. \nI started less than a year ago with html, css and javascript. So I did start python with some solid knowledge on what code and programming is about. \nOf course, when starting python, I fell in love with the syntax and simplicity this language offers. It also allowed me to easily learn some complex programming concepts that I am able to use in other languages I sometimes have fun with (for example VBA). \nAfter six months of python I have had a few projects going on and here are the major things I worked on, chronologically:\n\n1. I created a bot that plays an online game for me. It works recognizing images on the screen and clicking where appropriate. This was a great experience as it was the first \"big\" program I ever wrote.\n2. Raspberry pi: working on this one with my brother. We have had different projects. Starting from lighting up lights to a self driving car:\n1. \u200b\n\n\u200b\u200b\u200bour next goal with this one is to add a camera and be able to control the car from a website.\n2. OpenPyxl: I use a lot of Excel for work. VBA is a great language but I like being able to do things through python, completely in the dark and once I open excel it magically copied and compared data from different files. So many things happen \"underneath\" that I haven't been able to do with VBA.\n3. Django: is what I try to focus on the most more recently. If I want to one day work in the tech industry I believe Web Development is the best route for me to start in. Django is really powerful and starts making sense (I have to say the beginning seemed a little overwhelming because there are a lot of things that need to be understood, as well as the links between those things) to me. I think the fact I started with html, css and javascript is really going to help me out in Django projects so I'm looking forward to the future things I will create.\nAfter 6 months I'm pretty happy with the python level I have reached. I hope I'll manage to double this knowledge in the next six months. And be able to apply as much as possible in my day to day life (current job, personal apps, other projects) on order to keep it as exciting as it has been.\n\nI hope this answer gives you an idea of what the next few months might look like for you.",
                "Well, the answer is damn Yes.\n\nYes, you can learn Python within one month. But you have to read me carefully. There are a few steps you have to follow during this 1 month. So, here we go.\n\nStep 1: Don't go for any tutorial on the internet.\n\nWhy I'm saying is that,\n\n\n%3E You are the only Guru of yourself to get things very fast.\nBuy a book or collect from anywhere of Python 3. Yes, I'll repeat, go for Python 3. Please don't go for Python 2.\n\nI'll recommend you to take learn Python the Hard Way [ https://learnpythonthehardway.org/book/nopython3.html ] book as your guide. This is one of the best books in Python for beginners like you.\n\nStep 2: Download an app DataCamp - Learn R, Python & SQL - Apps on Google Play [ https://play.google.com/store/apps/details?id=com.datacamp ] from play store( If you are an Android user) or DataCamp - Learn R & Python on the App Store [ https://itunes.apple.com/us/app/datacamp-learn-r-python/id1263413087 ] from App store (If you are a Richie Rich)\n\nAnd keep doing practice on hand when you'll be free.\n\nStep 3: Stop watching some useless video and start watching some Python stuff on YouTube.\n\nAnd, that's it. After a month you'll see yourself as a Python programmer.\n\nYou've taken a great step, my dear mate. Go for it, because Python will tell you what you want to be after one month.\n\nHope it'll help you. Do follow me for a new programming relationship. All the best for your new beginning.",
                "Here is a Roadmap:\n\nWeek 1\n\nGoogle Python programming fundamentals and pick on any good site. Tutorialspoint or any other should be fine. This should take you about a week say 8 hours per day while doing small scripts for practice to enforce your understanding. Note: never memorize the syntax, it will come naturally and looking up each time when you need it until you finally internalize it is the way to go.\n\nWeek 2, 3, 3.5\n\nPick up a project and start working to finish it. Some general tips when working on the project:\n\n * Don\u2019t overthink, just pick on anything e.g basic calculator with a UI or could be anything that makes sense or google \u201cbeginner python projects\u201d or something.\n * You don\u2019t have to memorize or remember any syntax. So look up on the internet when you are stuck. Try to get hints on your problems.\n * Use an IDE (subjective but I prefer it since it is easier to debug and run scripts)\n * Breakdown the project to smaller components e.g if you are creating a calculator then:\n * \n * Graphical User Interface - further into various buttons\n * Addition, subtraction, division and multiplication functions assigned to various buttons (note: there is no golden rule on how to break down projects, as long as they are manageable chunks)\n\n * Stackoverflow is a good friend. But make sure you don\u2019t copy paste without understanding what the script does.\n * It will be hard and you need to be mentally strong to push through problems. When stuck on a problem:\n1. \n1. \n1. Look it up on google and try finding the solution by being more precise with your search queries\n2. If you don\u2019t find a solution then post it on a forum where you might find some help.\n\n\n * You need small breaks when programming. It helps to get away from the computer for a while and come back.\n * \n * May be this helps: Pomodoro Technique - Wikipedia [ https://en.wikipedia.org/wiki/Pomodoro_Technique ]\n\n * Invest time to learn version control. Should not take you more than a day or two. Basics should be fine. I prefer git.\n * Learn debugging as you go. Its not rocket science. Can take you literally take you only couple of hours to learn how to do it.\n * Ensure that you have a theoretical approach to solving the problem before you start writing any code. It won\u2019t magically give you the correct result. Step aside from wishful thinking and be more practical. You can draw diagrams or write down what you want to achieve the result.\n * If you get time try investing a few hours learning testing and implement some tests in your code.\n * Don\u2019t be frustrated if a problem takes long to solve but be aware of the time, you can move on to other components of the project to complete and come back later.\nWeek 3.5 / 4\n\nTry smoke testing and ensuring that bugs are fixed. Now, if you have not complete then its alright, many at times we might not depending on some factors. In that case:\n\n * \n * Allocate yourself more time (i.e more than a month)\n * Prioritize features that you might need for the project.\n\nCongrats you are better than many programmer that have been going through tutorials for a year claiming they have learnt a lot. May be even better than a big percentage of 1st and 2nd year CS undergrads.\n\nImagine the possibilities you keep building projects for the year!\n\nPS: To keep going you can start learning how databases work and learn some python frameworks to achieve what you want and build what you like.",
                "The easiest way to learn a programming language is to first learn the basics and then try to build something with it (learn by doing). And it's better if you are building something you are actually interested in rather than something out of a book because it will get you to think about the problem and be more meaningful. \n\nPython is easy to learn (not much syntax), easy to read (explicit vs implicit), has a big ecosystem (more packages/libraries), is taught at universities so it's easy to find good programmers to help, and is used by many large websites/companies (e.g., Quora is programmed in Python) so it's a good language to know.\n\nOnline Python Tutorials (in order from introductory to more advanced):\n\n1. \"A Byte of Python\" http://www.swaroopch.com/notes/Python\n2. Google's Into to Python Class (online) - http://code.google.com/edu/languages/google-python-class/\n3. \"Dive Into Python\", by Mark Pilgrim http://diveintopython.org/toc/index.html\n4. \"The New Boston\" Programming Python Tutorials - http://www.youtube.com/user/thenewboston#g/c/EA1FEF17E1E5C0DA\n5. \"Building Skills in Python\", by Steven F. Lott - http://homepage.mac.com/s_lott/books/python/html/index.html\n6. \"Think Python: How to Think Like a Computer Scientist\" - http://www.greenteapress.com/thinkpython/thinkpython.html\n7. \"Code Like a Pythonista: Idiomatic Python\"  -http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html\n8. OpenCourseWare: MIT 6.00 Introduction to Computer Science and Programming - http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-00-introduction-to-computer-science-and-programming-fall-2008/video-lectures.\n9. MIT 6.01 Course Readings (PDF) - http://mit.edu/6.01/mercurial/spring11/www/handouts/readings.pdf\n10. Google's \"Understanding Python\" (more advanced talk) - http://www.youtube.com/watch?v=HlNTheck1Hk\n11. \"A Guide to Python's Magic Methods\" - http://www.rafekettler.com/magicmethods.html\n12. \"Metaclasses Demystified\" -http://cleverdevil.org/computing/78/\n\nBook to Get: \"Python Cookbook\", by Alex Martelli (http://www.amazon.com/Python-Cookbook-Alex-Martelli/dp/0596007973/)\n\nAnd if you're building something Web based, look at using the Flask Web Framework (http://flask.pocoo.org/docs/).\n\nFlask is a modern, lightweight, and well-documented Python Web framework so you won't have to spend much time learning it or fighting with it -- you won't find yourself asking, \"Will I be able to do what I want in the framework without hacking it?\" Flask let's you program in Python rather than writing to the framework like you typically have to in larger, opinionated framework's like Django and Rails.",
                "1. First, know why python is called as python? This may sound silly but, interviewer sometimes ask this question(mostly for freshers)\n2. Know the difference between python 2.x and 3.x.\n3. Learn the different types of data types in python?\n4. Learn the difference between mutable and immutable. Difference between list and tuple\n5. list methods\n6. list slicing\n7. Dictionary - What is key, value in dictionary?\n8. Use of Sets/ forzensets\n9. range vs xrange, input vs raw_input\n10. loops\n11. File I/O\n12. Functions and classes\n13. What is args/kwargs?\n14. Built-in Functions\n15. Debugging and Error Handling (try...except)\n16. Regex\n17. list & dictionary comprehension\n18. Learn map, lambda, filter, reduce\n19. Iterator\n20. Generator\n21. Decorator\n22. Object Oriented Programming\n23. Inheritance\n24. Polymorphism\n25. Abstraction\n26. Encapsulation\n27. Duck typing\n28. classmethod and staticmethod\n29. deepcopy vs shallow copy\n30. External Modules\nYou must learn at least 1\u201316 points. These are really easy and must know for every python programmer. From 17 onward, complexity increases and it is good if you have at least basic idea.\n\nIt is really important to practice coding. \nYou may memorize answers however, you won\u2019t be able to answers follow up questions if you haven't practiced.\n\ncheck out the below answer for best python tutorials,\n\nPrafulla Kamble's answer to What is the best YouTube channel for learning Python? [ https://qr.ae/TWXRkY ]\n\nThanks for reading.",
                "Claim: You can\u2019t learn Python in one month.\n\nProof: If you can grok this extract in five minutes then yes, by all means you can learn Python programming in one month. But even reading this answer will take more than five minutes (let alone learning and practicing it), so we conclude that Python cannot be learnt in one month.\n\nWhat if you\u2019re an experienced programmer? Then you\u2019d not have asked this question. ;)\n\nChapter 0\n\nWhat do you think will be the result of dividing 7 over 5? Will it be a whole number or a decimal? In Python, a whole number is called an Integer (hereafter referred to as int) whereas a decimal is called a Float.\n\n[code]7 / 5\n%3E 1.4\n[/code]What about 10/5? Will the result be an int?\n\n[code]10 / 5\n%3E 2.0\n[/code]To our surprise, int division produced a float result. If we wanted to produce an int division, we should have done this:\n\n[code]10 // 5\n%3E 2\n[/code]The // is called floor division. here\u2019s why:\n\n[code]6 // 4\n%3E 1\n[/code]Because floor division must produce integers, whatever is fractional or decimal is cut down. In the case above, the mathematical answer should have been 1.5, but since // produces only an int, the 0.5 part was truncated, leaving only the 1.\n\nAlong the same topic, we also have % for remainder. If you divide 16 over 5, the result is 3, but the remainder is 1. In Python, % means mod or modulus, and it works like this:\n\n[code]16 % 3\n%3E 1\n[/code]The rest of the operations are:\n\n[code]1 * 2 \n%3E 2\n\n4 - 2 \n%3E 2\n\n(4 + 5) * 8\n%3E 72\n\n# raising to a power is done with **\n\n2**3\n%3E 8\n[/code]You must have noticed a comment (about raising to a power with **) that started with #. In programming, we write short comments within the code, so that we our future selves or others whose task is to read our code can make sense of some tricky and not immediately-obvious parts of our program. In Python, one-line comments start with #.\n\nSo far, we\u2019ve used the numbers freely. As soon as we computed some mathematical expression, the task was done. Sometimes, however, we need to save numbers in containers that should interact with other containers to produce increasingly-complex results.\n\nA container is merely a name that contains the data type\u2014here a number\u2014within itself.\n\nFor instance, let\u2019s measure some cube. The cube\u2019s height is 30 cm and its width is 15 cm, whereas its length is 40 cm.\n\nThus, we have three containers here: height, length and width. In python, locking a number within a container is called assignment. When you assign 10 to [code ]width[/code], you can then use width instead of typing 15 each time. Normally, a container is called a variable and that\u2019s what we\u2019ll say too.\n\nGiven the three variables, we can now compute the cube\u2019s \u201cVolume\u201d. Then we can save (assign) the result to a variable called [code ]volume[/code].\n\n[code]height = 30\nwidth  = 10\nlength = 40\n\nvolume = height * width * length\n[/code]So now that we have [code ]volume[/code], what can we do with it? In programming, the \u2018verb\u2019 which is something we do with variables goes by many names depending on the language. In Python, and more generally even, this is called a function.\n\nA function does something useful and can be already existing in Python\u2014called built-in\u2014or it can be made from scratch by us. Note that each function has a unique name and you must never assign to a custom function you wrote, a name that belongs to a built-in function.\n\nSuppose we wanted to inspect the contents of [code ]volume[/code]. The built-in function [code ]print[/code] will be enough:\n\n[code]print(volume)\n%3E 12000\n[/code]You also notice that our function print has an opening and a closing parenthesis. All functions in Python have this, even when there\u2019s nothing inside them. The thing inside the function\u2019s parenthesis is called function argument, and if it exists, then we can say a function will be a transitive verb whereas a function argument will be the object of that verb: in other words, the function \u201cdoes\u201d what it\u2019s supposed to do \u201cto\u201d the function argument.\n\nIn the jargon, we say \u201ccall a function\u201d instead of using a function, and adding an argument to a function is referred to as \u201cpassing an argument to a function\u201d.\n\nHere is another built-in function: round(). It\u2019s purpose is to round a decimal number. For instance, 900.9558 rounded to two decimals can be written as:\n\n[code]round(900.9558, 2)\n%3E 900.96\n[/code]Now we will turn to the last item before we have a scaffolding in place to jump into programming: strings.\n\nSimply, a string is something that is not considered a number and it is usually a letter, word, or a sentence. In Python, we use single or double quotes to represent a string. Here are some examples of strings:\n\n[code]\"j\"\n\"Jan\"\n\"I am Jan Meww, and you?\"\n\"39\"\n[/code]Note that \u201c39\u201d is not a number or not considered a number because of being enclosed in quotes. The main thing that differentiates a number from a string is this: when you type a number in the console (interactive session), Python will automatically accept it as a number, but if you type a word or a letter, Python will throw or declare an error.\n\nNow you\u2019re ready. If you recall from high school, a Fibonacci sequence is sequence of numbers such, that as long as our number is more than 1, the next number is a sum of the previous two:\n\n[math]0, 1, 2, 3, 4, 5[/math]\n\nThus, [math]0+1 = 1, 1+1 = 2, 1 + 2 = 3,  2 + 3 = 5,  3 + 5 = 8[/math], and [math]5 + 8 = 13[/math] and onwards. Note, as we add [math]1 + 2 = 3[/math], we take the two numbers on both sides of the equal sign and add them up: [math]2 + 3 = 5[/math]. Now the two numbers on both sides of the = are [math]3[/math] and [math]5[/math], so we add these up, and so forth.\n\nProgramming is giving well-defined instructions to a computer so that there are no ambiguities that might lead the computer to make a mistake. On the contrary, among humans, instructions can be ambiguous and still understood because our body language, tone, cultural references, and the context can provide plenty of cues to our minds.\n\nThere\u2019s a well-known joke (well-known among programmers, though) about this:\n\n\n%3E A woman asks her husband, a programmer, to go shopping:\n\n-Dear, please, go to the nearby grocery store to buy some bread. Also, if they have eggs, buy a dozen.\n\n-OK, hun.\n\nTwenty minutes later the husband comes back bringing 12 loaves of bread.\n\nHis wife is flabbergasted:\n\n-Why on earth did you buy a dozen loaves of bread?\n\n-They had eggs\nThe joke reflects how computers think and how programmers are affected by that precise level of thinking. This is not such a bad thing as the joke implies; in fact, the art of providing clear, concise instruction to a computer will sharpen your brain and also fine-tune it to detect weak arguments.\n\nThe problem in the above joke is that the wife said: \u201cif they have eggs, buy a dozen\u201d. For humans it is easy to see that the comma in the sentence above implies that the dozen is connected to the eggs, but computers cannot infer commas. A computer deals with sentences and instructions as they are, which makes it a little difficult to get used to at first.\n\nRecasting the same joke to make a computer understand it as we do should have resulted in:\n\n\u201cif they have eggs, then in addition to buying some bread, buy a dozen eggs, too\u201d.\n\nEven this has a problem, because the computer won\u2019t buy bread if the shop did not have eggs. We can rephrase it:\n\n\u201cFirst, buy some bread. Then, if they have eggs, buy a dozen eggs, too.\u201d\n\nNow this is unambiguous.\n\nReturning to the Fibonacci, our Fibonacci sequence is well-defined because it contains no ambiguities and the computer can make sense of it; we say \u201ctake two numbers [code ]a[/code] and [code ]b[/code], turn [code ]a[/code] to [code ]b[/code], and turn [code ]b[/code] to [code ]a+b[/code]\u201d. And most likely we will also add a stopping point so that it does not go on forever: \u201ccontinue to do this up to some limit.\u201d\n\nHow would we write this as code? We need a proper way to inform Python that we want to add the previous two numbers and present them in a sequence. This requires a loop.\n\nA loop is a programming concept that can be approximately called a \u201cdo this during this duration\u201d. One loop we will see now is called [code ]while[/code]. So let\u2019s first see this program and then dissect it:\n\n[code]a, b = 0, 1\nwhile a %3C 20:\n\ta, b = b, a+b\n\tprint(a)\n[/code]You will have noticed that we can assign several variables at the same time. We assigned [math]0[/math] to [code ]a[/code] and [math]1[/math] to[code ] b[/code] at the same time, separated by a comma. Next, we instruct Python that as long as [code ]a[/code] is less than [math]20[/math], do the following tasks. The colon at the end of [math]20[/math] is represents the concept of \u201cfollowing tasks\u201d.\n\nAnother thing you noticed after the colon is that the code automatically will go 4 spaces to the right (in most Python editors it does this automatically. If it does not, you have to do this manually yourself, but in practice this is done automatically to prevent errors in miscalculated spaces).\n\nMore specifically, this spacing is called indentation, and the four-space indentation is unique only to Python; because Python cares about the white space around the code, the core idea behind indentation is to make the code easier on the eyes, especially if we have long pages of code.\n\nNext, as long as [code ]a[/code] is less than [math]20[/math], we will start by assigning the value of [code ]b[/code] to [code ]a[/code], and the value of[code ] a+b[/code] to [code ]b[/code]. This is very similar to adding the two numbers on the two sides of the = sign I showed earlier. If [code ]a[/code] was [math]2[/math] and [code ]b[/code] was [math]3[/math], then the next [code ]a[/code] will be [math]3[/math] whereas the next [code ]b[/code] will be [math](2+3)[/math] or [math]5[/math]. Thus, for the next round, the next [code ]a[/code] will change from [math]3[/math] and become [math]5[/math], while the next [code ]b[/code] will become [math](3+5)[/math] or [math]8[/math], and so forth.\n\nFinally, we also instruct Python to display all the values that [code ]a[/code] takes during its [code ]while[/code] journey:\n\nNow that you\u2019re exposed to the [code ]while[/code] loop, another important foundation concept is the conditional; in Python, the conditional is [code ]if[/code], and acts like \u201cif\u201d as humans use it. Suppose I am 40 years old, and one of my acquaintances is Python. If we were to ask Python whether Jan is 40, we would make use of [code ]==[/code] which stands for \u2018equal to\u2019. Let\u2019s see what Python says about this:\n\n[code]# first we make sure Jan is 40\njan = 40\n\n# then we check whether this is really so\njan == 40\n%3E true\n[/code]Python agrees. What about:\n\n[code]Jan == 40\n[/code]Will it evaluate to [code ]true[/code] as well? No, because variable names are case-sensitive. jan and Jan are not the same thing.\n\nI also want to mention that assigning a value to a variable, say:\n\n[code]Meww = 40\n[/code]And then assigning 39 to Meww will result in Meww becoming 39, because that is the last value we assigned to Meww; A container can only contain one value and when we assign 39 to Meww, 40 has to go. (so that 39 can take its place.)\n\nBut go where? Python automatically clears the space for the newly assigned 39, and 40 goes to the virtual \u2018thrash can\u2019. In the jargon, this is called automatic garbage collection, a concept Python borrowed from Lisp. (And not all languages have this.)\n\nNext, you will see an error that was caused by mistyping. This is the most frequent cause of beginner level mistakes in code:\n\nAs you can see, Jann is something unrecognized by Python, and it will mention this to us in its error message, even though, Python\u2019s error messages are rather cryptic.\n\nNow, let\u2019s use the conditional[code ] if[/code]. If Jan is 40, Python should say something more than true; perhaps we can ask it to say: \u201cCongrats, you\u2019re still young!\u201d:\n\n[code]Jan = 40\nif Jan == 40:\n\tprint(\u201cCongrats, you\u2019re still young!\u201d)\n[/code]That is it. Now what happens if Jan is not 40? We can also insert additional conditional clauses in there: suppose we wanted this: if Jan is 40 then print \u201choorah\u201d and if Meww is 39, then print \u201ccongrats\u201d.\n\n[code]Jan, Meww = 40, 39\nif Jan == 40:\n\tprint(\"hoorah!\")\nif Meww == 39:\n\tprint(\"congrats!\")\n[/code]The conditional does not stop there. We can add further conditions. Suppose you instruct your robot to go to the Mall and buy milk and eggs. You could say something like this:\n\n[code]# define the variables:\n\neggs = \"eggs\"\nbread = \"bread\"\n\nif eggs == true:\n\tbuy(eggs)\nelse:\n\tbuy(bread)\n[/code]First, we instruct the robot to recognize eggs and bread when it sees them. In the solution space\u2014which is the page where we write our Python code\u2014this means assigning strings \u201ceggs\u201d and \u201cbread\u201d to the variables of the same name, so that we can use those names directly, without attaching quotes to them each time. This enhances readability but also prevents errors.\n\nNext, we add a conditional which is the equivalent of \u201cif eggs are available in the Mall\u201d. In Python, existence and availability is synonymous with [code ]true[/code]. If something is [code ]true[/code], then it exists and can be used.\n\nThe final step was instructing the robot what to do; we learned earlier that doing something was a function, and functions look like this: [code ]function(argument)[/code] if transitive but also [code ]function()[/code] if intransitive. While the syntax of our code above is correct, the function [code ]buy(something)[/code] is not already defined in Python, and the robot will not recognize it. Thus, we will also have to define\u2014create\u2014this function and include correct and clear instructions in the body of this function so that Python can execute it according to our wishes.\n\nA function is made with a keyword [code ]def[/code], a function name, and a body. Here the function name is [code ]greet[/code], and the body\u2019s task is to say \u201cHello!\u201d.\n\nLike so:\n\n[code]def greet():\n\tprint(\"Hello!\")\n[/code]This is an argumentless function (intransitive), so when we call (access or put it to use) it, it must also be argumentless:\n\n[code]greet()\n%3E Hello!\n[/code]We can also make functions that take arguments\u2014what I earlier termed transitive. Suppose the robot will greet by name. Thus, the name of the person it greets is the argument:\n\n[code]def greet(name):\n\tprint(\"Hello!\", name)\n[/code]Let\u2019s see it in action:\n\n[code]greet(\"Jan\")\n%3E Hello! Jan\n[/code]Note that the placeholder argument [code ]name[/code] must be a string, since names are made of letters\u2014and not numbers. This is why Jan was written as [code ]\u201cJan\u201d[/code]. But we could have done this too:\n\n[code]greet(5)\n%3E Hello! 5\n[/code]This is both a strength and a weakness in Python but beyond the scope of this post. Consider this, though: in certain languages, like Julia, argument type can be specified at the time we define a function, but as of this writing, not in Python.\n\nSo when you define\n\n[code]def greet(name):\n  print(\"hi\", name)\n[/code]In Python, you cannot force-specify what type of input name could be. This leads to more flexibility as you saw earlier, but also to potentially hidden bugs or errors.\n\nFinally, note the comma between name and \u201cHello!\u201d. This is the first time you see [code ]print()[/code] taking two arguments separated by a comma. However, the ability of a function to take n arguments is restricted by its construction.\n\nThus, [code ]print()[/code] is built-in and constructed to take more than one argument, whereas [code ]greet()[/code] is not because we only passed [code ]name[/code] as the maximum number of arguments it should accept.\n\nHow did you do? How long did it take?\n\n\nAll the best,\n\nJan",
                "Consider Hello World program in C,JAVA and PYTHON\n\nC CODE\n\n[code]#include %3Cstdio.h%3E\nint main()\n{\n\n   printf(\"Hello, World!\");\n   return 0;\n}\n[/code]JAVA CODE\n\n[code]public class HelloWorld {\n\n    public static void main(String[] args) {\n     \n        System.out.println(\"Hello, World\");\n    }\n\n}\n[/code]PYTHON CODE\n\n[code]print \"Hello, World\"\n[/code]The major difference between all three is syntax and Python is easy to understand compared to other languages.",
                "I've been learning python for about six months now. Not spending 8 hours a day on it but more as a hobby. \nI started less than a year ago with html, css and javascript. So I did start python with some solid knowledge on what code and programming is about. \nOf course, when starting python, I fell in love with the syntax and simplicity this language offers. It also allowed me to easily learn some complex programming concepts that I am able to use in other languages I sometimes have fun with (for example VBA). \nAfter six months of python I have had a few projects going on and here are the major things I worked on, chronologically:\n\n1. I created a bot that plays an online game for me. It works recognizing images on the screen and clicking where appropriate. This was a great experience as it was the first \"big\" program I ever wrote.\n2. Raspberry pi: working on this one with my brother. We have had different projects. Starting from lighting up lights to a self driving car:\n1. \u200b\n\n\u200b\u200b\u200bour next goal with this one is to add a camera and be able to control the car from a website.\n2. OpenPyxl: I use a lot of Excel for work. VBA is a great language but I like being able to do things through python, completely in the dark and once I open excel it magically copied and compared data from different files. So many things happen \"underneath\" that I haven't been able to do with VBA.\n3. Django: is what I try to focus on the most more recently. If I want to one day work in the tech industry I believe Web Development is the best route for me to start in. Django is really powerful and starts making sense (I have to say the beginning seemed a little overwhelming because there are a lot of things that need to be understood, as well as the links between those things) to me. I think the fact I started with html, css and javascript is really going to help me out in Django projects so I'm looking forward to the future things I will create.\nAfter 6 months I'm pretty happy with the python level I have reached. I hope I'll manage to double this knowledge in the next six months. And be able to apply as much as possible in my day to day life (current job, personal apps, other projects) on order to keep it as exciting as it has been.\n\nI hope this answer gives you an idea of what the next few months might look like for you.",
                "You may be comfortable with the language in an hour or two. depending on what you want to do with it, expect to feel almost as if you\u2019ve mastered it in only a week. There\u2019s a lot to learn about any language, but the basics of python are so obvious that you may guess some of them without looking them up.\n\nthe language is a miracle - there isn\u2019t another language like it in the world. it is regarded as the fastest language for software development purposes, because everything falls into place like magic. it\u2019s object oriented programming without all the crap.\n\nthe language works with you, not against you. as soon as you use python, you realize that all your life you\u2019ve been telling computers how to do things they should have been able to figure out for themselves, and it\u2019s 90% of all the code you\u2019ve typed.\n\ngetting a range from an array is like array[15:30]\n\nand getting every other item of it would be array[15:30:2]\n\nyou get input with variable = input(\u201center text: \u201c)\n\nand you print it with print(variable)\n\nand you never import anything, or instantiate anything, or inherit anything, while performing these simple tasks.\n\nbut when you want to import something, you do it with import something whether it\u2019s part of python, a library you installed by simply copying it from your downloads folder to your python directory, or any file in the same directory you\u2019re in, it just works.\n\neverything just works.\n\nyou don\u2019t need to accept the dumbness of computers as reality ever again. everything that has ever annoyed you or inconvenienced you will not occur in python.",
                "Computing and programming are mostly about abstraction and code. Most of the time you'll be transforming data from its existing state to its desired state and translating expert/business knowledge into rules / heuristics that computers can follow.\n\nYou get to build complex things as many layers of less complex components. You also need to find viable and efficient ways of representing the relevant properties of things in terms of properties of abstractions you already have, that's the \"codification\" part. E.g: a person's gender gets represented as an 'M' or 'F' ASCII character in a CHAR field of relation table or each player in a MMO game gets represented by a single process in an application server cluster.\n\nThat's why in computer programming we are always talking about \"platforms\" since we're generally building custom/specific solutions on top of existing general/horizontal ones and we call this solutions \"applications\" since we are applying a generic technology to an specific problem. \n\nHaving said that you'll be much more competent as a technologist / developer if you are familiar with a large number of the layers / components / platforms / frameworks   involved in the building process and life-cycle management of your product.\n\nIf someone is already reasonably-competent with a high-level scripting language like Javascript or PHP, has a basic but good understanding of the Object-Oriented principles, knows the ways around installing software in a computer, knows the basic of SQL and relational databases, has a basic idea of networking in a Internet setup and understands at least the basics of a network protocol like HTTP or TCP sockets I think he or she could be writing reasonably good Python web apps, simple IoT projects or automating operations stuff at a commercial level in matter of weeks, with viable prototypes even in days.\n\n For someone who needs to pick the basics of programming at the same time and then learn their way around the minimum set  of skills to work with a Linux-based VPS on the cloud, work with a relational database, get started with networking protocols, learn to use OOP effectively and all sort of basic computer science skills required to put together viable products we're talking months full time, possible a couple of years part-time or on weekends. This does require a lot of persistence, help from friends, peers or people from the internet and specially very good instincts to work around problems.\n\nAverage day to day professional software development is rarely about seriously complex things but mostly about a LOT of very simple yet interdependent concepts and techniques you need to get used to.",
                "Sounds like you are setting out to learn at least 2 things. the Python language, and computer programming. The Python language is reasonably compact and quite self-consistent. A few weeks study, if you already know programming in, say, Java, should be quite sufficient to know the language, but there's quite a bit more for you to learn. When I took Udacity CS101, it was nominally 8 weeks long. I understand they have since expanded it and now call it a 3-month course.\n\nIn my opinion, it's a great introductory course. See Is the Udacity CS101 course watered down? [ http://rdrewd.blogspot.com/2012/12/is-udacity-cs101-course-watered-down.html ]\n\nSuggestion: Take a look at Udacity CS253 as a next course after CS101. I expect it to get you much further into the nuts and bolts of building a web site (but I confess, I haven't tried CS253 myself). Web Development Online Course [ https://www.udacity.com/course/web-development--cs253 ]",
                "I would say it varies from one to one. One can\u2019t definitely say you can be very much comfortable with python within 6 months or 6 weeks. In case of programming it is fully depends on your learning capability. If you are enjoying on a daily basis what you are learning thus will automatically boost up your level of confidence and will surely take less time to learn. Or in another case like most of the people, programming may seem to be boring one to you as level of dedication it takes to mastering in programming may not be presented in you. In that case, either it takes long period of time to learn programming or you may end up without learning any language.This is the overall scenario of learning any language.\n\nLearning python, i must say is a very much smart approach as it is going to be the most powerful language not only in 2018 but also in upcoming years as the field of big data and data science are widely spreading. As it is comparatively easy to learn than other languages most of novices start their journey with this as i did.\n\nIt took me 3 months to learn ins and outs of python as i practiced on a daily basis approx. 2 hours a day . I used to practice 2 hours in a day . Never missed that deadline. Probably that reason helped me to complete the overall course within 3 months. Also i had simple programming knowledge before starting python but i am saying in that issue, that played a minimal role. All i need is to know what i am going to do with python and by learning python how can i implement python in my related field. Once you get the motivation and know what to do by using tools yo intend to learn i would say then your self motivation lifts you up and your self passion ties you till end of completing task.\n\nWelcome to python family!!\n\nPS: the books i used to learn\n\n1- Python for Data analyis\n\n2-Learn Python the hard way\n\n3-Learning Python\n\nOnline tools\n\n1- Codeacademy\n\n2- Welcome to Python.org [ http://Python.org ]\n\n3- Google\u2019s pyhon e book\n\n4- Learn Pyhon. org\n\n5-Python.org",
                "How people use Python from my own experience (12 years) and what i have seen in others.\n\n * After a couple of weeks to months: Being comfortable enough in the syntax and know most of the common features to solve your problems\n * After a couple of months to first few years: Oh wow, look at all those awesome crazy dynamic features Python has and neat tricks you can do \u2014 lets apply them everywhere!\n * After those years: Staying away from those awesome crazy dynamic features and neat tricks except in the ~1% of cases where they actually make sense to use.\nThose number will vary vastly depending on prior programming experience. And there is a variant in step two and three with people coming from other languages trying to force their ways on Python programs. Actually I think some forms of those 3 steps are there in any language.\n\nMastering a programming language is not terribly important, mastering programming is.",
                "I think most answers confuse \u2018basics\u2019 with \u2018mastering\u2019. So let me clarify few things\u2026\n\nBasic - understanding key concepts of data flow (if, loops, etc.), data structures, oop, exceptions and few others.\n\nAdvanced - Being able to navigate freely through the Standard Library and being able to use built-in libraries to their full potential. Being able to use documentation and if needed looking at the source code of the library being used and figuring it out based on code only. Having few large-scale projects (private or commercial) under your belt.\n\nMaster - Navigating freely in the whole Python ecosystem, being able to create own libraries, APIs, contributing to development of python through PEP. Thoroughly understanding each concept and programming paradigm (functional/procerdural/oop) and being able to apply it in real-life scenarios. Knowing how Python interacts with underlying OS resources and ability to optimize code for any platform. Being able to approach any problem and leverage built-in or other libraries. Deep understanding would also incline a master to teach others, but it\u2019s not a path for everybody.\n\nEven though it\u2019s true that Python has much shorter learning curve than let\u2019s say Java or C/C++ (I\u2019ve actually learned all of them through the years, not on master level though!) it is still powerful and huge concept to fully grasp. Also, have in mind that programming is not some kind of isolated activity but it\u2019s just one layer of multi-layered modern computer systems architecture. I would argue that you will not be able to become a master in any programming language without understanding operating systems, databases, algorithms, code optimization, testing, code deployment, at least basics to advanced in: hardware architecture, security, networking, and few others.\n\nTo answer your question I will take into consideration two most common scenaios:\n\n1. You want to become a self taught python developer, but you currently have another job so have only max 4\u20135 hours/a day to learn:\n2. \n1. Basics: 2\u20133 months\n2. Advanced: 1\u20135 years\n3. Master: 5+ years\n\n3. You don\u2019t have a job or other significant time commitments and can spare 10\u201312 hours/day on learning:\n4. \n1. Basics: 1\u20132 month\n2. Advanced: 6 months\u20132 years\n3. Master: 2+ years\n\nThis timelines are approximate. Exact timelines will be based on how quickly you learn in general and if you have any prior understanding of computers at all.\n\nAlso bear in mind, that I haven\u2019t mentioned fields like Data Science here. There is a reason for that. This field is huge in itself and becoming good at it requires different skillset and knowledge (maths, data viz, parallel or GPU computing, etc.) so my answer above is only about becoming a master in Python language.\n\nMost of all, please don\u2019t believe that you need some expensive course or university to learn Python (as some answers may suggest). The only thing you need is a basic computer, internet connection and a whole lot of dedication and hard work.\n\nHope this helps!",
                "Been programming in python for 2 years. No where near mastering it.\n\n * In general Natural Language processing, I use NLTK for lemmatizing to get root words.\n * In my fMRI project on brain image analysis, I use obscure libraries like nibabel, fmri, mvpa for performing multivariate feature analysis.\n * In my speech processing, I needed to downsample my mp3 recordings using ffmpg.\n * In data gathering for most of my data science projects, I need to scrape data from different online sources. I use BeautifulSoup.\n * In my deep learning projects for object recognition, I use Tensorflow and Keras.\nThere is no way I can make use of all these cool libraries without pulling up the docs. They work so fundamentally different that all the python I know doesn\u2019t make a difference. Heck, I still even google basic syntax. Yeah, I doubt I\u2019ll be mastering this anytime soon.",
                "I've been learning python for about six months now. Not spending 8 hours a day on it but more as a hobby. \nI started less than a year ago with html, css and javascript. So I did start python with some solid knowledge on what code and programming is about. \nOf course, when starting python, I fell in love with the syntax and simplicity this language offers. It also allowed me to easily learn some complex programming concepts that I am able to use in other languages I sometimes have fun with (for example VBA). \nAfter six months of python I have had a few projects going on and here are the major things I worked on, chronologically:\n\n1. I created a bot that plays an online game for me. It works recognizing images on the screen and clicking where appropriate. This was a great experience as it was the first \"big\" program I ever wrote.\n2. Raspberry pi: working on this one with my brother. We have had different projects. Starting from lighting up lights to a self driving car:\n1. \u200b\n\n\u200b\u200b\u200bour next goal with this one is to add a camera and be able to control the car from a website.\n2. OpenPyxl: I use a lot of Excel for work. VBA is a great language but I like being able to do things through python, completely in the dark and once I open excel it magically copied and compared data from different files. So many things happen \"underneath\" that I haven't been able to do with VBA.\n3. Django: is what I try to focus on the most more recently. If I want to one day work in the tech industry I believe Web Development is the best route for me to start in. Django is really powerful and starts making sense (I have to say the beginning seemed a little overwhelming because there are a lot of things that need to be understood, as well as the links between those things) to me. I think the fact I started with html, css and javascript is really going to help me out in Django projects so I'm looking forward to the future things I will create.\nAfter 6 months I'm pretty happy with the python level I have reached. I hope I'll manage to double this knowledge in the next six months. And be able to apply as much as possible in my day to day life (current job, personal apps, other projects) on order to keep it as exciting as it has been.\n\nI hope this answer gives you an idea of what the next few months might look like for you.",
                "Want to Master Python?\n\nOn average, learning Python basics takes roughly 8 weeks. Basic syntax, links, if statements, loops, variables, functions, and data types are all covered. How quickly you adapt to the Python language basics depends on how much time you spend learning the language, just as it does with any new programming language. The length of time you spend learning is determined by your schedule, as well as what you want to learn and how much you want to learn.\n\nLearning the fundamentals takes roughly 6-8 weeks on average. This gives you ample time to learn the majority of Python code lines. If you want to become an expert in Python and its field and work in data science, you'll need months or years of study.\n\nThe online, as well as classroom courses, claim that someone can learn Python in one or two months, and that is just the beginning; learning Python completely is a difficult and impressive feat that should not be underestimated.\n\nLearning basic Python as a working professional can take a lot longer than learning it as a student. The best way to learn is to join a coding Bootcamp.\n\nAnother option is to commit five months to learn Python. For those of you who work full-time, this is for you. It must be planned to spend 2-3 hours a day on the computer. One day you'll learn, and the next day you'll put what you've learned into practice.\n\nThis should be done daily to guarantee that you learn consistently and effectively. Complete data science online certification will assist you in mastering python as well as other tools and languages required to compete in this fast-paced business.\n\nLearning Python necessitates commitment and honesty. The learning process is time well spent because it significantly enhances your work prospects.\n\nPython Learning Levels\n\nLearning Python has its own levels.\n\n * Elementary Python\n * Advanced Python\n * Professional Python\nYou can learn syntax, keywords, data types, functions, classes, and other things by learning simple Python. An intermediate coder can learn this in around 8 weeks.\n\nAdvanced education Multithreading, data synchronization mechanisms, socket programming, socket programming, database programming, and so forth are all included in Python. The nature of the work would determine this. The amount of time it takes to learn these skills is entirely dependent on the learner's ability level.\n\nProfessionals in training Data analytics, executing libraries/packages, image processing, and other ideas are all part of the Python language. These are highly sophisticated technology. These strategies can be learned through both online and offline training. Again, depending on the level of the information, learning these complicated procedures can take anywhere from two weeks to a month.\n\nIf you are curious about learning python, data science to be in the front of fast-paced technological advancements, check out Mastering Python Program on Ekeeda website and upskill yourself for the future.\n\nYou can also check out the \u201cMastering Python\u201d Playlist on YouTube Posted by Ekeeda Channel!\n\nhttps://bit.ly/3IEQyfM\nHope this helps, All the best :)!",
                "Like others have said, mastering Python doesn't take long. You could learn enough to create something useful in a week, contribute to other simple code in a month, and have a solid advanced grasp of the language and its concepts in six months to a year.\n\nBut that won't make you a good programmer.\n\nProgramming is about more than just writing code in a language. It's about understanding the idioms of the language and figuring out what the rules, best practices, etc, are. It's about understanding the why's of a language as well as the how's."
            ]
        },
        {
            "tag": "artificial_intelligence",
            "patterns": [
                "Is AI an existential threat to humanity?",
                "Is AI an existential threat to humanity?",
                "I want to be an artificial intelligence engineer, which should I study, computer science or software engineer?",
                "Should I choose artificial intelligence or software engineering?"
            ],
            "responses": [
                "Sure, Not.\n\nYou know\u2026 the term \u201cArtificial Intelligence\u201d is largely overused. So much, that it become an empty buzzword. Seven IFs in JavaScript, on your webpage, will be called \u201cAI\u201d by the marketing company\u2026 because it sounds better. The term is totally spoiled.\n\n",
                "First\u2026Stop it.\n\nArtificial intelligence is not \"intelligence\". And it's not \"artificial consciousness\".\n\nEveryone is afraid that AI will suddenly wake up, get upset, and take over the world.\n\nOr that AI will wake up and take all of our jobs. This will happen. But without the \"wake up\" part.\n\nBelow I describe what real AI is.\n\nIf we want to understand the \u201cexistential threat\u201d we first need to know what AI is.\n\nThen, if you are at a cocktail party and someone says, \"but what if robots are intelligent?\" you can argue with facts, mixed with a little bit of alcohol.\n\n---------\n\nA) STATISTICS\n\nStatistics is at the heart of most AI programs.\n\nJust like statistics is at the heart of a lot of human decision making.\n\nFor instance, if you see clouds in the sky, your brain thinks: \"Hmmm, the last 100 times I saw clouds this dark, it usually meant it was about to rain\".\n\nWhen you think like that, you are using statistics to make the decision: \"I should _probably_ go inside now.\"\n\nI'll give an AI example: Siri or Alexa. How does Alexa understand the words you just said?\n\nIn 1989 I was visiting Carnegie Mellon to decide if I would go to graduate school there.\n\nOne of the graduate students, Kai-Fu-Lee (now one of the most famous investors in the world and I would check out his excellent recent TED talk on AI) showed me what he was working on:\n\nIt was speech recognition for the 60 or so commands that might happen on a Navy battleship (ten guesses as to who was funding his project).\n\nWhen you say the word \"Fire!\" a sound wave is created. When you say the word \"hello\" a sound wave that looks different is created.\n\nIf 100 people say \"Fire\" and 100 people say \"hello\", all of those sounds waves are stored in a database.\n\nNow, if a brand new person says \"Hello\" the computer program needs to determine if that person said \"Hello\" or \"fire\".\n\nThere might be 10 different attributes of every sound wave. It breaks the new person's sound wave into those 10 attributes.\n\nThen it compares that \"vector\" of 10 attributes with all of the vectors in its database for \"Hello\" and \"Fire!\"\n\nIt uses a statistical technique called \"Hidden Markov Analysis\" to determine if the sound wave is more like the \"hello\"s in the database or more like the \"Fire!\" in the database.\n\nThen it says to itself, \"This guy said \"Hello\". \"\n\nIt then has a line of code that says, \"If someone says \"Hello\" Then say \"Hello\" back\".\n\nAdditionally, it adds your \"Hello\" to its database.\n\nYour \"Hello\" might be slightly different than the other 100 \"Hello\"s so it just learned a new way to say \"Hello\". That gives it greater ability in the future to recognize the word \"hello\".\n\nIn other words, it \"learned\".\n\nSo it used Statistics to hear you, code to respond to you, and database technology to learn. There's no real intelligence there but it feels like it's intelligence.\n\nMultiply that by 30 years and millions of patterns and computers a million times faster and you have Alexa and Siri in today's kitchens.\n\nAsk \"Siri\" what gender it is.\n\n-----\n\nB) EVALUATION FUNCTION\n\nI just mentioned about language recognition. But how does a self-driving car work?\n\nEvery second it has to make a decision. Does it move forward? Does it brake? Does it swerve to avoid an accident? Does it turn left?\n\nHow does it get from point A to point B?\n\n1) Google Maps. - Using GPS it knows where it is. And it puts itself on Google Maps.\n\n2) List all of the possible routes. This is a \"hard\" problem in the mathematical sense (there's no way for it to guess the fastest route. It has to list each route and then sort by the shortest. )\n\nBut now computers are so fast what would normally be a slow decision (drive me from this corner in Piscataway, New Jersey to the capital building of Sacramento, California) now just takes seconds.\n\n3) Waze. Use Waze to eliminate the routes with too much traffic.\n\n4) Start driving.\n\n5) Statistics: Every microsecond it uses statistics to see if there is blank space or an object that must be avoided or a traffic sign that must be followed.\n\n6) Decide what to do according to the code. For each traffic sign, it has code that tells it what to do (if a sign says \"Stop\" it Stops for a second, uses Statistics to see if any traffic is happening on its sides (with radar and cameras to provide the images). )\n\nIf there is a person standing in front of it, it might just stop.\n\nIf there's traffic it didn't expect, it might trigger the program to re-route.\n\nIf it's blank space it will just keep going.\n\nIf there's a baby crossing the street and it has to swerve to avoid hitting it, but if swerving will cause the car to hit a truck, killing the passenger in the car, then the \"AI\" of the car is dependent on the ethical decisions of the programmer of the car.\n\nIn other words, in every situation, it determines it's options, then uses an \"evaluation function\" programmed by a coder, to determine which option has the most successful outcome (move the trip forward, don't kill anyone).\n\nEventually the evaluation function will NOT be programmed by a human coder.\n\nInstead, through thousands of experiences of other self-driving cars, the experiences plus the outcomes will all be put into a central database.\n\nWhen a new experience is encountered, the code will look up that experience in the database and the database will spit back the best possible outcome.\n\nThe code will learn statistically what the best outcomes are of each possibly decision and change the code accordingly and send updates to all self-driving cars.\n\n-----\n\nC) TREES\n\nThe hardest game in the world is a board game called GO. With chess, if a computer can evaluate a billion possibilities a second, it can be a world champion level player.\n\nBut a Go game can involve trillions of possibilities. How did Google make a program, Deep Go, to beat the world's best Go player. This was thought to be impossible.\n\nAnd yet Google did it.\n\nFor any game, a computer program first builds a tree of possibilities. Much like a human would.\n\nA human thinks: \"If I make this move in checkers, my opponent might respond with A, B, or C and then I can do D, E, or F and then my opponent can do G, H, I if I do D or it can do J, K, L if I do E and I'm never going to do F.\n\nA computer doesn't select as well as a human so it builds the FULL tree. Meaning, what are ALL of the possible moves it can do, what are ALL of the possible responses of my opponent, etc.\n\nAnd then it uses a programmed evaluation function to look at the leaves of the tree it built.\n\nWhichever move results in the best leaf of the tree (as determined by the evaluation function) that is the move it makes.\n\nThat's how computer chess worked for decades. I'll get to the secret sauce in a second for how computers conquered chess.\n\nAnd then after that I'll describe how computers miraculously conquered Go.\n\nIt's only a miracle until science can explain it. It's only \"intelligence\" until it can be coded by a programmer.\n\nD) HARDWARE\n\nEverybody thought for decades (including many Nobel Prize winners) that the best computer chess programs would be developed when scientists encoded the knowledge of the best chess players in the world into the evaluation function.\n\nHow does the world champion value a position instead of a weak player?\n\nThis turned out to be wrong.\n\nThe MORE code in the evaluation function (i.e. the \"smarter\" the evaluation function was from a human perspective) the SLOWER the program.\n\nWhich meant a smaller tree would be built, which meant less possibilities would be analyzed.\n\nWhat really allowed the programmers at IBM to build \"Deep Blue\" which beat Garry Kasparov in 1997 were two things.\n\nBoth related to hardware.\n\na. Computers got faster. \nb. First the creators of Deep Blue developed software. But then they made the software into hardware, building the logic right into the hardware infrastructure of the computer. Making the program 100x faster than it would have been.\n\nAnd finally, they made the evaluation function STUPID in order to use less code so the hardware could value more positions.\n\nThen, before anyone caught on to their \"artificial intelligence\" they retired Deep Blue right after it beat the World Champion of chess.\n\nAs hardware gets faster, artificial intelligence gets \"smarter\".\n\n[as an aside, I once gave a date a chip that was the initial chip for \u201cChip Test\u201d - the \u201cancestor\u201d of what became the best chess computer, Deep Blue. She was weirded out.]\n\n----\n\nINTERLUDE\n\nWhat I just described is all the basics. You can stop now.\n\nThe rest of artificial intelligence is simply combining the basics to make more advanced techniques.\n\n-----\n\nE) STATISTICS + TREE\n\nRemember the TREE from computer gaming. And STATISTICS from speech recognition.\n\nNow let's go to the impossible game of Go. Google developed the program \"AlphaGo\" to win at Go when everyone else thought it would take another 20 to 50 years.\n\nFirst, remember Kai-Fu Lee who worked on speech recognition. And later developed Apple's first attempts at speech recognition in the 90s?\n\nAt one point in his grad student days, he was getting tired of navy battleship commands (as one does) and decided to focus on building a program to play Othello.\n\nHe ended up building the world champion of Othello.\n\nHe took a lot of games, let's say a million, and put them in a database. And each position from each game, he would label, \"winning\" (if it was a position on the winning side) or \"losing\" in a massive database.\n\nHe would identify several attributes of each position (how many white pieces, versus black pieces, how many corners were controlled, how many pieces were on the sides, etc).\n\nNow, if the computer was playing a brand new game, it would determine all the attributes of that position, then use Hidden Markov Analysis (remember: speech recognition) to match that position to the database.\n\nIf the position pattern-matched a \"winning position\" then it would make the move that would lead to that winning position. If it matched a \"losing position\" it would not make that move.\n\nThat program became the world champion of Othello.\n\nAlphaGo took it one step further.\n\nIt put in the positions of millions of Go positions and did the same sort of breakdown.\n\nIt used faster hardware to speed up the process.\n\nThen, once it became pretty good at GO, it played BILLIONS of games against ITSELF to put many BILLIONS of new positions into the database. In other words, it \"learned\".\n\nNow it was ready to play Go. It crushed the world champion\n\n------\n\nThat's basically it. That's all of artificial intelligence.\n\nLet's say a bank wanted to fire all of the employees in charge of lending. And replace them by artificial intelligence.\n\nHow would the bank lend money?\n\nWell, there's 100s of millions of loans already out there. And for each person who has ever borrowed money I know:\n\n- their age\n- where they grew up\n- what their job is, are they married? \n- are they divorced? do they have kids? \n- How often do they move? how have they done on prior loans like this? and I even know what they buy on Amazon and how often they fly to Las Vegas.\n\nI can put all these vectors in a database and divide them into people \"most likely to pay back the loan\" and people \"most likely to default\".\n\nThen, just like speech recognition or the Othello program above, I can use statistics to determine who I should loan money to.\n\nAnd if I say \"no\", I don't have to explain. On to the next one!\n\n---\n\nLet's say I want to fight terrorists.\n\nI already have examples of many terrorists who trained in the US and then went on to perform or attempt acts of terror.\n\nI know everything about their bank accounts. How often they transferred money. How often they traveled. How often they took out cash versus using a debit card.\n\nAnd so on.\n\nI can build a vector of attributes of what a terrorist bank account looks like. Then I can match new people against that database of vectors of terrorists.\n\nBelieve me, every time you do a bank transfer, some AI program is out there trying to determine if you are a terrorist.\n\n----\n\nThis is all that AI is.\n\nIt is nothing more. It's not \"intelligent\" from a human sense. It's not conscious, nor will it ever be.\n\nHere's how AI has improved in the past forty years (and how it will improve the next 40):\n\n- statistics has gotten better\n- methods of building the trees have gotten better (this was the subject of some of my research when I was in graduate school)\n- hardware has gotten faster\n- more data is available about everything.\n\nWhat is changing the fastest is data. The land grab of modern society is not land, or gold, or oil.\n\nIt's data.\n\nI have been invested in many companies that collect and sell data. I was an early investor (and on the board of) bit.ly [ https://l.facebook.com/l.php?u=https%3A%2F%2Fbit.ly%2F&h=AT3orGSwD27WWXcfNXOHYJT0fOlnbBcd09o-VTkqcool3TKr48HWxNENlGOPP6lIfwsStbD5Wf7coWSceGuXu4VLQTVAJtAj-Q-2ke-ur9MTAM23hnj0a_5zbrkqQhb9TrGsoLpl-pGplBVeF60t_yvCiN66VqBvRYQ0_xDtDkrxwyBg_gN_-TtTlSUZ6SxG4hwtOA41HAKFNXUZ8FLWvNqsywTK9jtKXcnOHM07XQUsDPz_zK2kuuD2y3Co3nXT3AKcvuqHp8yxH32IXkvumgJQW8tNnMLm8UNbKI0wQaXKzCVp09YD460Pz4LL8hlV5jz1zcZQaWgBTn-nDaEpvTiwmY-URZ2GkqasxK75 ], as an example. bit.ly [ https://l.facebook.com/l.php?u=https%3A%2F%2Fbit.ly%2F&h=AT0lhT5PDHPHSEeypFdJU2oliY61fKhCFq3VQNdtUb7zjGXHzJqFHFSI7ixaicN-wB79C_501XLqdAbfXuYrJc5fgV3tD_9VI6Rco0iKt-FqEHsvYQTKNPM5JIv0uzLszxVib1011tImkDOu7J5WhRI4V6KIv_7z-LZkWZ-N3Dv2BOF1lN_Wh3dChh87uU85kU9q0b-5OiIIOqf56rmWDTtCSQjKPmPkTaMlinknTvFbmnnyTyC0Ebyi4eBK6Wk5snRDNgf3eNlgU7ta19FqLeC3fEt1vfSqZFFyo7ttvva9FAbwcBU4MurvnutxRX_DBDZE45vNs7ygP4ZzyZEwHPYN9GEJM3XAmg ] accounts for about 2-5% of all Internet traffic.\n\nBelieve me when I say, data-driven companies know how many strawberries you ate last summer.\n\nAnd right now that data is used mostly to target you for ads about sneakers. Or politics.\n\nBut this is AI 1.0. Soon that data will be used to target your every movement, your every want, your every need.\n\nAmazon Prime won't be about delivering you what you want tomorrow. Amazon Prime Plus will be about delivering you what you want yesterday.\n\nPolice 2.0 will be like the movie \"Minority Report\".\n\nEven art and music will be driven by AI that studies the neurochemical responses to music you like to music you don't like. And then compose accordingly.\n\nWhere will humans still be unique?\n\nI don't know. Ask the humans with AI implants that enhance their brains so when they look at you they know exactly what answers will make you happy.\n\n\nBUT\u2026 will AI replace jobs?\n\nThe answer (at least in the next decade or so\u2026) is NO.\n\nLook at recent examples:\n\nA) Many people were worried ATM machines would replace bank tellers.\n\nInstead, the banks made so much in profits they opened up more branches than ever, creating new jobs.\n\nB) Will autonomous delivery services cost jobs.\n\nRight now there are millions of truck drivers involved in delivering goods. With autonomous delivery, less people will go shopping, more people will be required to shop in the aisles, finding products for people.\n\nObviously this is not a high-end job. But this replaces the fact that less cashiers and drivers will be needed.\n\nMeanwhile, there will be more high-end jobs. More maintenance engineers for the cars, customer service, marketing, etc.\n\nC) Ecommerce. Branding will become less important (branding is VERY important when everyone is shopping at the big box store but advertising will have to become more clever and digital) so the millions in profits that are generated from AI will filter down to more people starting e-commerce ventures and the ancillary businesses associated with that.)\n\nFinal conclusion:\n\n * AI will probably create a \u201chave\u201d and \u201chave not\u201d situation, particularly as humans start to use AI to increase mental and physical capacity\nThis is probably a net NEGATIVE for society as the higher classes will be able to afford \u201csuper AI\u201d capabilities, making them demi-gods to lower-classes.\n\n * AI will not destroy as many jobs\nInstead, massive profits will be generated, which will be soaked into the economy through a rising stock market, increase in opportunities, etc.\n\n * We can\u2019t predict. ATMs didn\u2019t destroy bank tellers. VCRs didn\u2019t destroy movie theaters. Spotify/Pandora/etc did destroy music stores and record sales but that was replaced by growing revenues in music tours.\n * Education needs to scale up. AI, programming, and the higher-end jobs that will be created need to be studied. College is not the place to study these opportunities. Instead: Khan Academy, Lynda, CodeAcademy, Coursera, etc should become accredited and get people ready for the opportunities that will arise.\n",
                "All of the original AI work was done by cognitive psychologists, because it was thought that they were the best option for making a machine capable of thinking like a person. I had a work-study job for one, in college, writing code in Prolog and LISP, in fact.\n\nWhen it was realized that Minsky had it all wrong with his \u201csociety of mind\u201d model, people started modeling artificial neural networks instead. And the CS people took over AI, because they were able to get actual forward progress. Or at lest spin off a bunch of useful and therefore monetizable technologies in the process of their research.\n\nSo if you want to design the algorithms, probably CS. If you want to write code, probably SWE.\n\nBut realize, that when I went to school, CS was a thing, and SWE was merely an \u201cemphasis\u201d you could elect, and everyone had to learn 5 or 6 programming languages, in language-specific classes.\n\nToday, you might learn \u201cdatabase theory, using C\u201d, and be on your own in learning the language itself, unless you go to a top-tier school, and take the harder option, if there are multiple degree tracks.\n\nFor example, CS-101 at Harvard is pretty much a C language class, and if you can\u2019t pass it, you aren\u2019t getting a CS degree from Harvard.\n\nOther top universities also still teach language classes, so that you know how to use your tools, before you are expected to use them on something. It helps keep students from being distracted by not knowing both the language, and the subject matter, at the same time.",
                "It is a misconception among the laymen that AI and SE have to be two distinct career paths. If you are indecisive, just go with both.\n\nIf you want to become a data scientist or AI researcher, you have to be decently good at software engineering, and vice versa. There are high demands in silicon valley for talents with hybrid skillsets in both AI and SE. The line is so blurred such that you could transit between AI and SE roles easily in tech giants like FANG\n\nAI and SE are so interconnected.\n\nSoftware projects (such as Tensorflow, OpenCog, Label Studio etc.) support the whole ecosystem of AI R&D, to cope with the rapidly increasing complexity and requirements of the AI models.\n\nTo make a AI model practical, you have to make sure it can be integrated in a software architecture. You have to make sure it is maintainable and scalable. You have to make sure it is cost efficient. That means you have to be knowledgeable in database design, operating system, software design, algorithm and architecture.\n\nOn the other hand, it does not make sense to be an expert in software engineering without knowing AI. As AI continue to advance, AI will become a general tool used in typical software projects.\n\nAI will be one of the core knowledges every software developer need to master. It is just a part of software engineering, along with technologies like database, cloud architecture, distributed system, security, compilers, operating system, user experience, automated testing, etc."
            ]
        },
        {
            "tag": "software_engineering",
            "patterns": [
                "What's the work-life balance like as an engineer in Germany?",
                "Can I be both data scientist & software engineer? Or do I need to choose between them?",
                "Can I be a software engineer with an MIS degree or a data scientist?",
                "What distinguishes a great software engineer from a good one?",
                "What do the top 1% of software engineers do that the other 99% do not?",
                "What distinguishes a great software engineer from a good one?"
            ],
            "responses": [
                "The work part\n\n * Salary-wise - it\u2019s lower than the US in general, but it\u2019s decent enough to have a comfortable life in Germany since the living costs are low compared to most highly developed countries.\n * Work contract - As Quora User has answered, there are two types of contracts in Germany - Tarif (group negotiated) or Aussertarif (individually negotiated). Under Tarif, you\u2019re well protected by a work union such as IG Metall or alike. However, an outside-tariff salary is also desirable. You get more base salary than the tariff workers. Managers or senior engineers / scientists usually fall into this category.\n * Tax rate - As a single, the disposable income ratio is one of the lowest in the world. (Producing more kids would solve this problem ;) ) We worked both in Canada and Germany and the income tax rates are comparable with a kid. You can expect an income tax rate between 25%~35% from a junior software engineer to a senior software engineer/scientist in Germany, depending on your salary.\n * Healthcare / public pension - it\u2019s a complex topic, so I won\u2019t go into detail. You can expect around 17% to be taken from your salary by default. After this, you can see why the disposable income in Germany is so low.\n * As for working hours, if you\u2019re under tariff, you only need to work the hours agreed upon in your contract. If you\u2019re outside tariff, it still doesn\u2019t mean you have to work your ass off. It often means you cannot build up overtime hours over a long window but instead use them up on a weekly or monthly basis.\n * 30 vacation days a year + overtime hours you\u2019ve built up through the year. If you\u2019re under the biggest work union IG-Metall, you can also get 6 or 8 more vacation days if you have a kid under age of 8.\nThe life part\n\n * Language - Most engineers have a very good command of English. In my previous company and the current company in Stuttgart, all official emails are in English and German. And meetings are usually held in English esp. if a non-German speaker is present. However, this may vary from company to company.\n * Outdoor activities and nature - Amazing, probably only after Scandinavian countries + New Zealand :) The country is very friendly to cyclists, hikers and skiers. It\u2019s also the decision point why we chose Germany over Canada. However, Germans wouldn\u2019t understand this part. :p If you love outdoor activities, Germany is a wonderful place for you.\n * Free time - You get lots of free time compared to North America. So you definitely need to learn to plan your free time, for example, overcoming seven summits or alike.\n * Education - Free school/university education. Oftentimes, PhD positions are paid like a regular employee\n * Healthcare - there\u2019s no universal healthcare but a two-class system. There are multiple public healthcare and private providers. Private insurance companies usually provide a better coverage and as an engineer, you probably are qualified to choose a private one.\n * If you\u2019re an introvert, and you like to reason with people and rationalize things based on rules and orders, this is the right place for you. Otherwise you\u2019ll feel miserable.\nI can go on and on, but you get the idea.",
                "In technology companies, usually these roles are kept separate.  Of course, data scientists do a fair amount of engineering (but not always the other way around).  At a small company you can probably wear both hats pretty easily.  \n\nIn finance, the role of \"quant\" is basically a combination of the two, so if that is your goal, it is another industry to consider.",
                "I would say the short answer is, yes.\n\nBut I don\u2019t think that\u2019s what you\u2019re necessarily asking.\n\nAs someone who graduated with an MIS degree, I can tell you it is uncommon to see MIS graduates go into software engineering. I took a Java course that took me through all the classic programming concepts (variables, loops, recursion, hash tables, etc\u2026). In conjunction with taking online courses (Harvard CS50, Fullstack Web Development, I felt I was able to develop a solid programming foundation. That being said, most MIS graduates will go into database, operations, or analysis work.\n\nIf you want to be a software engineer or data scientist, you will want to set yourself up such a way where you can understand concepts such as data structures and developing an algorithmic approach (learn about big \u201cO\u201d notation). It\u2019s not that an MIS degree does not prepare you in these areas; it is most likely a C.S or D.S learning prepares you even more so.\n\nDefinitely feel free to reach out; happy to be of help in any way possible!",
                "1. It is not the code they write. It is the code they don't have to write.\n2. It is not how fast they grow a code base, in terms of lines of code or complexity, but about how fast they shrink it without losing feature or functionality.\n3. If you try to start the 'whats the best language' argument with them, do they smile, or maybe look bored, and then change the subject? Or do they evangelize? If they evangelize, they're not a great software engineer.\n4. It is not about the code or the language. Nor is it about 'obsession', 'knack', 'talent' or any other pseudo magical term. Simply- do they understand software beyond the level of code? Do they understand the software on the architectural level? Or can they only think about lines of code? Can they slip between mathematical abstractions of problems and software? Can they work with stakeholders to understand their needs for the system, or will they develop the system they want to code, that they think you should really want? Someone can be a great hacker or coder or programmer, but that is not he same as a great Software Engineer. I say this without a value scale- a great programmer is a great programmer... But you don't ask a master welder to design a bridge.\n5. Can they 'spot the flaw' when everyone else in the room is enamored of some solution or hot new thing, and moreover, can they explain that fundamental flaw in a way that makes it clear to *everyone* in the room.\n6. Can they Listen?  If not then they are not a great software engineer.\n",
                "1. Invest in learning and building tools. I've yet to meet a top engineer who didn't acquire mastery of their editor (EMACS, vi, etc), source control system, debuggers (though that's sadly going away), and programming environment. By contrast, mediocre programmers can (for example) edit code in EMACS but don't treat it like a development environment either to speed up the edit/compile/debug cycle or to reduce memory load (by off-loading knowledge of where a function is by using TAGS or some meta-search tool). Top engineers not only understand all the basic features, but usually make heavy use of the extension languages and customizability of their environment.\n2. Give credit where credit is due. Again, if you're a top engineer, you don't need to hoard credit and can happily acknowledge other people's work. This makes other great people want to work with you, and it's a self-fulfilling cycle.\n3. Impatience. Rather than wanting to spend time in meetings, top engineers write code, and spend time writing code rather than talking about it. Jeff Dean, Pengtoh, and Amit Singh have done major code changes to Google infrastructure without hesitancy. As Bill Coughran once said, \"You come in one morning and discover that the universe has changed.\"\n4. Total stack comprehension. Great software engineers don't stop at an abstraction or module boundaries. They penetrate those boundaries and attack problems or bugs until they solve them. I was once sitting down next to Pengtoh while he dug down into misbehaving code and discovered a bug in the assembly the compiler was generating. He then figured out a stable workaround for that bug. If you're intimidated by abstractions or have too much respect for module boundaries, you wouldn't have found that bug.\nIf you do all of those 4, there's zero chance you won't be one of the top 1%.",
                "I won't try to give a comprehensive answer to this question, but there are a few qualities I've noticed in great programmers that I don't hear mentioned too often:\n * Able to balance pragmatism and perfectionism - Great programmers have the ability to make both masterful/quick/dirty hacks and elegant/refined/robust solutions, and the wisdom to choose which is appropriate for a given problem.  Some lesser programmers seem to lack the extreme attention to detail necessary for some problems.  Others are stuck in perfectionist mode.\n * Not averse to debugging and bugfixing - Mediocre programmers often fear and loathe debugging, even of their own code.  Great programmers seem to dive right and drill down with Churchill-esque tenacity.  They might not be happy if it turns out that the bug is outside their code, but they will find it.\n * Healthy skepticism - A good programmer will get a solution that appears to work and call it a day.  A great programmer will tend to not trust their own code until they've tested it extensively.  This also comes up a lot in data analysis and system administration.  An average programmer might see a small innocuous-looking discrepancy and ignore it.  If a great programmer sees something like that they will suspect it could be a hint of a greater problem, and investigate further.  Great programmers tend to do more cross-checking and sanity checking, and in doing so discover subtle bugs.\n"
            ]
        },
        {
            "tag": "data_science",
            "patterns": [
                "What are the best companies for a data scientist to work for?",
                "What companies employ the world\u2019s best data scientists?",
                "What qualities separate great data scientists from good data scientists?",
                "What is a data scientist's career path?",
                "Is data science so easy that the market will eventually be oversaturated and jobs will be hard to find?",
                "Should I be a data scientist or software engineer?",
                "What is the difference between a software engineer and a data scientist? Which is the best choice?"
            ],
            "responses": [
                "I\u2019ve once bumped into the CEO of DataRobot [ https://www.datarobot.com/ ] (tried to sell him some of our services).\n\nTurned out they\u2019ve had offices all over the world and some of my acquaintances worked for the company.\n\nThey hire the best data science staff worldwide, so if you consider a good company to start with, DataRobot is the place.",
                "The predictable answer is that many of the world\u2019s best data scientists work at some of the largest, most successful and most well-known tech companies in the world. Places like Microsoft, Apple, Amazon, Google, Facebook, Alibaba and Tencent. The usual suspects.\n\nThese companies employ armadas of data scientists and offer good benefits, interesting challenges and prestige. They\u2019re a safe option and a natural first place to look for a job.\n\nIt may almost seem like Big Tech is where all the best data scientists work, but that\u2019s simply not the case.\n\nI know some truly amazing data scientists who work at startups you\u2019ve never heard of. Semi-early stage startups may offer competitive compensation packages for experienced data scientists along with more responsibility and technical challenges that some might find more interesting. There\u2019s also the allure of being a key employee in a company that could potentially become a unicorn.\n\nSome successful data scientists go all out and start their own companies as well.\n\nThen you have data scientists who work as consultants and contractors, being either self-employed or employed at consulting firms. This type of gig may offer more money at the cost of higher risk. The work is more diverse, requiring a more generalized skillset as well as strong interpersonal skills. It\u2019s not for everyone, but for some it\u2019s great.\n\nFinally, there are data scientists who work at regular non-tech companies. It may be an investment bank, a petroleum company, a retail chain or any other type of company you can think of. Although it may not be the first place you\u2019d expect to find a world-class data scientist, that doesn\u2019t mean they don\u2019t exist. Sometimes, if a company really wants a superstar to kickstart their data science initiatives, they can go to great lengths to attract the right talent.",
                "I have had the pleasure to work with a few exceptional data scientists (with some of them way back when it was not even called data science) and I have worked with plenty good ones. What they all had in common: self-sufficient coding, good \u2018tech\u2019 communication, solid statistics knowledge, predictive modeling background, and data experience.\n\nBut what made the 4 great ones great?\n\nInsatiable curiosity, healthy amount of common (business) sense, deep rooted scepticism, and finally some form of sixth sense when it came to data. Two of them were statisticians, the other two computer scientists. They are also the only 4 people in the world whose findings I will trust blindly. (Did I mention skepticism?)\n\nThese things are all closely interconnected. What makes them so vital is one of the biggest challenges in data science: Quality control. How sure are you that what you just build is good? That the analysis you just did truly generalizes to the question you are supposed to answer?\n\nThe reality is that even I often have a vague feeling at best how close I have come to \u2018the truth\u2019 or let\u2019s say to the best that can be done. I do not know if tinkering another week will improve the performance of my model by 0 or by 25%. I do not know the probability with which digging for another hour or two I will discover that my data had a huge sampling problem. I do not know. And I am the one who build it! In data science we are constantly dealing with the nagging feeling that we do not know about something we don\u2019t know.\n\nThe only hope is, that I have found 90% of all the mistakes in my analysis because I know the data as well as humanly possibly, I have rejected the first 10 answers I got, I have this sense that tells me when things look too good to be true. My model can predict the probability that somebody clicks on an ad with 95% certainty? I doubt it! Is this overfitting? No I actually had duplicates in my training set (this is different from the classical sense of overfitting) \u2026 how did they get there?\n\nIt is not sufficient to deal with the symptom \u2026 if there were duplicates where none should have been - I need to go back and UNDERSTAND how they got there \u2026 Chances are, wherever there was one problem, there will be more \u2026\n\nMy model\u2019s performance doubles and all I did was retrain it on a more recent time period? Not likely\u2026 I need to find the cause. Unfortunately it was not that we had new and better features, but instead we discovered ad-fraud.\n\nThe best indicator of somebody shopping for jewelry on Amazon is that they have no prior purchases? Possible (you would not believe how many people will come up with nice stories why this might be true \u2026) but rather unlikely! In fact - this is a subtle sampling problem - in order to be in the Amazon database you must have bought something - and so a clear indication of buying jewelry is being in the database and not having any other purchases.\n\nThe patient identifier is really good at predicting who has breast cancer? Either you are a terrible data scientist OR something is wrong with the data (it was, the data had originated from 4 different locations with different patient number systems and different base rates).\n\nCan you learn this? Maybe, if you are curious and have a great mentor. But more than anything, it is who you are.",
                "The most in-demand profession of the twenty-first century: \"Data Scientist\". However, the role has been exaggerated to the point of absurdity. It's too misleading, and unfortunately, about 55% of Ds aspirants fail to start a successful data science career, even after having profitable eligibility for the same.\n\nA career in data science isn't just about being a \u2018data scientist\u2019, though. As an alternative, there are numerous other roles to consider. Even so, not everyone is a good fit for every role.\n\nHence, it's better to discuss data scientists' career paths; let's direct the same toward DS career trajectories.\n\nAt the very beginning, I must say, the career path will not be the same for all. It solely depends on your present career stage and past qualification background.\n\nUsually, a DS career path might look like as follows:\n\nDifferent people play different roles at different points in the DS career stages. Choosing the initial one is the most vital one. If you make any mistake at this stage, your entire career may get ruined.\n\nMy personal experience has led me to the realisation that it's always better to go with expert guidance regarding this most sensitive stage.\n\nHence, going out of the box, I suggest everyone have data science career counselling (only the professional ones).\n\nInstitutes like Learnbay offer extremely personalised and unbiased data science career counselling (free).\n\nSuch counselling sessions offer you the following benefits:-\n\n * Assess your individual ability to learn data science.\n * Identify the right path of mastering both statistical programming and analytical skills.\n * Assess your domain knowledge and accordingly plan your custom-fit learning path.\n * Assess your critical thinking and communication skills and accordingly plan your assignments.\n * You get respective instructors based on your present career stage and expertise level.\n * According to the final result, Learnbay experts identify the DS roles and career scopes that fit best for you. Your training progresses accordingly.\nIf you ask me about the learning path, I must say that it should be adequately customised. Neither the statistical approach nor programming ability sound is ignored. Both have to be proceeds hand-in-hand.\n\nAs already mentioned, Learnbay offers completely custom-fit training, so if you are interested in grabbing a lucrative and secure DS career within a year, you can check out their courses.\n\nHere are some of the most popular ones:\n\n * Data Science and AI Foundational Certification with Domain Specialisation\n * Data Science and AI Certification for Managers and Leaders\n * Data Analytics and Business Analytics Fast Track Course\n * Data Science and AI Certification with Job or 100% money-back guarantee.\nHow does Learnbay ensure a successful data science career path?\n\nA simple online answer it\u2019s via its amazing training features.\n\n * Interactive and real-time training-\nYou can choose any from the online/ offline/ blended mode. But every mode provides the experience of offline classroom study. It\u2019s not the recorded video-based classes and weekly group discussion. Each session is live and interactive. To ensure 1 to 1 attention, the batches are kept small. In case you are a non-programming guy, you get additional programming classes free of cost.\n\n * Additional support in the form of one-on-one tutoring-\nEven for the shy candidates, the Learnbay course fits perfectly; you can reach the instructors via a one-to-one doubt clearance session multiple times until your concept becomes transparent.\n\n * Full-stack domain-specialised studies-\nAs per your existing domain experience, you experienced industry-specific DS training from day one. As a result, you become the master of applying industry best practices at the end of the pros. In fact, this is the key reason why recruiters quickly grab Learnbay candidates. Plenty of elective domain options are there. A few of them are BFSI, Marketing, Energy, telecom, Pharma, HR, etc.\n\n * Project expertise-\nA course without an effective capstone project is just like a car without wheels. Instead of repeating practice projects, Learnbay offers you the scopes of pursuing a fresh project that solves one of your domain issues. Apart from that, you learn through 15+ live MNC projects throughout the course.\n\n * Job guarantee-\nNot only the placement supports but also you get dedicated job assistance. If you remain unable to grab a DS opportunity within the six months beyond the course ending data, you get a full refund.\n\nOverall, if you enrol at this institute, it will help you draw your DS career path from bottom to top. And most importantly, with 100% of success assurance. All the support you get at a very reasonable cost. No need to worry about neither the learning path nor the growth path.\n\nI sincerely hope my advice was beneficial to you. Thanks for your time.",
                "The mechanics behind data science is very easy. The math is also easy.\n\nEvery year universities are pushing out 10,000s of folks who can code in 24 languages or more.\n\nOn top of that you have 1000s of folks who train, study online and make a career switch to data science.\n\nBut I'd argue that over 90% struggle with understanding, interpretation and ability to practically do something useful with the \u201cdata\u201d they use.\n\nMany \u201cdata scientists\u201d who apply for a role feel there is a lot of competition. There isn't. Most can do the work but come up with ridiculous findings, backing this up by simple curve fitting that serves their purpose.\n\nData science is easy. Ability to do something with it, different story.\n\nBottom line, you prove your worth in $, you get yourself a job.",
                "That's the dilemma of my life :)\n\nI found that there are ways to combine the two. A data scientist doesn't have to work with data or build models all day long. One can also be building components that infer something from data, as a part of an automated software piece.\n\nFor example, if you're building a fitness tracking app that is in charge of counting steps, you have two major steps for developing the solution:\n\n1. Algorithmic: taking some data you collected A priori and develop an algorithm that is capable of figuring out how many steps had been taken. This might include:\n\n * Researching the problem\n * Looking at the data and finding patterns\n * Data cleansing, feature engineering and processing of the data (fft etc.)\n * Developing the model\n * Evaluating the model's performance\n2. Software Implementation: implement the algorithm:\n\n * Get the sensory data out of the sensors\n * Make sure the algorithm doesn't fail (exceptions and such)\n * Take care of different cases\n * Make sure the performance is acceptable\n * Send the output to the UI / other modules\nSo this task, which is essentially a \"data scientist\" task, has lots of challenges both from the data perspective and the software implementation perspective.\n\nI'm sure there are many more examples like this one.\n\n---EDIT:\n\nThe question had changed since I wrote this answer. The original question was something like \"Which career should I select? data science or software development?",
                "The skills overlap, particularly in programming, but the fundamental difference is the purpose of the roles. Data scientists are about insight - teasing out patterns from data and using algorithms to create predictions and recommendations. Software engineers are about action - creating tools and applications that users interact with to drive specific outcomes\n\nA more explicit difference also lies in the ancillary skills required outside of programming. Data scientists will need strong mathematical and statistical knowledge to be able to deploy and interpret algorithms and formulas to play with data. Software engineers need to be strong on process formulation and functional design to be able to deploy efficient code as well as user interfaces to make tools that are intuitive to users. Both roles can also benefit from knowing about the business and/or scientific domain where they will be playing in, since that determines the problems they will solve and the data they will encounter.\n\nThe two can collaborate and overlap - for example for e-commerce a data scientist might come up with an algorithm to detect associations between users and products to produce purchase recommendations. The software engineer will take the algorithm and its outputs and integrate this into a web or mobile portal that users can see which leads to product checkout if selected.\n\nWhich is best? That depends on your inclination: I\u2019d say if you\u2019re a detective and are concerned about discovering truths and answering questions from data, data science is for you. If you\u2019re a builder and are concerned about creating products that solve problems for users, then software engineering is probably a better path."
            ]
        }
    ]
}